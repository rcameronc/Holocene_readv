---------------
d6g_h6g_l96C_ump5_lm10 run number 0
number of datapoints =  (128, 8)
model built, time= 0.13532590866088867
model minimized, time= 7.0817999839782715
time elapsed =  8.051090002059937
negative log marginal likelihood = 42.64260154691016
Filename: readv_it.py

Line #    Mem usage    Increment   Line Contents
================================================
    41    306.7 MiB    306.7 MiB   @profile
    42                             
    43                             def readv():
    44                             
    45                                 # set the colormap and centre the colorbar
    46    306.7 MiB      0.0 MiB       class MidpointNormalize(Normalize):
    47    306.7 MiB      0.0 MiB           """Normalise the colorbar.  e.g. norm=MidpointNormalize(mymin, mymax, 0.)"""
    48    306.7 MiB      0.0 MiB           def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
    49                                         self.midpoint = midpoint
    50                                         Normalize.__init__(self, vmin, vmax, clip)
    51                             
    52    306.7 MiB      0.0 MiB           def __call__(self, value, clip=None):
    53                                         x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
    54                                         return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))
    55                             
    56                             
    57                                 ####################  Initialize parameters #######################
    58                                 #################### ---------------------- #######################
    59                             
    60    306.7 MiB      0.0 MiB       parser = argparse.ArgumentParser(description='import vars via c-line')
    61    306.7 MiB      0.0 MiB       parser.add_argument("--mod", default='d6g_h6g_')
    62    306.7 MiB      0.0 MiB       parser.add_argument("--lith", default='l71C')
    63    306.7 MiB      0.0 MiB       parser.add_argument("--um", default="p2")
    64    306.7 MiB      0.0 MiB       parser.add_argument("--lm", default="3")
    65                             
    66    306.7 MiB      0.0 MiB       args = parser.parse_args()
    67    306.7 MiB      0.0 MiB       ice_models = [args.mod]
    68    306.7 MiB      0.0 MiB       lith_thicknesses = [args.lith]
    69    306.7 MiB      0.0 MiB       um = args.um
    70    306.7 MiB      0.0 MiB       lm = args.lm
    71                             
    72                                 #ice_models = ['d6g_h6g_']# , 'glac1d_']
    73                                 #lith_thicknesses = ['l96C']# , 'l71C']
    74                             
    75    991.6 MiB      0.0 MiB       for i, ice_model in enumerate(ice_models):
    76    991.6 MiB      0.0 MiB           for k, lith_thickness in enumerate(lith_thicknesses):
    77    306.7 MiB      0.0 MiB               plotting = 'false'
    78    306.7 MiB      0.0 MiB               decomp = 'false'
    79    306.7 MiB      0.0 MiB               ice_model = ice_model # 'd6g_h6g_' # 'glac1d_' #   #
    80    306.7 MiB      0.0 MiB               lith_thickness = lith_thickness # 'l96'  # 'l90C'
    81    306.7 MiB      0.0 MiB               model = ice_model + lith_thickness
    82    306.7 MiB      0.0 MiB               place = 'fennoscandia'
    83                             
    84                                         locs = {
    85    306.7 MiB      0.0 MiB                   'england': [-12, 2, 50, 60],
    86    306.7 MiB      0.0 MiB                   'easternhem': [50, 178, -45, 80],
    87    306.7 MiB      0.0 MiB                   'westernhem': [-175, 30, -80, 75],
    88    306.7 MiB      0.0 MiB                   'world': [-179.8, 179.8, -89.8, 89.8],
    89    306.7 MiB      0.0 MiB                   'namerica': [-150, -20, 10, 75],
    90    306.7 MiB      0.0 MiB                   'eastcoast': [-88, -65, 15, 40],
    91    306.7 MiB      0.0 MiB                   'europe': [-20, 15, 35, 70],
    92    306.7 MiB      0.0 MiB                   'atlantic':[-85,10, 25, 60],
    93    306.7 MiB      0.0 MiB                   'fennoscandia': [-15, 50, 45, 73],
    94                                         }
    95    306.7 MiB      0.0 MiB               extent = locs[place]
    96    306.7 MiB      0.0 MiB               tmax, tmin, tstep = 4010, 3490, 100
    97                             
    98    306.7 MiB      0.0 MiB               ages_lgm = np.arange(100, 26000, tstep)[::-1]
    99                             
   100                                         #import khan dataset
   101    306.7 MiB      0.0 MiB               path = 'data/GSL_LGM_120519_.csv'
   102                             
   103    325.2 MiB     18.5 MiB               df = pd.read_csv(path, encoding="ISO-8859-15", engine='python')
   104    330.3 MiB      5.1 MiB               df = df.replace('\s+', '_', regex=True).replace('-', '_', regex=True).\
   105    333.5 MiB      0.4 MiB                       applymap(lambda s:s.lower() if type(s) == str else s)
   106    333.5 MiB      0.0 MiB               df.columns = df.columns.str.lower()
   107    333.5 MiB      0.0 MiB               df.rename_axis('index', inplace=True)
   108    333.6 MiB      0.1 MiB               df = df.rename({'latitude': 'lat', 'longitude': 'lon'}, axis='columns')
   109    333.6 MiB      0.0 MiB               dfind, dfterr, dfmar = df[(df.type == 0)
   110    333.7 MiB      0.1 MiB                                         & (df.age > 0)], df[df.type == 1], df[df.type == -1]
   111    333.7 MiB      0.0 MiB               np.sort(list(set(dfind.regionname1)))
   112                             
   113                                         #select location
   114    333.7 MiB      0.0 MiB               df_place = dfind[(dfind.age > tmin) & (dfind.age < tmax) &
   115                                                          (dfind.lon > extent[0])
   116                                                          & (dfind.lon < extent[1])
   117                                                          & (dfind.lat > extent[2])
   118    333.7 MiB      0.0 MiB                                & (dfind.lat < extent[3])][[
   119    333.8 MiB      0.1 MiB                                    'lat', 'lon', 'rsl', 'rsl_er_max', 'age'
   120                                                          ]]
   121                                         # & (df_place.rsl_er_max < 1)
   122    333.8 MiB      0.0 MiB               df_place.shape
   123                             
   124                                         ####################  	Plot locations  	#######################
   125                                         #################### ---------------------- #######################
   126                             
   127                                         #get counts by location rounded to nearest 0.1 degree
   128    333.8 MiB      0.0 MiB               df_rnd = df_place.copy()
   129    333.8 MiB      0.0 MiB               df_rnd.lat = np.round(df_rnd.lat, 1)
   130    333.8 MiB      0.0 MiB               df_rnd.lon = np.round(df_rnd.lon, 1)
   131    333.8 MiB      0.0 MiB               dfcounts_place = df_rnd.groupby(
   132    333.9 MiB      0.1 MiB                   ['lat', 'lon']).count().reset_index()[['lat', 'lon', 'rsl', 'age']]
   133                             
   134                                         #plot
   135    339.7 MiB      5.8 MiB               fig = plt.figure(figsize=(10, 7))
   136    339.9 MiB      0.2 MiB               ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())
   137                             
   138    340.1 MiB      0.2 MiB               ax.set_extent(extent)
   139    340.1 MiB      0.0 MiB               ax.coastlines(resolution='110m', linewidth=1, zorder=2)
   140    340.1 MiB      0.0 MiB               ax.add_feature(cfeature.OCEAN, zorder=0)
   141    340.1 MiB      0.0 MiB               ax.add_feature(cfeature.LAND, color='palegreen', zorder=1)
   142    340.1 MiB      0.0 MiB               ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=3)
   143    340.1 MiB      0.0 MiB               ax.gridlines(linewidth=1, color='white', alpha=0.5, zorder=4)
   144    340.1 MiB      0.0 MiB               scat = ax.scatter(dfcounts_place.lon,
   145    340.1 MiB      0.0 MiB                                 dfcounts_place.lat,
   146    340.1 MiB      0.0 MiB                                 s=dfcounts_place.rsl * 70,
   147    340.1 MiB      0.0 MiB                                 c='lightsalmon',
   148    340.1 MiB      0.0 MiB                                 vmin=-20,
   149    340.1 MiB      0.0 MiB                                 vmax=20,
   150    340.1 MiB      0.0 MiB                                 cmap='coolwarm',
   151    340.1 MiB      0.0 MiB                                 edgecolor='k',
   152    340.1 MiB      0.0 MiB                                 linewidths=1,
   153    340.1 MiB      0.0 MiB                                 transform=ccrs.PlateCarree(),
   154    340.6 MiB      0.5 MiB                                 zorder=5)
   155    340.6 MiB      0.0 MiB               size = Line2D(range(4),
   156    340.6 MiB      0.0 MiB                             range(4),
   157    340.6 MiB      0.0 MiB                             color="black",
   158    340.6 MiB      0.0 MiB                             marker='o',
   159    340.6 MiB      0.0 MiB                             linewidth=0,
   160    340.6 MiB      0.0 MiB                             linestyle='none',
   161    340.6 MiB      0.0 MiB                             markersize=16,
   162    340.6 MiB      0.0 MiB                             markerfacecolor="lightsalmon")
   163    340.6 MiB      0.0 MiB               labels = ['RSL datapoint location']
   164    340.6 MiB      0.0 MiB               leg = plt.legend([size],
   165    340.6 MiB      0.0 MiB                                labels,
   166    340.6 MiB      0.0 MiB                                loc='lower left',
   167    340.6 MiB      0.0 MiB                                bbox_to_anchor=(0.00, 0.00),
   168    340.6 MiB      0.0 MiB                                prop={'size': 20},
   169    340.6 MiB      0.0 MiB                                fancybox=True)
   170    340.6 MiB      0.0 MiB               leg.get_frame().set_edgecolor('k')
   171    340.6 MiB      0.0 MiB               ax.set_title('')
   172                             
   173                                         ####################  Make 3D fingerprint  #######################
   174                                         #################### ---------------------- #######################
   175                             
   176    340.6 MiB      0.0 MiB               filename = 'data/WAISreadvance_VM5_6ka_1step.mat'
   177                             
   178    339.6 MiB      0.0 MiB               waismask = io.loadmat(filename, squeeze_me=True)
   179    339.6 MiB      0.0 MiB               ds_mask = xr.Dataset({'rsl': (['lat', 'lon', 'age'], waismask['RSL'])},
   180                                                              coords={
   181    339.6 MiB      0.0 MiB                                        'lon': waismask['lon_out'],
   182    339.6 MiB      0.0 MiB                                        'lat': waismask['lat_out'],
   183    339.6 MiB      0.0 MiB                                        'age': np.round(waismask['ice_time_new'])
   184                                                              })
   185    339.6 MiB      0.0 MiB               fingerprint = ds_mask.sel(age=ds_mask.age[0])
   186                             
   187                             
   188    339.6 MiB      0.0 MiB               def make_fingerprint(start, end, maxscale):
   189                             
   190                                             #palindromic scaling vector
   191    339.6 MiB      0.0 MiB                   def palindrome(maxscale, ages):
   192                                                 """ Make palindrome scale 0-maxval with number of steps. """
   193    339.6 MiB      0.0 MiB                       half = np.linspace(0, maxscale, 1 + (len(ages) - 1) // 2)
   194    339.6 MiB      0.0 MiB                       scalefactor = np.concatenate([half, half[::-1]])
   195    339.6 MiB      0.0 MiB                       return scalefactor
   196                             
   197    339.6 MiB      0.0 MiB                   ages_readv = ages_lgm[(ages_lgm < start) & (ages_lgm >= end)]
   198    339.6 MiB      0.0 MiB                   scale = palindrome(maxscale, ages_readv)
   199                             
   200                                             #scale factor same size as ice model ages
   201    339.6 MiB      0.0 MiB                   pre = np.zeros(np.where(ages_lgm == start)[0])
   202    339.6 MiB      0.0 MiB                   post = np.zeros(len(ages_lgm) - len(pre) - len(scale))
   203                             
   204    339.6 MiB      0.0 MiB                   readv_scale = np.concatenate([pre, scale, post])
   205                             
   206                                             #scale factor into dataarray
   207    339.6 MiB      0.0 MiB                   da_scale = xr.DataArray(readv_scale, coords=[('age', ages_lgm)])
   208                             
   209                                             # broadcast fingerprint & scale to same dimensions;
   210    339.6 MiB      0.0 MiB                   fingerprint_out, fing_scaled = xr.broadcast(fingerprint.rsl, da_scale)
   211                             
   212                                             # mask fingerprint with scale to get LGM-pres timeseries
   213    339.6 MiB      0.0 MiB                   ds_fingerprint = (fingerprint_out *
   214    598.6 MiB    259.0 MiB                                     fing_scaled).transpose().to_dataset(name='rsl')
   215                             
   216                                             # scale dataset with fingerprint to LGM-present length & 0-max-0 over x years
   217    598.6 MiB      0.0 MiB                   xrlist = []
   218    856.7 MiB      0.0 MiB                   for i, key in enumerate(da_scale):
   219    856.7 MiB      1.1 MiB                       mask = ds_fingerprint.sel(age=ds_fingerprint.age[i].values) * key
   220    856.7 MiB      0.0 MiB                       mask = mask.assign_coords(scale=key,
   221    856.7 MiB      0.0 MiB                                                 age=ages_lgm[i]).expand_dims(dim=['age'])
   222    856.7 MiB      0.0 MiB                       xrlist.append(mask)
   223   1116.0 MiB    259.2 MiB                   ds_readv = xr.concat(xrlist, dim='age')
   224                             
   225   1116.0 MiB      0.0 MiB                   ds_readv.coords['lon'] = pd.DataFrame((ds_readv.lon[ds_readv.lon >= 180] - 360)- 0.12) \
   226   1116.0 MiB      0.0 MiB                                           .append(pd.DataFrame(ds_readv.lon[ds_readv.lon < 180]) + 0.58) \
   227   1116.0 MiB      0.0 MiB                                           .reset_index(drop=True).squeeze()
   228   1116.0 MiB      0.0 MiB                   ds_readv = ds_readv.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   229                             
   230                                             # Add readv to modeled RSL at locations with data
   231                                             ##### Need to fix this, as currently slice does not acknowledge new coords #########
   232   1116.0 MiB      0.0 MiB                   ds_readv = ds_readv.sel(age=slice(tmax, tmin),
   233   1116.0 MiB      0.0 MiB                                           lon=slice(df_place.lon.min() + 180 - 2,
   234   1116.0 MiB      0.0 MiB                                                     df_place.lon.max() + 180 + 2),
   235   1116.0 MiB      0.0 MiB                                           lat=slice(df_place.lat.max() + 2,
   236   1116.1 MiB      0.1 MiB                                                     df_place.lat.min() - 2))
   237   1116.1 MiB      0.0 MiB                   return ds_readv
   238                             
   239                             
   240                                         #Make deterministic readvance fingerprint
   241    339.6 MiB      0.0 MiB               start, end = 6100, 3000
   242    339.6 MiB      0.0 MiB               maxscale = 2.25
   243    612.9 MiB      0.0 MiB               ds_readv = make_fingerprint(start, end, maxscale)
   244                             
   245                             
   246                                         ####################  Build  GIA models 	#######################
   247                                         #################### ---------------------- #######################
   248                             
   249                                         #Use either glac1d or ICE6G
   250                             
   251                             
   252    945.7 MiB      0.0 MiB               def build_dataset(path, model):
   253                                             """download model runs from local directory."""
   254    945.7 MiB      0.0 MiB                   path = path
   255    945.7 MiB      0.0 MiB                   files = f'{path}*.nc'
   256    945.7 MiB      0.0 MiB                   basefiles = glob.glob(files)
   257                                             modelrun = [
   258    945.7 MiB      0.0 MiB                       key.split('output_', 1)[1][:-3].replace('.', '_')
   259    945.7 MiB      0.0 MiB                       for key in basefiles
   260                                             ]
   261    945.7 MiB      0.0 MiB                   dss = xr.open_mfdataset(files,
   262    945.7 MiB      0.0 MiB                                           chunks=None,
   263    945.7 MiB      0.0 MiB                                           concat_dim='modelrun',
   264    945.7 MiB     29.4 MiB                                           combine='nested')
   265    945.7 MiB      2.6 MiB                   lats, lons, times = dss.LAT.values[0], dss.LON.values[
   266    945.7 MiB      0.5 MiB                       0], dss.TIME.values[0]
   267    945.7 MiB      0.0 MiB                   ds = dss.drop(['LAT', 'LON', 'TIME'])
   268    945.7 MiB      0.0 MiB                   ds = ds.assign_coords(lat=lats,
   269    945.7 MiB      0.0 MiB                                         lon=lons,
   270    945.7 MiB      0.0 MiB                                         time=times,
   271    945.7 MiB      0.0 MiB                                         modelrun=modelrun).rename({
   272    945.7 MiB      0.0 MiB                                             'time': 'age',
   273    945.7 MiB      0.0 MiB                                             'RSL': 'rsl'
   274                                                                   })
   275    945.7 MiB      0.0 MiB                   return ds
   276                             
   277    945.7 MiB      0.0 MiB               def one_mod(path, names):
   278                                             """Organize model runs into xarray dataset."""
   279    945.7 MiB      0.0 MiB                   ds1 = build_dataset(path, names[0])
   280    945.7 MiB      0.0 MiB                   names = names[1:]
   281    969.7 MiB     29.0 MiB                   ds = ds1.chunk({'lat': 10, 'lon': 10})
   282    969.7 MiB      0.0 MiB                   for i in range(len(names)):
   283                                                 temp = build_dataset(names[i])
   284                                                 temp1 = temp.interp_like(ds1)
   285                                                 temp1['modelrun'] = temp['modelrun']
   286                                                 ds = xr.concat([ds, temp1], dim='modelrun')
   287    969.7 MiB      0.0 MiB                   ds['age'] = ds['age'] * 1000
   288   1079.9 MiB    110.8 MiB                   ds = ds.roll(lon=256, roll_coords=True)
   289   1080.0 MiB      0.1 MiB                   ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \
   290   1080.0 MiB      0.0 MiB                                           .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \
   291   1080.0 MiB      0.0 MiB                                           .reset_index(drop=True).squeeze()
   292   1080.0 MiB      0.0 MiB                   ds.coords['lat'] = ds.lat[::-1]
   293   1080.0 MiB      0.0 MiB                   ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   294   1080.0 MiB      0.0 MiB                   return ds
   295                             
   296                                         #make composite of a bunch of GIA runs, i.e. GIA prior
   297                             
   298    612.9 MiB      0.0 MiB               if ice_model == 'glac1d_':
   299                                             path = f'data/glac1d_/output_{model}'
   300                             
   301                                             #make composite of a bunch of GIA runs, i.e. GIA prior
   302                                             ds = one_mod(path, [model])
   303                                             ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   304                                                                    lon=slice(df_place.lon.min() - 2,
   305                                                                              df_place.lon.max() + 2),
   306                                                                    lat=slice(df_place.lat.min() - 2,
   307                                                                              df_place.lat.max() + 2))
   308                             
   309    612.9 MiB      0.0 MiB               elif ice_model == 'd6g_h6g_':
   310    612.9 MiB      0.0 MiB                   path = f'data/d6g_h6g_/output_{model}'
   311                             
   312                                             #make GIA prior std.
   313    785.3 MiB      0.0 MiB                   ds = one_mod(path, [model])
   314    785.3 MiB      0.0 MiB                   ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   315    785.3 MiB      0.0 MiB                                          lon=slice(df_place.lon.min() - 2,
   316    785.3 MiB      0.0 MiB                                                    df_place.lon.max() + 2),
   317    785.3 MiB      0.0 MiB                                          lat=slice(df_place.lat.min() - 2,
   318    785.7 MiB      0.4 MiB                                                    df_place.lat.max() + 2))
   319                             
   320    945.8 MiB    160.1 MiB               ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(
   321    945.7 MiB      0.0 MiB                   age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   322                             
   323                                         # loop through all models to calculate GPR log likelihood
   324                                         # runs = ds.modelrun.values.tolist()
   325    945.7 MiB      0.0 MiB               runs = [f'{ice_model}{lith_thickness}_um{um}_lm{lm}']
   326                             
   327    945.7 MiB      0.0 MiB               modrunlist = []
   328    945.7 MiB      0.0 MiB               loglikelist = []
   329    989.6 MiB      0.0 MiB               for i, modelrun in enumerate(runs):
   330                             
   331    945.7 MiB      0.0 MiB                   print('---------------')
   332    945.7 MiB      0.0 MiB                   print(f'{modelrun} run number {i}')
   333                             
   334    945.7 MiB      0.0 MiB                   if ice_model == 'glac1d_':
   335                                                 # make prior RSL
   336                                                 ds_area = one_mod(path,
   337                                                     [ice_model + lith_thickness]).sel(modelrun=modelrun).rsl.sel(
   338                                                         age=slice(tmax, tmin),
   339                                                         lon=slice(df_place.lon.min() - 2,
   340                                                                   df_place.lon.max() + 2),
   341                                                         lat=slice(df_place.lat.min() - 2,
   342                                                                   df_place.lat.max() + 2)).load().to_dataset().interp(
   343                                                                       age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   344                             
   345                                             else:
   346                                                 # make prior RSL
   347    945.7 MiB      0.0 MiB                       ds_area = one_mod(path,
   348   1080.1 MiB      0.1 MiB                           [ice_model + lith_thickness]).sel(modelrun=modelrun).rsl.sel(
   349   1080.1 MiB      0.0 MiB                               age=slice(tmax, tmin),
   350   1080.1 MiB      0.0 MiB                               lon=slice(df_place.lon.min() - 2,
   351   1080.1 MiB      0.0 MiB                                         df_place.lon.max() + 2),
   352   1080.1 MiB      0.0 MiB                               lat=slice(df_place.lat.min() - 2,
   353    950.0 MiB      0.0 MiB                                         df_place.lat.max() + 2)).load().to_dataset().interp(
   354    950.4 MiB      0.4 MiB                                             age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   355                             
   356                             
   357                                             #sample each model at points where we have RSL data
   358    950.4 MiB      0.0 MiB                   def ds_select(ds):
   359    950.4 MiB      0.0 MiB                       return ds.rsl.sel(age=[row.age],
   360    950.4 MiB      0.0 MiB                                         lon=[row.lon],
   361    950.4 MiB      0.0 MiB                                         lat=[row.lat],
   362    950.4 MiB      0.0 MiB                                         method='nearest').squeeze().values
   363                             
   364                                             #select points at which RSL data exists
   365    950.4 MiB      0.0 MiB                   for i, row in df_place.iterrows():
   366    950.4 MiB      0.0 MiB                       df_place.loc[i, 'rsl_realresid'] = df_place.rsl[i] - ds_select(ds_area)
   367    950.4 MiB      0.0 MiB                       df_place.loc[i, 'rsl_giaprior'] = ds_select(ds_area)
   368    950.4 MiB      0.0 MiB                       df_place.loc[i, 'rsl_giaprior_std'] = ds_select(ds_areastd)
   369                             
   370    950.4 MiB      0.0 MiB                   print('number of datapoints = ', df_place.shape)
   371                             
   372                                             ##################	  RUN GP REGRESSION 	#######################
   373                                             ##################  --------------------	 ######################
   374    950.4 MiB      0.0 MiB                   start = time.time()
   375                             
   376    950.4 MiB      0.0 MiB                   def run_gpr():
   377                             
   378    950.4 MiB      0.0 MiB                       Data = Tuple[tf.Tensor, tf.Tensor]
   379    950.4 MiB      0.0 MiB                       likelihood = df_place.rsl_er_max.ravel()**2 # + df_place.rsl_giaprior_std.ravel()**2  # here we define likelihood
   380                             
   381    950.4 MiB      0.0 MiB                       class GPR_diag(gpf.models.GPModel):
   382                                                     r"""
   383                                                     Gaussian Process Regression.
   384                                                     This is a vanilla implementation of GP regression with a pointwise Gaussian
   385                                                     likelihood.  Multiple columns of Y are treated independently.
   386                                                     The log likelihood of this models is sometimes referred to as the 'marginal log likelihood',
   387                                                     and is given by
   388                                                     .. math::
   389                                                        \log p(\mathbf y \,|\, \mathbf f) =
   390                                                             \mathcal N\left(\mathbf y\,|\, 0, \mathbf K + \sigma_n \mathbf I\right)
   391    950.4 MiB      0.0 MiB                           """
   392    953.4 MiB      0.0 MiB                           def __init__(self,
   393                                                                  data: Data,
   394                                                                  kernel: Kernel,
   395    950.4 MiB      0.0 MiB                                        mean_function: Optional[MeanFunction] = None,
   396    950.4 MiB      0.0 MiB                                        likelihood=likelihood):
   397    953.5 MiB      0.0 MiB                               likelihood = gpf.likelihoods.Gaussian(variance=likelihood)
   398    953.5 MiB      0.0 MiB                               _, y_data = data
   399    953.5 MiB      0.0 MiB                               super().__init__(kernel,
   400    953.5 MiB      0.0 MiB                                                likelihood,
   401    953.5 MiB      0.0 MiB                                                mean_function,
   402    953.5 MiB      0.0 MiB                                                num_latent=y_data.shape[-1])
   403    953.5 MiB      0.0 MiB                               self.data = data
   404                             
   405   1086.0 MiB      0.0 MiB                           def log_likelihood(self):
   406                                                         """
   407                                                         Computes the log likelihood.
   408                                                         """
   409   1086.0 MiB      0.0 MiB                               x, y = self.data
   410   1086.2 MiB      0.6 MiB                               K = self.kernel(x)
   411   1086.2 MiB      0.0 MiB                               num_data = x.shape[0]
   412   1086.2 MiB      0.0 MiB                               k_diag = tf.linalg.diag_part(K)
   413   1086.2 MiB      0.0 MiB                               s_diag = tf.convert_to_tensor(self.likelihood.variance)
   414   1086.2 MiB      0.0 MiB                               jitter = tf.cast(tf.fill([num_data], default_jitter()),
   415   1086.2 MiB      0.0 MiB                                                'float64')  # stabilize K matrix w/jitter
   416   1086.2 MiB      0.0 MiB                               ks = tf.linalg.set_diag(K, k_diag + s_diag + jitter)
   417   1086.2 MiB      0.0 MiB                               L = tf.linalg.cholesky(ks)
   418   1086.2 MiB      0.1 MiB                               m = self.mean_function(x)
   419                             
   420                                                         # [R,] log-likelihoods for each independent dimension of Y
   421   1086.2 MiB      0.0 MiB                               log_prob = multivariate_normal(y, m, L)
   422   1086.2 MiB      0.0 MiB                               return tf.reduce_sum(log_prob)
   423                             
   424    984.0 MiB      0.0 MiB                           def predict_f(self,
   425                                                                   predict_at: tf.Tensor,
   426                                                                   full_cov: bool = False,
   427    950.4 MiB      0.0 MiB                                         full_output_cov: bool = False):
   428                                                         r"""
   429                                                         This method computes predictions at X \in R^{N \x D} input points
   430                                                         .. math::
   431                                                             p(F* | Y)
   432                                                         where F* are points on the GP at new data points, Y are noisy observations at training data points.
   433                                                         """
   434    984.0 MiB      0.0 MiB                               x_data, y_data = self.data
   435    984.0 MiB      0.0 MiB                               err = y_data - self.mean_function(x_data)
   436                             
   437    984.0 MiB      0.0 MiB                               kmm = self.kernel(x_data)
   438    984.0 MiB      0.0 MiB                               knn = self.kernel(predict_at, full=full_cov)
   439   1086.2 MiB    102.1 MiB                               kmn = self.kernel(x_data, predict_at)
   440                             
   441   1086.2 MiB      0.0 MiB                               num_data = x_data.shape[0]
   442   1086.2 MiB      0.0 MiB                               s = tf.linalg.diag(tf.convert_to_tensor(
   443   1086.2 MiB      0.0 MiB                                   self.likelihood.variance))  #changed from normal GPR
   444                             
   445   1086.2 MiB      0.0 MiB                               conditional = gpf.conditionals.base_conditional
   446   1086.2 MiB      0.0 MiB                               f_mean_zero, f_var = conditional(
   447   1086.2 MiB      0.0 MiB                                   kmn, kmm + s, knn, err, full_cov=full_cov,
   448   1086.2 MiB      0.0 MiB                                   white=False)  # [N, P], [N, P] or [P, N, N]
   449   1086.2 MiB      0.0 MiB                               f_mean = f_mean_zero + self.mean_function(predict_at)
   450   1086.2 MiB      0.0 MiB                               return f_mean, f_var
   451                             
   452                             
   453    950.4 MiB      0.0 MiB                       def normalize(df):
   454    950.4 MiB      0.0 MiB                           return np.array((df - df.mean()) / df.std()).reshape(len(df), 1)
   455                             
   456                             
   457   1085.9 MiB      0.0 MiB                       def denormalize(y_pred, df):
   458   1085.9 MiB      0.0 MiB                           return np.array((y_pred * df.std()) + df.mean())
   459                             
   460                             
   461    953.4 MiB      0.0 MiB                       def bounded_parameter(low, high, param):
   462                                                     """Make parameter tfp Parameter with optimization bounds."""
   463    953.4 MiB      0.0 MiB                           affine = tfb.AffineScalar(shift=tf.cast(low, tf.float64),
   464    953.4 MiB      0.0 MiB                                                     scale=tf.cast(high - low, tf.float64))
   465    953.4 MiB      0.0 MiB                           sigmoid = tfb.Sigmoid()
   466    953.4 MiB      0.0 MiB                           logistic = tfb.Chain([affine, sigmoid])
   467    953.4 MiB      0.1 MiB                           parameter = gpf.Parameter(param, transform=logistic, dtype=tf.float64)
   468    953.4 MiB      0.0 MiB                           return parameter
   469                             
   470                             
   471    950.4 MiB      0.0 MiB                       class HaversineKernel_Matern52(gpf.kernels.Matern52):
   472                                                     """
   473                                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   474                                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   475    950.4 MiB      0.0 MiB                           """
   476                                                     def __init__(
   477                                                         self,
   478                                                         lengthscale=1.0,
   479                                                         variance=1.0,
   480    950.4 MiB      0.0 MiB                               active_dims=None,
   481                                                     ):
   482                                                         super().__init__(
   483                                                             active_dims=active_dims,
   484                                                             variance=variance,
   485                                                             lengthscale=lengthscale,
   486                                                         )
   487                             
   488    950.4 MiB      0.0 MiB                           def haversine_dist(self, X, X2):
   489                                                         pi = np.pi / 180
   490                                                         f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   491                                                         f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   492                                                         d = tf.sin((f - f2) / 2)**2
   493                                                         lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   494                                                                     tf.expand_dims(X2[:, 0] * pi, -2)
   495                                                         cos_prod = tf.cos(lat2) * tf.cos(lat1)
   496                                                         a = d[:, :, 0] + cos_prod * d[:, :, 1]
   497                                                         c = tf.asin(tf.sqrt(a)) * 6371 * 2
   498                                                         return c
   499                             
   500    950.4 MiB      0.0 MiB                           def scaled_squared_euclid_dist(self, X, X2):
   501                                                         """
   502                                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   503                                                         """
   504                                                         if X2 is None:
   505                                                             X2 = X
   506                                                         dist = da.square(self.haversine_dist(X, X2) / self.lengthscale)
   507                                                 #             dist = tf.convert_to_tensor(dist)
   508                                                         return dist
   509                             
   510                             
   511    950.4 MiB      0.0 MiB                       class HaversineKernel_Matern32(gpf.kernels.Matern32):
   512                                                     """
   513                                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   514                                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   515    950.4 MiB      0.0 MiB                           """
   516                                                     def __init__(
   517                                                         self,
   518                                                         lengthscale=1.0,
   519                                                         variance=1.0,
   520    950.4 MiB      0.0 MiB                               active_dims=None,
   521                                                     ):
   522                                                         super().__init__(
   523                                                             active_dims=active_dims,
   524                                                             variance=variance,
   525                                                             lengthscale=lengthscale,
   526                                                         )
   527                             
   528    950.4 MiB      0.0 MiB                           def haversine_dist(self, X, X2):
   529                                                         pi = np.pi / 180
   530                                                         f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   531                                                         f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   532                                                         d = tf.sin((f - f2) / 2)**2
   533                                                         lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   534                                                                     tf.expand_dims(X2[:, 0] * pi, -2)
   535                                                         cos_prod = tf.cos(lat2) * tf.cos(lat1)
   536                                                         a = d[:, :, 0] + cos_prod * d[:, :, 1]
   537                                                         c = tf.asin(tf.sqrt(a)) * 6371 * 2
   538                                                         return c
   539                             
   540    950.4 MiB      0.0 MiB                           def scaled_squared_euclid_dist(self, X, X2):
   541                                                         """
   542                                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   543                                                         """
   544                                                         if X2 is None:
   545                                                             X2 = X
   546                                                         dist = tf.square(self.haversine_dist(X, X2) / self.lengthscale)
   547                                                 #             dist = tf.convert_to_tensor(dist) # return to tensorflow
   548                                                         return dist
   549                             
   550                             
   551                                                 ########### Section to Run GPR######################
   552                                                 ##################################3#################
   553                             
   554                                                 # Input space, rsl normalized to zero mean, unit variance
   555    950.4 MiB      0.0 MiB                       X = np.stack((df_place['lon'], df_place['lat'], df_place['age']), 1)
   556    950.4 MiB      0.0 MiB                       RSL = normalize(df_place.rsl_realresid)
   557                             
   558                                                 #define kernels  with bounds
   559                             
   560                                 #                 k1 = HaversineKernel_Matern32(active_dims=[0, 1])
   561                                 #                 k1.lengthscale = bounded_parameter(5000, 30000, 10000)  #hemispheric space
   562                                 #                 k1.variance = bounded_parameter(0.1, 100, 2)
   563                             
   564    953.3 MiB      2.9 MiB                       k1 = gpf.kernels.Matern32(active_dims=[0, 1])
   565    953.4 MiB      0.0 MiB                       k1.lengthscale = bounded_parameter(50, 500, 60)  #hemispheric space
   566    953.4 MiB      0.0 MiB                       k1.variance = bounded_parameter(0.05, 100, 2)
   567                             
   568                                 #                 k2 = HaversineKernel_Matern32(active_dims=[0, 1])
   569                                 #                 k2.lengthscale = bounded_parameter(10, 5000, 100)  #GIA space
   570                                 #                 k2.variance = bounded_parameter(0.1, 100, 2)
   571                             
   572    953.4 MiB      0.0 MiB                       k2 = gpf.kernels.Matern32(active_dims=[0,1])
   573    953.4 MiB      0.0 MiB                       k2.lengthscale = bounded_parameter(1, 50, 5)  #GIA space
   574    953.4 MiB      0.0 MiB                       k2.variance = bounded_parameter(0.05, 100, 2)
   575                             
   576    953.4 MiB      0.0 MiB                       k3 = gpf.kernels.Matern32(active_dims=[2])  #GIA time
   577    953.4 MiB      0.0 MiB                       k3.lengthscale = bounded_parameter(8000, 20000, 10000)
   578    953.4 MiB      0.0 MiB                       k3.variance = bounded_parameter(0.1, 100, 1)
   579                             
   580    953.4 MiB      0.0 MiB                       k4 = gpf.kernels.Matern32(active_dims=[2])  #shorter time
   581    953.4 MiB      0.0 MiB                       k4.lengthscale = bounded_parameter(1, 8000, 1000)
   582    953.4 MiB      0.0 MiB                       k4.variance = bounded_parameter(0.1, 100, 1)
   583                             
   584    953.4 MiB      0.0 MiB                       k5 = gpf.kernels.White(active_dims=[2])
   585    953.4 MiB      0.0 MiB                       k5.variance = bounded_parameter(0.1, 100, 1)
   586                             
   587    953.4 MiB      0.0 MiB                       kernel = (k1 * k3) + (k2 * k4) + k5
   588                             
   589                                                 #build & train model
   590    953.5 MiB      0.0 MiB                       m = GPR_diag((X, RSL), kernel=kernel, likelihood=likelihood)
   591    953.5 MiB      0.0 MiB                       print('model built, time=', time.time() - start)
   592                             
   593                             
   594    953.5 MiB      0.0 MiB                       @tf.function(autograph=False)
   595                                                 def objective():
   596    954.0 MiB      0.0 MiB                           return - m.log_marginal_likelihood()
   597                             
   598    953.5 MiB      0.0 MiB                       o = gpf.optimizers.Scipy()
   599    983.6 MiB     29.7 MiB                       o.minimize(objective, variables=m.trainable_variables)
   600    983.6 MiB      0.0 MiB                       print('model minimized, time=', time.time() - start)
   601                             
   602                                                 # output space
   603    983.6 MiB      0.0 MiB                       nout = 50
   604    983.7 MiB      0.0 MiB                       lat = np.linspace(min(ds_area.lat), max(ds_area.lat), nout)
   605    983.7 MiB      0.0 MiB                       lon = np.linspace(min(ds_area.lon), max(ds_area.lon), nout)
   606    983.7 MiB      0.0 MiB                       ages = ages_lgm[(ages_lgm < tmax) & (ages_lgm > tmin)]
   607    984.0 MiB      0.4 MiB                       xyt = np.array(list(product(lon, lat, ages)))
   608                             
   609                                                 #query model & renormalize data
   610   1085.9 MiB      0.0 MiB                       y_pred, var = m.predict_f(xyt)
   611   1085.9 MiB      0.0 MiB                       y_pred_out = denormalize(y_pred, df_place.rsl_realresid)
   612                             
   613                                                 #reshape output vectors
   614                             #                    Xlon = np.array(xyt[:, 0]).reshape((nout, nout, len(ages)))
   615                              #                   Xlat = np.array(xyt[:, 1]).reshape((nout, nout, len(ages)))
   616   1085.9 MiB      0.0 MiB                       Zp = np.array(y_pred_out).reshape(nout, nout, len(ages))
   617   1086.0 MiB      0.1 MiB                       varp = np.array(var).reshape(nout, nout, len(ages))
   618                             
   619                                                 #print kernel details
   620                                             #     print_summary(m, fmt='notebook')
   621   1086.0 MiB      0.0 MiB                       print('time elapsed = ', time.time() - start)
   622                             
   623   1086.0 MiB      0.0 MiB                       print('negative log marginal likelihood =',
   624   1085.9 MiB      0.0 MiB                             m.neg_log_marginal_likelihood().numpy())
   625                             
   626                             
   627   1085.9 MiB      0.0 MiB                       modrunlist.append(modelrun)
   628   1085.9 MiB      0.0 MiB                       loglikelist.append(m.neg_log_marginal_likelihood().numpy())
   629                             
   630                             
   631                             
   632                                                 ##################	  INTERPOLATE MODELS 	#######################
   633                                                 ##################  --------------------	 ######################
   634                             
   635                                                 # turn GPR output into xarray dataarray
   636   1085.9 MiB      0.0 MiB                       da_zp = xr.DataArray(Zp, coords=[lon, lat, ages],
   637   1085.9 MiB      0.0 MiB                                            dims=['lon', 'lat',
   638   1085.9 MiB      0.0 MiB                                                  'age']).transpose('age', 'lat', 'lon')
   639   1085.9 MiB      0.0 MiB                       da_varp = xr.DataArray(varp,
   640   1085.9 MiB      0.0 MiB                                              coords=[lon, lat, ages],
   641   1085.9 MiB      0.0 MiB                                              dims=['lon', 'lat',
   642   1085.9 MiB      0.0 MiB                                                    'age']).transpose('age', 'lat', 'lon')
   643                             
   644                             
   645   1085.9 MiB      0.0 MiB                       def interp_likegpr(ds):
   646   1087.1 MiB      1.2 MiB                           return ds.rsl.load().transpose().interp_like(da_zp)
   647                             
   648                             
   649                                                 #interpolate all models onto GPR grid
   650   1087.1 MiB      0.0 MiB                       da_giapriorinterp = interp_likegpr(ds_area)
   651   1043.8 MiB      0.0 MiB                       ds_giapriorinterp = ds_area.interp(age=ages)
   652   1045.2 MiB      0.0 MiB                       da_giapriorinterpstd = interp_likegpr(ds_areastd)
   653                             
   654                                                 # add total prior RSL back into GPR
   655   1045.2 MiB      0.0 MiB                       da_priorplusgpr = da_zp + da_giapriorinterp
   656                             
   657   1045.2 MiB      0.0 MiB                       return ages, da_zp, da_giapriorinterp, da_priorplusgpr, da_varp, modrunlist, loglikelist
   658                             
   659   1015.9 MiB      0.0 MiB                   ages, da_zp, da_giapriorinterp, da_priorplusgpr, da_varp, modrunlist, loglikelist = run_gpr()
   660                                             ##################	  	 SAVE NETCDFS 	 	#######################
   661                                             ##################  --------------------	 ######################
   662                             
   663   1015.9 MiB      0.0 MiB                   path_gen = f'{ages[0]}_{ages[-1]}_{model}_{place}'
   664   1002.2 MiB      0.0 MiB                   da_zp.to_netcdf('output/' + path_gen + '_da_zp')
   665    987.4 MiB      0.0 MiB                   da_giapriorinterp.to_netcdf('output/' + path_gen + '_giaprior')
   666    989.4 MiB      2.0 MiB                   da_priorplusgpr.to_netcdf('output/' + path_gen + '_posterior')
   667    989.4 MiB      0.0 MiB                   da_varp.to_netcdf('output/' + path_gen + '_gp_variance')
   668                             
   669                                             ##################		  PLOT  MODELS 		#######################
   670                                             ##################  --------------------	 ######################
   671    989.4 MiB      0.0 MiB                   dirName = f'figs/{place}/'
   672    989.4 MiB      0.0 MiB                   if not os.path.exists(dirName):
   673                                                 os.mkdir(dirName)
   674                                                 print("Directory ", dirName, " Created ")
   675                                             else:
   676                                                 pass
   677                                     #             print("Directory ", dirName, " already exists")
   678                             
   679    989.4 MiB      0.0 MiB                   if plotting == 'true':
   680                                                 for i, age in enumerate(ages):
   681                                                     if (age / 500).is_integer():
   682                                                         step = (ages[0] - ages[1])
   683                                                         df_it = df_place[(df_place.age < age) & (df_place.age > age - step)]
   684                                                         resid_it = da_zp.sel(age=slice(age, age - step))
   685                                                         rsl, var = df_it.rsl, df_it.rsl_er_max.values**2
   686                                                         lat_it, lon_it = df_it.lat, df_it.lon
   687                                                         vmin = ds_giapriorinterp.rsl.min().values  # + 10
   688                                                         vmax = ds_giapriorinterp.rsl.max().values  # - 40
   689                                                         vmin_std = 0
   690                                                         vmax_std = 1
   691                                                         tmin_it = np.round(age - step, 2)
   692                                                         tmax_it = np.round(age, 2)
   693                                                         cbarscale = 0.3
   694                                                         fontsize = 20
   695                                                         cmap = 'coolwarm'
   696                                                         cbar_kwargs = {'shrink': cbarscale, 'label': 'RSL (m)'}
   697                             
   698                                                         proj = ccrs.PlateCarree()
   699                                                         projection = ccrs.PlateCarree()
   700                                                         fig, (ax1, ax2, ax3,
   701                                                               ax4) = plt.subplots(1,
   702                                                                                   4,
   703                                                                                   figsize=(24, 16),
   704                                                                                   subplot_kw=dict(projection=projection))
   705                             
   706                                                         # total prior mean + "true" data
   707                                                         ax1.coastlines(color='k')
   708                                                         pc1 = ds_giapriorinterp.rsl[i].transpose().plot(ax=ax1,
   709                                                                                                         transform=proj,
   710                                                                                                         cmap=cmap,
   711                                                                                                         norm=MidpointNormalize(
   712                                                                                                             vmin, vmax, 0),
   713                                                                                                         add_colorbar=False,
   714                                                                                                         extend='both')
   715                                                         cbar = fig.colorbar(pc1,
   716                                                                             ax=ax1,
   717                                                                             shrink=.3,
   718                                                                             label='RSL (m)',
   719                                                                             extend='both')
   720                                                         scat = ax1.scatter(lon_it,
   721                                                                            lat_it,
   722                                                                            s=80,
   723                                                                            c=rsl,
   724                                                                            edgecolor='k',
   725                                                                            vmin=vmin,
   726                                                                            vmax=vmax,
   727                                                                            norm=MidpointNormalize(vmin, vmax, 0),
   728                                                                            cmap=cmap)
   729                                                         ax1.set_title(f'{np.round(ds_giapriorinterp.rsl[i].age.values, -1)} yrs',
   730                                                                       fontsize=fontsize)
   731                                                         #         ax1.set_extent(extent_)
   732                             
   733                                                         # Learned difference between prior and "true" data
   734                                                         ax2.coastlines(color='k')
   735                                                         pc = da_zp[i, :, :].plot(ax=ax2,
   736                                                                                  transform=proj,
   737                                                                                  cmap=cmap,
   738                                                                                  extend='both',
   739                                                                                  norm=MidpointNormalize(
   740                                                                                      resid_it.min(), resid_it.max(), 0),
   741                                                                                  add_colorbar=False)
   742                                                         cbar = fig.colorbar(pc,
   743                                                                             ax=ax2,
   744                                                                             shrink=.3,
   745                                                                             label='RSL (m)',
   746                                                                             extend='both')
   747                                                         scat = ax2.scatter(lon_it,
   748                                                                            lat_it,
   749                                                                            s=80,
   750                                                                            facecolors='k',
   751                                                                            cmap=cmap,
   752                                                                            edgecolor='k',
   753                                                                            transform=proj,
   754                                                                            norm=MidpointNormalize(resid_it.min(),
   755                                                                                                   resid_it.max(), 0))
   756                                                         ax2.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   757                                                         #         ax2.set_extent(extent_)
   758                             
   759                                                         # GP regression
   760                                                         ax3.coastlines(color='k')
   761                                                         pc = da_priorplusgpr[i].plot(ax=ax3,
   762                                                                                      transform=proj,
   763                                                                                      norm=MidpointNormalize(vmin, vmax, 0),
   764                                                                                      cmap=cmap,
   765                                                                                      extend='both',
   766                                                                                      add_colorbar=False)
   767                                                         scat = ax3.scatter(lon_it,
   768                                                                            lat_it,
   769                                                                            s=80,
   770                                                                            c=rsl,
   771                                                                            edgecolor='k',
   772                                                                            cmap=cmap,
   773                                                                            norm=MidpointNormalize(vmin, vmax, 0))
   774                                                         cbar = fig.colorbar(pc,
   775                                                                             ax=ax3,
   776                                                                             shrink=.3,
   777                                                                             label='RSL (m)',
   778                                                                             extend='both')
   779                                                         ax3.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   780                                                         #         ax3.set_extent(extent_)
   781                             
   782                                                         #GP regression standard deviation
   783                                                         ax4.coastlines(color='k')
   784                                                         pc = (2 * np.sqrt(da_varp[i])).plot(
   785                                                             ax=ax4,
   786                                                             transform=proj,
   787                                                             vmin=vmin_std,
   788                                                             vmax=vmax_std * 2,
   789                                                             cmap='Reds',
   790                                                             extend='both',
   791                                                             add_colorbar=False,
   792                                                         )
   793                                                         scat = ax4.scatter(lon_it,
   794                                                                            lat_it,
   795                                                                            s=80,
   796                                                                            c=2 * np.sqrt(var),
   797                                                                            vmin=vmin_std,
   798                                                                            vmax=vmax_std * 2,
   799                                                                            cmap='Reds',
   800                                                                            edgecolor='k',
   801                                                                            transform=proj)
   802                                                         cbar = fig.colorbar(pc,
   803                                                                             ax=ax4,
   804                                                                             shrink=.3,
   805                                                                             extend='both',
   806                                                                             label='RSL (m) (2 $\sigma$)')
   807                                                         ax4.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   808                                                 #         ax4.set_extent(extent_)
   809                             
   810                                                 ########## ----- Save figures -------- #######################
   811                                                     fig.savefig(dirName + f'{path_gen}_{age}_3D_fig', transparent=True)
   812                             
   813                                                 ##################	CHOOSE LOCS W/NUF SAMPS #######################
   814                                                 ##################  --------------------	 ######################
   815                             
   816                             
   817                                                 def locs_with_enoughsamples(df_place, place, number):
   818                                                     """make new dataframe, labeled, of sites with [> number] measurements"""
   819                                                     df_lots = df_place.groupby(['lat',
   820                                                                                 'lon']).filter(lambda x: len(x) > number)
   821                             
   822                                                     df_locs = []
   823                                                     for i, group in enumerate(df_lots.groupby(['lat', 'lon'])):
   824                                                         singleloc = group[1].copy()
   825                                                         singleloc['location'] = place
   826                                                         singleloc['locnum'] = place + '_site' + str(
   827                                                             i)  # + singleloc.reset_index().index.astype('str')
   828                                                         df_locs.append(singleloc)
   829                                                     df_locs = pd.concat(df_locs)
   830                             
   831                                                     return df_locs
   832                             
   833                             
   834                                                 number = 6
   835                                                 df_nufsamps = locs_with_enoughsamples(df_place, place, number)
   836                                                 len(df_nufsamps.locnum.unique())
   837                             
   838                                                 ##################	PLOT LOCS W/NUF SAMPS   #######################
   839                                                 ##################  --------------------	 ######################
   840                             
   841                             
   842                                                 def slice_dataarray(da):
   843                                                     return da.sel(lat=site[1].lat.unique(),
   844                                                                   lon=site[1].lon.unique(),
   845                                                                   method='nearest')
   846                             
   847                             
   848                                                 fig, ax = plt.subplots(1, len(df_nufsamps.locnum.unique()), figsize=(18, 4))
   849                                                 ax = ax.ravel()
   850                                                 colors = ['darkgreen', 'darkblue', 'darkred']
   851                                                 fontsize = 18
   852                             
   853                                                 for i, site in enumerate(df_nufsamps.groupby('locnum')):
   854                             
   855                                                     #slice data for each site
   856                                                     prior_it = slice_dataarray(da_giapriorinterp)
   857                                                     priorvar_it = slice_dataarray(da_giapriorinterpstd)
   858                                                     top_prior = prior_it + priorvar_it * 2
   859                                                     bottom_prior = prior_it - priorvar_it * 2
   860                             
   861                                                     var_it = slice_dataarray(np.sqrt(da_varp))
   862                                                     post_it = slice_dataarray(da_priorplusgpr)
   863                                                     top = post_it + var_it * 2
   864                                                     bottom = post_it - var_it * 2
   865                             
   866                                                     site_err = 2 * (site[1].rsl_er_max)
   867                             
   868                                                     ax[i].scatter(site[1].age, site[1].rsl, c=colors[0], label='"true" RSL')
   869                                                     ax[i].errorbar(
   870                                                         site[1].age,
   871                                                         site[1].rsl,
   872                                                         site_err,
   873                                                         c=colors[0],
   874                                                         fmt='none',
   875                                                         capsize=1,
   876                                                         lw=1,
   877                                                     )
   878                             
   879                                                     prior_it.plot(ax=ax[i], c=colors[2], label='Prior $\pm 2 \sigma$')
   880                                                     ax[i].fill_between(prior_it.age,
   881                                                                        bottom_prior.squeeze(),
   882                                                                        top_prior.squeeze(),
   883                                                                        color=colors[2],
   884                                                                        alpha=0.3)
   885                             
   886                                                     post_it.plot(ax=ax[i], c=colors[1], label='Posterior $\pm 2 \sigma$')
   887                                                     ax[i].fill_between(post_it.age,
   888                                                                        bottom.squeeze(),
   889                                                                        top.squeeze(),
   890                                                                        color=colors[1],
   891                                                                        alpha=0.3)
   892                                                     #     ax[i].set_title(f'{site[0]} RSL', fontsize=fontsize)
   893                                                     ax[i].set_title('')
   894                             
   895                                                     ax[i].legend(loc='lower left')
   896                             
   897                                                 fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_realdata_fig_1D',
   898                                                             transparent=True)
   899                             
   900                                                 #plot locations of data
   901                                                 fig, ax = plt.subplots(1,
   902                                                                        len(df_nufsamps.locnum.unique()),
   903                                                                        figsize=(18, 4),
   904                                                                        subplot_kw=dict(projection=projection))
   905                                                 ax = ax.ravel()
   906                             
   907                                                 da_zeros = xr.zeros_like(da_zp)
   908                             
   909                                                 for i, site in enumerate(df_nufsamps.groupby('locnum')):
   910                                                     ax[i].coastlines(color='k')
   911                                                     ax[i].plot(site[1].lon.unique(),
   912                                                                site[1].lat.unique(),
   913                                                                c=colors[0],
   914                                                                ms=7,
   915                                                                marker='o',
   916                                                                transform=proj)
   917                                                     ax[i].plot(site[1].lon.unique(),
   918                                                                site[1].lat.unique(),
   919                                                                c=colors[0],
   920                                                                ms=25,
   921                                                                marker='o',
   922                                                                transform=proj,
   923                                                                mfc="None",
   924                                                                mec='red',
   925                                                                mew=4)
   926                                                     da_zeros[0].plot(ax=ax[i], cmap='Greys', add_colorbar=False)
   927                                                     ax[i].set_title(site[0], fontsize=fontsize)
   928                             
   929                                                 fig.savefig(dirName + f'{path_gen}_1Dlocs_fig', transparent=True)
   930                             
   931                                                 #################   DECOMPOSE GPR INTO KERNELS ####################
   932                                                 ##################  --------------------	 ######################
   933                             
   934                                                 if decomp == 'true':
   935                             
   936                                                     def predict_decomp_f(m,
   937                                                                          custom_kernel,
   938                                                                          predict_at: tf.Tensor,
   939                                                                          full_cov: bool = False,
   940                                                                          full_output_cov: bool = False,
   941                                                                          var=None):
   942                                                         """Decompose GP into individual kernels."""
   943                             
   944                                                         x_data, y_data = m.data
   945                                                         err = y_data - m.mean_function(x_data)
   946                                                         kmm = m.kernel(x_data)
   947                                                         knn = custom_kernel(predict_at, full=full_cov)
   948                                                         kmn = custom_kernel(x_data, predict_at)
   949                                                         num_data = x_data.shape[0]
   950                                                         s = tf.linalg.diag(tf.convert_to_tensor(var))  # added diagonal variance
   951                                                         conditional = gpf.conditionals.base_conditional
   952                                                         f_mean_zero, f_var = conditional(
   953                                                             kmn, kmm + s, knn, err, full_cov=full_cov,
   954                                                             white=False)  # [N, P], [N, P] or [P, N, N]
   955                                                         f_mean = np.array(f_mean_zero + m.mean_function(predict_at))
   956                                                         f_var = np.array(f_var)
   957                                                         return f_mean, f_var
   958                             
   959                             
   960                                                     def reshape_decomp(k, var=None):
   961                                                         A, var = predict_decomp_f(m, k, xyt, var=var)
   962                                                         A = A.reshape(nout, nout, len(ages))
   963                                                         var = var.reshape(nout, nout, len(ages))
   964                                                         return A, var
   965                             
   966                             
   967                                                     def make_dataarray(da):
   968                                                         coords = [lon, lat, ages]
   969                                                         dims = ['lon', 'lat', 'age']
   970                                                         return xr.DataArray(da, coords=coords,
   971                                                                             dims=dims).transpose('age', 'lat', 'lon')
   972                             
   973                             
   974                                                     A1, var1 = reshape_decomp(k1,
   975                                                                               var=df_place.rsl_er_max.ravel()**2)  #gia spatial
   976                                                     A2, var2 = reshape_decomp(k2,
   977                                                                               var=df_place.rsl_er_max.ravel()**2)  #gia temporal
   978                                                     A3, var3 = reshape_decomp(
   979                                                         k3,
   980                                                         var=df_place.rsl_er_max.ravel()**2)  #readvance spatial
   981                                                     A4, var4 = reshape_decomp(
   982                                                         k4,
   983                                                         var=df_place.rsl_er_max.ravel()**2)  #readvance temporal
   984                                                     A5, var5 = reshape_decomp(
   985                                                         k5,
   986                                                         var=df_place.rsl_er_max.ravel()**2)  #readvance spatial
   987                             
   988                                                     da_A1 = make_dataarray(A1)
   989                                                     da_var1 = make_dataarray(var1)
   990                             
   991                                                     da_A2 = make_dataarray(A2)
   992                                                     da_var2 = make_dataarray(var2)
   993                             
   994                                                     da_A3 = make_dataarray(A3)
   995                                                     da_var3 = make_dataarray(var3)
   996                             
   997                                                     da_A4 = make_dataarray(A4)
   998                                                     da_var4 = make_dataarray(var4)
   999                             
  1000                                                     da_A5 = make_dataarray(A5)
  1001                                                     da_var5 = make_dataarray(var5)
  1002                             
  1003                                                     #################   PLOT DECOMPOSED KERNELS    ####################
  1004                                                     ##################  --------------------	   ####################
  1005                             
  1006                                                     fig, ax = plt.subplots(1, 6, figsize=(24, 4))
  1007                                                     ax = ax.ravel()
  1008                                                     da_A1[0, :, :].plot(ax=ax[0], cmap='RdBu_r')
  1009                             
  1010                                                     da_A2[0, :, :].plot(ax=ax[1], cmap='RdBu_r')
  1011                             
  1012                                                     da_A3[0, :, :].plot(ax=ax[2], cmap='RdBu_r')
  1013                             
  1014                                                     da_A4[:, 0, 0].plot(ax=ax[3])
  1015                             
  1016                                                     da_A5[:, 0, 0].plot(ax=ax[4])
  1017                             
  1018                                                     fig.savefig(dirName + f'{path_gen}_decompkernels', transparent=True)
  1019                                                 else:
  1020                                                     pass
  1021                             
  1022                                             else:
  1023                                                 pass
  1024                             
  1025                                         #store log likelihood in dataframe
  1026    989.4 MiB      0.0 MiB                   df_out = pd.DataFrame({'modelrun': modrunlist,
  1027    989.4 MiB      0.0 MiB                                    'log_marginal_likelihood': loglikelist})
  1028                             
  1029                             
  1030    989.4 MiB      0.0 MiB                   writepath = f'output/{path_gen}_loglikelihood'
  1031    989.4 MiB      0.0 MiB                   df_out.to_csv(writepath, index=False)
  1032    989.6 MiB      0.2 MiB                   df_likes = pd.read_csv(writepath)
  1033                             
  1034                                         # make heatmap for upper vs. lower mantle viscosities at one lithosphere thickness
  1035                             
  1036    989.6 MiB      0.0 MiB               if ice_model =='glac1d_':
  1037                                             df_likes['um'] = [key.split('_')[2][3:] for key in df_likes.modelrun]
  1038                                             df_likes['lm'] = [key.split('_')[3][2:] for key in df_likes.modelrun]
  1039                                             df_likes['lith'] = [key.split('_')[1][1:3] for key in df_likes.modelrun]
  1040                                             df_likes['icemodel'] = [key.split('_')[0] for key in df_likes.modelrun]
  1041    989.6 MiB      0.0 MiB               elif ice_model == 'd6g_h6g_':
  1042                             #                 df_likes = df_likes.drop([36])
  1043    989.6 MiB      0.0 MiB                   df_likes['um'] = [key.split('_')[3][3:] for key in df_likes.modelrun]
  1044    989.6 MiB      0.0 MiB                   df_likes['lm'] = [key.split('_')[4][2:] for key in df_likes.modelrun]
  1045    989.6 MiB      0.0 MiB                   df_likes['lith'] = [key.split('_')[2][1:3] for key in df_likes.modelrun]
  1046    989.6 MiB      0.0 MiB                   df_likes['icemodel'] = [key.split('_l')[0] for key in df_likes.modelrun]
  1047                             
  1048    989.6 MiB      0.0 MiB               df_likes.lm = df_likes.lm.astype(float)
  1049    989.6 MiB      0.0 MiB               df_likes.um = df_likes.um.astype(float)
  1050    989.8 MiB      0.1 MiB               heatmap = df_likes.pivot_table(index='um', columns='lm', values='log_marginal_likelihood')
  1051                             
  1052                             
  1053    989.8 MiB      0.0 MiB               fig, ax = plt.subplots(1, 1, figsize=(6, 6))
  1054    991.5 MiB      1.7 MiB               sns.heatmap(heatmap,  cmap='coolwarm', ax=ax,  cbar_kws={'label': 'negative log likelihood'})
  1055    991.5 MiB      0.0 MiB               ax.set_title(f'{place} {ages[0]} - {ages[-1]} yrs \n {ice_model} : {df_likes.lith[0]} km lithosphere'); # (havsine)
  1056                             
  1057    991.6 MiB      0.1 MiB               fig.savefig(dirName + f'{path_gen}_likelihood_heatmap', transparent=True)   # _havsine


