model built, time= 0.16888999938964844
model minimized, time= 60.39790177345276
<IPython.core.display.HTML object>
time elapsed =  71.84483599662781
negative log marginal likelihood = 266.28788989451965
Directory  figs/europe/  already exists
Filename: readv_012920_realdata_europe_glac1d.py

Line #    Mem usage    Increment   Line Contents
================================================
    39    294.3 MiB    294.3 MiB   @profile
    40                             def readv():
    41                                 
    42                                 # set the colormap and centre the colorbar
    43    294.3 MiB      0.0 MiB       class MidpointNormalize(Normalize):
    44    294.3 MiB      0.0 MiB           """Normalise the colorbar.  e.g. norm=MidpointNormalize(mymin, mymax, 0.)"""
    45    657.9 MiB      0.0 MiB           def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
    46    657.9 MiB      0.0 MiB               self.midpoint = midpoint
    47    657.9 MiB      0.0 MiB               Normalize.__init__(self, vmin, vmax, clip)
    48                             
    49    678.9 MiB     15.2 MiB           def __call__(self, value, clip=None):
    50    678.9 MiB      0.0 MiB               x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
    51    678.9 MiB      0.0 MiB               return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))
    52                             
    53                             
    54                                 ####################  Initialize parameters #######################
    55                                 #################### ---------------------- #######################
    56                             
    57    294.3 MiB      0.0 MiB       ice_model = 'd6g_h6g_'  #'glac1d_'
    58    294.3 MiB      0.0 MiB       lith_thickness = 'l7'  # 'l90C'
    59    294.3 MiB      0.0 MiB       place = 'europe'
    60                             
    61                                 locs = {
    62    294.3 MiB      0.0 MiB           'england': [-12, 2, 50, 60],
    63    294.3 MiB      0.0 MiB           'southchina': [110, 117, 19, 23],
    64    294.3 MiB      0.0 MiB           'easternhem': [50, 178, -45, 80],
    65    294.3 MiB      0.0 MiB           'westernhem': [-175, 30, -80, 75],
    66    294.3 MiB      0.0 MiB           'world': [-179.8, 179.8, -89.8, 89.8],
    67    294.3 MiB      0.0 MiB           'namerica': [-150, -20, 10, 75],
    68    294.3 MiB      0.0 MiB           'eastcoast': [-88, -65, 15, 40],
    69    294.3 MiB      0.0 MiB           'europe': [-20, 15, 35, 70]
    70                                 }
    71    294.3 MiB      0.0 MiB       extent = locs[place]
    72    294.3 MiB      0.0 MiB       tmax, tmin, tstep = 7050, 4450, 100
    73                             
    74    294.3 MiB      0.0 MiB       ages_lgm = np.arange(100, 26000, tstep)[::-1]
    75                             
    76                                 #import khan dataset
    77    294.3 MiB      0.0 MiB       path = 'data/GSL_LGM_120519_.csv'
    78                             
    79    312.6 MiB     18.3 MiB       df = pd.read_csv(path, encoding="ISO-8859-15", engine='python')
    80    317.6 MiB      5.0 MiB       df = df.replace('\s+', '_', regex=True).replace('-', '_', regex=True).\
    81    320.5 MiB      0.4 MiB               applymap(lambda s:s.lower() if type(s) == str else s)
    82    320.5 MiB      0.0 MiB       df.columns = df.columns.str.lower()
    83    320.5 MiB      0.0 MiB       df.rename_axis('index', inplace=True)
    84    320.6 MiB      0.1 MiB       df = df.rename({'latitude': 'lat', 'longitude': 'lon'}, axis='columns')
    85    320.6 MiB      0.0 MiB       dfind, dfterr, dfmar = df[(df.type == 0)
    86    320.8 MiB      0.1 MiB                                 & (df.age > 0)], df[df.type == 1], df[df.type == -1]
    87    320.8 MiB      0.0 MiB       np.sort(list(set(dfind.regionname1)))
    88                             
    89                                 #select location
    90    320.8 MiB      0.0 MiB       df_place = dfind[(dfind.age > tmin) & (dfind.age < tmax) &
    91                                                  (dfind.lon > extent[0])
    92                                                  & (dfind.lon < extent[1])
    93                                                  & (dfind.lat > extent[2])
    94                                                  & (dfind.lat < extent[3])
    95    320.8 MiB      0.0 MiB                        & (dfind.rsl_er_max < 1)][[
    96    320.9 MiB      0.1 MiB                            'lat', 'lon', 'rsl', 'rsl_er_max', 'age'
    97                                                  ]]
    98                                 # & (df_place.rsl_er_max < 1)
    99    320.9 MiB      0.0 MiB       df_place.shape
   100                             
   101                                 ####################  	Plot locations  	#######################
   102                                 #################### ---------------------- #######################
   103                             
   104                                 #get counts by location rounded to nearest 0.1 degree
   105    320.9 MiB      0.0 MiB       df_rnd = df_place.copy()
   106    320.9 MiB      0.0 MiB       df_rnd.lat = np.round(df_rnd.lat, 1)
   107    320.9 MiB      0.0 MiB       df_rnd.lon = np.round(df_rnd.lon, 1)
   108    320.9 MiB      0.0 MiB       dfcounts_place = df_rnd.groupby(
   109    321.0 MiB      0.1 MiB           ['lat', 'lon']).count().reset_index()[['lat', 'lon', 'rsl', 'age']]
   110                             
   111                                 #plot
   112    327.2 MiB      6.1 MiB       fig = plt.figure(figsize=(10, 7))
   113    327.8 MiB      0.6 MiB       ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())
   114                             
   115    328.0 MiB      0.2 MiB       ax.set_extent(extent)
   116    328.0 MiB      0.0 MiB       ax.coastlines(resolution='110m', linewidth=1, zorder=2)
   117    328.0 MiB      0.0 MiB       ax.add_feature(cfeature.OCEAN, zorder=0)
   118    328.0 MiB      0.0 MiB       ax.add_feature(cfeature.LAND, color='palegreen', zorder=1)
   119    328.0 MiB      0.0 MiB       ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=3)
   120    328.0 MiB      0.0 MiB       ax.gridlines(linewidth=1, color='white', alpha=0.5, zorder=4)
   121    328.0 MiB      0.0 MiB       scat = ax.scatter(dfcounts_place.lon,
   122    328.0 MiB      0.0 MiB                         dfcounts_place.lat,
   123    328.0 MiB      0.0 MiB                         s=dfcounts_place.rsl * 70,
   124    328.0 MiB      0.0 MiB                         c='lightsalmon',
   125    328.0 MiB      0.0 MiB                         vmin=-20,
   126    328.0 MiB      0.0 MiB                         vmax=20,
   127    328.0 MiB      0.0 MiB                         cmap='coolwarm',
   128    328.0 MiB      0.0 MiB                         edgecolor='k',
   129    328.0 MiB      0.0 MiB                         linewidths=1,
   130    328.1 MiB      0.0 MiB                         transform=ccrs.PlateCarree(),
   131    328.7 MiB      0.6 MiB                         zorder=5)
   132    328.7 MiB      0.0 MiB       size = Line2D(range(4),
   133    328.7 MiB      0.0 MiB                     range(4),
   134    328.7 MiB      0.0 MiB                     color="black",
   135    328.7 MiB      0.0 MiB                     marker='o',
   136    328.7 MiB      0.0 MiB                     linewidth=0,
   137    328.7 MiB      0.0 MiB                     linestyle='none',
   138    328.7 MiB      0.0 MiB                     markersize=16,
   139    328.7 MiB      0.0 MiB                     markerfacecolor="lightsalmon")
   140    328.7 MiB      0.0 MiB       labels = ['RSL datapoint location']
   141    328.7 MiB      0.0 MiB       leg = plt.legend([size],
   142    328.7 MiB      0.0 MiB                        labels,
   143    328.7 MiB      0.0 MiB                        loc='lower left',
   144    328.7 MiB      0.0 MiB                        bbox_to_anchor=(0.00, 0.00),
   145    328.7 MiB      0.0 MiB                        prop={'size': 20},
   146    328.7 MiB      0.1 MiB                        fancybox=True)
   147    328.7 MiB      0.0 MiB       leg.get_frame().set_edgecolor('k')
   148    328.7 MiB      0.0 MiB       ax.set_title('')
   149                             
   150                                 ####################  Make 3D fingerprint  #######################
   151                                 #################### ---------------------- #######################
   152                             
   153    328.7 MiB      0.0 MiB       filename = 'data/WAISreadvance_VM5_6ka_1step.mat'
   154                             
   155    327.7 MiB      0.0 MiB       waismask = io.loadmat(filename, squeeze_me=True)
   156    327.7 MiB      0.0 MiB       ds_mask = xr.Dataset({'rsl': (['lat', 'lon', 'age'], waismask['RSL'])},
   157                                                      coords={
   158    327.7 MiB      0.0 MiB                                'lon': waismask['lon_out'],
   159    327.7 MiB      0.0 MiB                                'lat': waismask['lat_out'],
   160    327.7 MiB      0.0 MiB                                'age': np.round(waismask['ice_time_new'])
   161                                                      })
   162    327.8 MiB      0.1 MiB       fingerprint = ds_mask.sel(age=ds_mask.age[0])
   163                             
   164                             
   165    601.3 MiB      0.0 MiB       def make_fingerprint(start, end, maxscale):
   166                             
   167                                     #palindromic scaling vector
   168    601.3 MiB      0.0 MiB           def palindrome(maxscale, ages):
   169                                         """ Make palindrome scale 0-maxval with number of steps. """
   170    601.3 MiB      0.0 MiB               half = np.linspace(0, maxscale, 1 + (len(ages) - 1) // 2)
   171    601.3 MiB      0.0 MiB               scalefactor = np.concatenate([half, half[::-1]])
   172    601.3 MiB      0.0 MiB               return scalefactor
   173                             
   174    601.3 MiB      0.0 MiB           ages_readv = ages_lgm[(ages_lgm < start) & (ages_lgm >= end)]
   175    601.3 MiB      0.0 MiB           scale = palindrome(maxscale, ages_readv)
   176                             
   177                                     #scale factor same size as ice model ages
   178    601.3 MiB      0.0 MiB           pre = np.zeros(np.where(ages_lgm == start)[0])
   179    601.3 MiB      0.0 MiB           post = np.zeros(len(ages_lgm) - len(pre) - len(scale))
   180                             
   181    601.3 MiB      0.0 MiB           readv_scale = np.concatenate([pre, scale, post])
   182                             
   183                                     #scale factor into dataarray
   184    601.3 MiB      0.0 MiB           da_scale = xr.DataArray(readv_scale, coords=[('age', ages_lgm)])
   185                             
   186                                     # broadcast fingerprint & scale to same dimensions;
   187    601.3 MiB      0.0 MiB           fingerprint_out, fing_scaled = xr.broadcast(fingerprint.rsl, da_scale)
   188                             
   189                                     # mask fingerprint with scale to get LGM-pres timeseries
   190    601.3 MiB      0.0 MiB           ds_fingerprint = (fingerprint_out *
   191    860.4 MiB    259.1 MiB                             fing_scaled).transpose().to_dataset(name='rsl')
   192                             
   193                                     # scale dataset with fingerprint to LGM-present length & 0-max-0 over x years
   194    860.4 MiB      0.0 MiB           xrlist = []
   195   1103.4 MiB      0.0 MiB           for i, key in enumerate(da_scale):
   196   1103.4 MiB      1.1 MiB               mask = ds_fingerprint.sel(age=ds_fingerprint.age[i].values) * key
   197   1103.4 MiB      0.0 MiB               mask = mask.assign_coords(scale=key,
   198   1103.4 MiB      0.0 MiB                                         age=ages_lgm[i]).expand_dims(dim=['age'])
   199   1103.4 MiB      0.0 MiB               xrlist.append(mask)
   200   1362.5 MiB    259.3 MiB           ds_readv = xr.concat(xrlist, dim='age')
   201                             
   202   1362.5 MiB      0.0 MiB           ds_readv.coords['lon'] = pd.DataFrame((ds_readv.lon[ds_readv.lon >= 180] - 360)- 0.12) \
   203   1362.5 MiB      0.0 MiB                                   .append(pd.DataFrame(ds_readv.lon[ds_readv.lon < 180]) + 0.58) \
   204   1362.5 MiB      0.0 MiB                                   .reset_index(drop=True).squeeze()
   205   1362.5 MiB      0.0 MiB           ds_readv = ds_readv.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   206                             
   207                                     # Add readv to modeled RSL at locations with data
   208                                     ##### Need to fix this, as currently slice does not acknowledge new coords #########
   209   1362.5 MiB      0.0 MiB           ds_readv = ds_readv.sel(age=slice(tmax, tmin),
   210   1362.5 MiB      0.0 MiB                                   lon=slice(df_place.lon.min() + 180 - 2,
   211   1362.5 MiB      0.0 MiB                                             df_place.lon.max() + 180 + 2),
   212   1362.5 MiB      0.0 MiB                                   lat=slice(df_place.lat.max() + 2,
   213   1362.5 MiB      0.1 MiB                                             df_place.lat.min() - 2))
   214   1362.5 MiB      0.0 MiB           return ds_readv
   215                             
   216                             
   217                                 #Make deterministic readvance fingerprint
   218    327.8 MiB      0.0 MiB       start, end = 6100, 3000
   219    327.8 MiB      0.0 MiB       maxscale = 2.25
   220    601.3 MiB      0.0 MiB       ds_readv = make_fingerprint(start, end, maxscale)
   221                             
   222                                 #Make readvance prior
   223    601.3 MiB      0.0 MiB       start, end = 8000, 2000
   224    601.3 MiB      0.0 MiB       maxscale = 2.25
   225    860.5 MiB      0.0 MiB       ds_readvprior = make_fingerprint(start, end, maxscale)
   226    860.8 MiB      0.2 MiB       ds_readvprior_std = ds_readvprior * 0.3
   227                             
   228                                 ####################  Build  GIA models 	#######################
   229                                 #################### ---------------------- #######################
   230                             
   231                                 #Use either glac1d or ICE6G
   232    860.8 MiB      0.0 MiB       if ice_model == 'glac1d_':
   233                             
   234                                     def build_dataset(model):
   235                                         """download model runs from local directory."""
   236                             
   237                                         path = f'data/glac1d_/output_{model}'
   238                                         files = f'{path}*.nc'
   239                                         basefiles = glob.glob(files)
   240                                         modelrun = [
   241                                             key.split('glac1d/output_', 1)[1][:-3].replace('.', '_')
   242                                             for key in basefiles
   243                                         ]
   244                                         dss = xr.open_mfdataset(files,
   245                                                                 chunks=None,
   246                                                                 concat_dim='modelrun',
   247                                                                 combine='nested')
   248                                         lats, lons, times = dss.LAT.values[0], dss.LON.values[
   249                                             0], dss.TIME.values[0]
   250                                         ds = dss.drop(['LAT', 'LON', 'TIME'])
   251                                         ds = ds.assign_coords(lat=lats,
   252                                                               lon=lons,
   253                                                               time=times,
   254                                                               modelrun=modelrun).rename({
   255                                                                   'time': 'age',
   256                                                                   'RSL': 'rsl'
   257                                                               })
   258                                         return ds
   259                             
   260                                     def one_mod(names):
   261                                         """Organize model runs into xarray dataset."""
   262                                         ds1 = build_dataset(names[0])
   263                                         names = names[1:]
   264                                         ds = ds1.chunk({'lat': 10, 'lon': 10})
   265                                         for i in range(len(names)):
   266                                             temp = build_dataset(names[i])
   267                                             temp1 = temp.interp_like(ds1)
   268                                             temp1['modelrun'] = temp['modelrun']
   269                                             ds = xr.concat([ds, temp1], dim='modelrun')
   270                                         ds['age'] = ds['age'] * 1000
   271                                         ds = ds.roll(lon=256, roll_coords=True)
   272                                         ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \
   273                                                                 .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \
   274                                                                 .reset_index(drop=True).squeeze()
   275                                         ds.coords['lat'] = ds.lat[::-1]
   276                                         ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   277                                         return ds
   278                             
   279                                     #make composite of a bunch of GIA runs, i.e. GIA prior
   280                                     ds = one_mod([ice_model + lith_thickness])
   281                             
   282                                     ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   283                                                            lon=slice(df_place.lon.min() - 2,
   284                                                                      df_place.lon.max() + 2),
   285                                                            lat=slice(df_place.lat.min() - 2,
   286                                                                      df_place.lat.max() + 2))
   287                                     ds_area = ds_sliced.mean(dim='modelrun').load().to_dataset().interp(
   288                                         age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   289                                     ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(
   290                                         age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   291                             
   292                                     # make "true" RSL by adding single GIA run and fingerprint
   293                                     lithmantle = 'l71C_ump2_lm50'
   294                                     ds_diff = one_mod(
   295                                         [ice_model + 'l71C']).sel(modelrun=ice_model + lithmantle).rsl.sel(
   296                                             age=slice(tmax, tmin),
   297                                             lon=slice(df_place.lon.min() - 2,
   298                                                       df_place.lon.max() + 2),
   299                                             lat=slice(df_place.lat.min() - 2,
   300                                                       df_place.lat.max() + 2)).load().to_dataset().interp(
   301                                                           age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   302                             
   303                                 else:
   304                             
   305   1131.9 MiB      0.0 MiB           def build_dataset(model):
   306                                         """download model runs from local directory."""
   307                             
   308   1131.9 MiB      0.0 MiB               path = f'data/d6g_h6g_/output_{model}'
   309   1131.9 MiB      0.0 MiB               files = f'{path}*.nc'
   310   1131.9 MiB      0.0 MiB               basefiles = glob.glob(files)
   311                                         modelrun = [
   312   1131.9 MiB      0.0 MiB                   key.split('d6g_h6g_/output_', 1)[1][:-3].replace('.', '_')
   313   1131.9 MiB      0.0 MiB                   for key in basefiles
   314                                         ]
   315   1131.9 MiB      0.0 MiB               dss = xr.open_mfdataset(files,
   316   1131.9 MiB      0.0 MiB                                       chunks=None,
   317   1131.9 MiB      0.0 MiB                                       concat_dim='modelrun',
   318   1131.9 MiB     29.2 MiB                                       combine='nested')
   319   1131.9 MiB      3.1 MiB               lats, lons, times = dss.LAT.values[0], dss.LON.values[
   320   1131.9 MiB      0.5 MiB                   0], dss.TIME.values[0]
   321   1131.9 MiB      0.0 MiB               ds = dss.drop(['LAT', 'LON', 'TIME'])
   322   1131.9 MiB      0.0 MiB               ds = ds.assign_coords(lat=lats,
   323   1131.9 MiB      0.0 MiB                                     lon=lons,
   324   1131.9 MiB      0.0 MiB                                     time=times,
   325   1131.9 MiB      0.0 MiB                                     modelrun=modelrun).rename({
   326   1131.9 MiB      0.0 MiB                                         'time': 'age',
   327   1131.9 MiB      0.0 MiB                                         'RSL': 'rsl'
   328                                                               })
   329   1131.9 MiB      0.0 MiB               return ds
   330                             
   331   1131.9 MiB      0.0 MiB           def one_mod(names):
   332                                         """Organize model runs into xarray dataset."""
   333   1131.9 MiB      0.0 MiB               ds1 = build_dataset(names[0])
   334   1131.9 MiB      0.0 MiB               names = names[1:]
   335   1158.1 MiB     29.1 MiB               ds = ds1.chunk({'lat': 10, 'lon': 10})
   336   1158.1 MiB      0.0 MiB               for i in range(len(names)):
   337                                             temp = build_dataset(names[i])
   338                                             temp1 = temp.interp_like(ds1)
   339                                             temp1['modelrun'] = temp['modelrun']
   340                                             ds = xr.concat([ds, temp1], dim='modelrun')
   341   1158.1 MiB      0.0 MiB               ds['age'] = ds['age'] * 1000
   342   1268.0 MiB    111.2 MiB               ds = ds.roll(lon=256, roll_coords=True)
   343   1268.2 MiB      0.1 MiB               ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \
   344   1268.2 MiB      0.0 MiB                                       .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \
   345   1268.2 MiB      0.0 MiB                                       .reset_index(drop=True).squeeze()
   346   1268.2 MiB      0.0 MiB               ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   347   1268.2 MiB      0.0 MiB               return ds
   348                             
   349                                     #make composite of a bunch of GIA runs, i.e. GIA prior
   350   1034.0 MiB      0.0 MiB           ds = one_mod([ice_model + lith_thickness])
   351                             
   352   1034.0 MiB      0.0 MiB           ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   353   1034.0 MiB      0.0 MiB                                  lon=slice(df_place.lon.min() - 2,
   354   1034.0 MiB      0.0 MiB                                            df_place.lon.max() + 2),
   355   1034.0 MiB      0.0 MiB                                  lat=slice(df_place.lat.max() + 2,
   356   1034.1 MiB      0.1 MiB                                            df_place.lat.min() - 2))
   357   1150.9 MiB    116.8 MiB           ds_area = ds_sliced.mean(dim='modelrun').load().to_dataset().interp(
   358   1151.3 MiB      0.4 MiB               age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   359   1179.3 MiB     28.0 MiB           ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(
   360   1131.9 MiB      0.0 MiB               age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   361                             
   362                                     # make "true" RSL by adding single GIA run and fingerprint
   363   1131.9 MiB      0.0 MiB           lithmantle = 'l71C_ump2_lm50'
   364   1131.9 MiB      0.0 MiB           ds_diff = one_mod(
   365   1268.3 MiB      0.2 MiB               [ice_model + 'l71C']).sel(modelrun=ice_model + lithmantle).rsl.sel(
   366   1268.3 MiB      0.0 MiB                   age=slice(tmax, tmin),
   367   1268.3 MiB      0.0 MiB                   lon=slice(df_place.lon.min() - 2,
   368   1268.3 MiB      0.0 MiB                             df_place.lon.max() + 2),
   369   1268.3 MiB      0.0 MiB                   lat=slice(df_place.lat.max() + 2,
   370   1135.9 MiB      0.0 MiB                             df_place.lat.min() - 2)).load().to_dataset().interp(
   371   1136.0 MiB      0.1 MiB                                 age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   372                             
   373                                 #make residual by subtracting GIA prior and fingerprint prior from "true" GIA
   374   1136.0 MiB      0.0 MiB       ds_true = ds_diff + ds_readv
   375   1136.0 MiB      0.0 MiB       ds_prior = ds_area + ds_readvprior
   376   1136.0 MiB      0.0 MiB       ds_priorstd = ds_areastd + ds_readvprior_std
   377   1136.0 MiB      0.0 MiB       ds_truelessprior = ds_true - ds_prior
   378                             
   379                             
   380                                 #sample each model at points where we have RSL data
   381   1136.1 MiB      0.0 MiB       def ds_select(ds):
   382   1136.1 MiB      0.0 MiB           return ds.rsl.sel(age=[row.age],
   383   1136.1 MiB      0.0 MiB                             lon=[row.lon],
   384   1136.1 MiB      0.0 MiB                             lat=[row.lat],
   385   1136.1 MiB      0.1 MiB                             method='nearest').squeeze().values
   386                             
   387                             
   388                                 #select points at which RSL data exists
   389   1136.1 MiB      0.0 MiB       for i, row in df_place.iterrows():
   390   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_true'] = ds_select(ds_true)
   391   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_resid'] = ds_select(ds_truelessprior)
   392   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_realresid'] = df_place.rsl[i] - ds_select(ds_area)
   393                             
   394   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_totalprior'] = ds_select(ds_prior)
   395   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_totalprior_std'] = ds_select(ds_priorstd)
   396   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_giaprior'] = ds_select(ds_area)
   397   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_giaprior_std'] = ds_select(ds_areastd)
   398   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_readvprior'] = ds_select(ds_readvprior)
   399   1136.1 MiB      0.0 MiB           df_place.loc[i, 'rsl_readvprior_std'] = ds_select(ds_readvprior_std)
   400    815.9 MiB      0.0 MiB       df_place.shape
   401                             
   402                                 ##################	  RUN GP REGRESSION 	#######################
   403                                 ##################  --------------------	 ######################
   404                             
   405    815.9 MiB      0.0 MiB       start = time.time()
   406                             
   407    815.9 MiB      0.0 MiB       Data = Tuple[tf.Tensor, tf.Tensor]
   408    815.9 MiB      0.0 MiB       likelihood = df_place.rsl_er_max.ravel()**2 + df_place.rsl_giaprior_std.ravel(
   409    815.9 MiB      0.0 MiB       )**2  # here we define likelihood
   410                             
   411                             
   412    815.9 MiB      0.0 MiB       class GPR_diag(gpf.models.GPModel):
   413                                     r"""
   414                                     Gaussian Process Regression.
   415                                     This is a vanilla implementation of GP regression with a pointwise Gaussian
   416                                     likelihood.  Multiple columns of Y are treated independently.
   417                                     The log likelihood of this models is sometimes referred to as the 'marginal log likelihood',
   418                                     and is given by
   419                                     .. math::
   420                                        \log p(\mathbf y \,|\, \mathbf f) =
   421                                             \mathcal N\left(\mathbf y\,|\, 0, \mathbf K + \sigma_n \mathbf I\right)
   422    815.9 MiB      0.0 MiB           """
   423    821.0 MiB      0.0 MiB           def __init__(self,
   424                                                  data: Data,
   425                                                  kernel: Kernel,
   426    815.9 MiB      0.0 MiB                        mean_function: Optional[MeanFunction] = None,
   427    815.9 MiB      0.0 MiB                        likelihood=likelihood):
   428    821.0 MiB      0.0 MiB               likelihood = gpf.likelihoods.Gaussian(variance=likelihood)
   429    821.0 MiB      0.0 MiB               _, y_data = data
   430    821.0 MiB      0.0 MiB               super().__init__(kernel,
   431    821.0 MiB      0.0 MiB                                likelihood,
   432    821.0 MiB      0.0 MiB                                mean_function,
   433    821.0 MiB      0.0 MiB                                num_latent=y_data.shape[-1])
   434    821.0 MiB      0.0 MiB               self.data = data
   435                             
   436    815.9 MiB      0.0 MiB           def log_likelihood(self):
   437                                         """
   438                                         Computes the log likelihood.
   439                                         """
   440    814.7 MiB      0.0 MiB               x, y = self.data
   441    815.7 MiB      0.4 MiB               K = self.kernel(x)
   442    815.7 MiB      0.0 MiB               num_data = x.shape[0]
   443    815.7 MiB      0.0 MiB               k_diag = tf.linalg.diag_part(K)
   444    815.8 MiB      0.0 MiB               s_diag = tf.convert_to_tensor(self.likelihood.variance)
   445    815.8 MiB      0.0 MiB               jitter = tf.cast(tf.fill([num_data], default_jitter()),
   446    815.8 MiB      0.0 MiB                                'float64')  # stabilize K matrix w/jitter
   447    815.8 MiB      0.0 MiB               ks = tf.linalg.set_diag(K, k_diag + s_diag + jitter)
   448    815.8 MiB      0.0 MiB               L = tf.linalg.cholesky(ks)
   449    815.9 MiB      0.1 MiB               m = self.mean_function(x)
   450                             
   451                                         # [R,] log-likelihoods for each independent dimension of Y
   452    815.9 MiB      0.0 MiB               log_prob = multivariate_normal(y, m, L)
   453    815.9 MiB      0.0 MiB               return tf.reduce_sum(log_prob)
   454                             
   455    687.8 MiB      0.0 MiB           def predict_f(self,
   456                                                   predict_at: tf.Tensor,
   457                                                   full_cov: bool = False,
   458    815.9 MiB      0.0 MiB                         full_output_cov: bool = False):
   459                                         r"""
   460                                         This method computes predictions at X \in R^{N \x D} input points
   461                                         .. math::
   462                                             p(F* | Y)
   463                                         where F* are points on the GP at new data points, Y are noisy observations at training data points.
   464                                         """
   465    687.8 MiB      0.0 MiB               x_data, y_data = self.data
   466    687.8 MiB      0.0 MiB               err = y_data - self.mean_function(x_data)
   467                             
   468    698.0 MiB      0.0 MiB               kmm = self.kernel(x_data)
   469    690.8 MiB      0.0 MiB               knn = self.kernel(predict_at, full=full_cov)
   470    932.2 MiB      0.0 MiB               kmn = self.kernel(x_data, predict_at)
   471                             
   472    932.2 MiB      0.0 MiB               num_data = x_data.shape[0]
   473    932.2 MiB      0.0 MiB               s = tf.linalg.diag(tf.convert_to_tensor(
   474    934.8 MiB      2.6 MiB                   self.likelihood.variance))  #changed from normal GPR
   475                             
   476    934.8 MiB      0.0 MiB               conditional = gpf.conditionals.base_conditional
   477    934.8 MiB      0.0 MiB               f_mean_zero, f_var = conditional(
   478    937.4 MiB      2.6 MiB                   kmn, kmm + s, knn, err, full_cov=full_cov,
   479    983.4 MiB     46.0 MiB                   white=False)  # [N, P], [N, P] or [P, N, N]
   480    983.8 MiB      0.5 MiB               f_mean = f_mean_zero + self.mean_function(predict_at)
   481    983.8 MiB      0.0 MiB               return f_mean, f_var
   482                             
   483                             
   484    815.9 MiB      0.0 MiB       def normalize(df):
   485    815.9 MiB      0.0 MiB           return np.array((df - df.mean()) / df.std()).reshape(len(df), 1)
   486                             
   487                             
   488    815.9 MiB      0.0 MiB       def denormalize(y_pred, df):
   489    694.7 MiB      0.0 MiB           return np.array((y_pred * df.std()) + df.mean())
   490                             
   491                             
   492    821.0 MiB      0.0 MiB       def bounded_parameter(low, high, param):
   493                                     """Make parameter tfp Parameter with optimization bounds."""
   494    821.0 MiB      0.0 MiB           affine = tfb.AffineScalar(shift=tf.cast(low, tf.float64),
   495    821.0 MiB      0.0 MiB                                     scale=tf.cast(high - low, tf.float64))
   496    821.0 MiB      0.0 MiB           sigmoid = tfb.Sigmoid()
   497    821.0 MiB      0.0 MiB           logistic = tfb.Chain([affine, sigmoid])
   498    821.0 MiB      0.1 MiB           parameter = gpf.Parameter(param, transform=logistic, dtype=tf.float64)
   499    821.0 MiB      0.0 MiB           return parameter
   500                             
   501                             
   502    815.9 MiB      0.0 MiB       class HaversineKernel_Matern52(gpf.kernels.Matern52):
   503                                     """
   504                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   505                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   506    815.9 MiB      0.0 MiB           """
   507                                     def __init__(
   508                                         self,
   509                                         lengthscale=1.0,
   510                                         variance=1.0,
   511    815.9 MiB      0.0 MiB               active_dims=None,
   512                                     ):
   513                                         super().__init__(
   514                                             active_dims=active_dims,
   515                                             variance=variance,
   516                                             lengthscale=lengthscale,
   517                                         )
   518                             
   519    815.9 MiB      0.0 MiB           def haversine_dist(self, X, X2):
   520                                         pi = np.pi / 180
   521                                         f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   522                                         f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   523                                         d = tf.sin((f - f2) / 2)**2
   524                                         lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   525                                                     tf.expand_dims(X2[:, 0] * pi, -2)
   526                                         cos_prod = tf.cos(lat2) * tf.cos(lat1)
   527                                         a = d[:, :, 0] + cos_prod * d[:, :, 1]
   528                                         c = tf.asin(tf.sqrt(a)) * 6371 * 2
   529                                         return c
   530                             
   531    815.9 MiB      0.0 MiB           def scaled_squared_euclid_dist(self, X, X2):
   532                                         """
   533                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   534                                         """
   535                                         if X2 is None:
   536                                             X2 = X
   537                                         dist = tf.square(self.haversine_dist(X, X2) / self.lengthscale)
   538                                         return dist
   539                             
   540                             
   541    815.9 MiB      0.0 MiB       class HaversineKernel_Matern32(gpf.kernels.Matern32):
   542                                     """
   543                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   544                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   545    815.9 MiB      0.0 MiB           """
   546    820.9 MiB      0.0 MiB           def __init__(
   547                                         self,
   548                                         lengthscale=1.0,
   549                                         variance=1.0,
   550    815.9 MiB      0.0 MiB               active_dims=None,
   551                                     ):
   552    820.9 MiB      0.0 MiB               super().__init__(
   553    820.9 MiB      0.0 MiB                   active_dims=active_dims,
   554    820.9 MiB      0.0 MiB                   variance=variance,
   555    820.9 MiB      4.8 MiB                   lengthscale=lengthscale,
   556                                         )
   557                             
   558    932.2 MiB      0.0 MiB           def haversine_dist(self, X, X2):
   559    932.2 MiB      0.0 MiB               pi = np.pi / 180
   560    932.2 MiB      0.0 MiB               f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   561    932.2 MiB      0.0 MiB               f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   562   1505.5 MiB    573.3 MiB               d = tf.sin((f - f2) / 2)**2
   563   1505.5 MiB      0.0 MiB               lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   564   1505.5 MiB      2.0 MiB                           tf.expand_dims(X2[:, 0] * pi, -2)
   565   1792.2 MiB    286.6 MiB               cos_prod = tf.cos(lat2) * tf.cos(lat1)
   566   2078.8 MiB    286.6 MiB               a = d[:, :, 0] + cos_prod * d[:, :, 1]
   567   2365.4 MiB    286.6 MiB               c = tf.asin(tf.sqrt(a)) * 6371 * 2
   568   2365.4 MiB      0.0 MiB               return c
   569                             
   570    932.2 MiB      0.5 MiB           def scaled_squared_euclid_dist(self, X, X2):
   571                                         """
   572                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   573                                         """
   574    932.2 MiB      0.0 MiB               if X2 is None:
   575    815.2 MiB      0.0 MiB                   X2 = X
   576   1218.9 MiB      0.1 MiB               dist = tf.square(self.haversine_dist(X, X2) / self.lengthscale)
   577   1218.9 MiB      0.0 MiB               return dist
   578                             
   579                             
   580                                 ########### Section to Run GPR######################
   581                                 ##################################3#################
   582                             
   583                                 # Input space, rsl normalized to zero mean, unit variance
   584    815.9 MiB      0.0 MiB       X = np.stack((df_place['lon'], df_place['lat'], df_place['age']), 1)
   585    815.9 MiB      0.0 MiB       RSL = normalize(df_place.rsl_realresid)
   586                             
   587                                 #define kernels  with bounds
   588                             
   589    820.7 MiB      0.0 MiB       k1 = HaversineKernel_Matern32(active_dims=[0, 1])
   590    820.9 MiB      0.0 MiB       k1.lengthscale = bounded_parameter(5000, 30000, 10000)  #hemispheric space
   591    820.9 MiB      0.0 MiB       k1.variance = bounded_parameter(0.1, 100, 2)
   592                             
   593    820.9 MiB      0.0 MiB       k2 = HaversineKernel_Matern32(active_dims=[0, 1])
   594    820.9 MiB      0.0 MiB       k2.lengthscale = bounded_parameter(10, 5000, 100)  #GIA space
   595    820.9 MiB      0.0 MiB       k2.variance = bounded_parameter(0.1, 100, 2)
   596                             
   597    821.0 MiB      0.0 MiB       k3 = gpf.kernels.Matern32(active_dims=[2])  #GIA time
   598    821.0 MiB      0.0 MiB       k3.lengthscale = bounded_parameter(8000, 20000, 10000)
   599    821.0 MiB      0.0 MiB       k3.variance = bounded_parameter(0.1, 100, 1)
   600                             
   601    821.0 MiB      0.0 MiB       k4 = gpf.kernels.Matern32(active_dims=[2])  #shorter time
   602    821.0 MiB      0.0 MiB       k4.lengthscale = bounded_parameter(1, 8000, 1000)
   603    821.0 MiB      0.0 MiB       k4.variance = bounded_parameter(0.1, 100, 1)
   604                             
   605    821.0 MiB      0.0 MiB       k5 = gpf.kernels.White(active_dims=[2])
   606    821.0 MiB      0.0 MiB       k5.variance = bounded_parameter(0.1, 100, 1)
   607                             
   608    821.0 MiB      0.0 MiB       kernel = (k1 * k3) + (k2 * k4) + k5
   609                             
   610                                 #build & train model
   611    821.0 MiB      0.0 MiB       m = GPR_diag((X, RSL), kernel=kernel, likelihood=likelihood)
   612    821.0 MiB      0.0 MiB       print('model built, time=', time.time() - start)
   613                             
   614                             
   615    821.0 MiB      0.0 MiB       @tf.function(autograph=False)
   616                                 def objective():
   617    815.9 MiB      0.0 MiB           return -m.log_marginal_likelihood()
   618                             
   619                             
   620    821.0 MiB      0.0 MiB       o = gpf.optimizers.Scipy()
   621    690.2 MiB      0.0 MiB       o.minimize(objective, variables=m.trainable_variables)
   622    690.2 MiB      0.0 MiB       print('model minimized, time=', time.time() - start)
   623                             
   624                                 # output space
   625    690.2 MiB      0.0 MiB       nout = 50
   626    690.2 MiB      0.0 MiB       lat = np.linspace(min(ds_area.lat), max(ds_area.lat), nout)
   627    690.2 MiB      0.0 MiB       lon = np.linspace(min(ds_area.lon), max(ds_area.lon), nout)
   628    690.2 MiB      0.0 MiB       ages = ages_lgm[(ages_lgm < tmax) & (ages_lgm > tmin)]
   629    687.8 MiB      0.0 MiB       xyt = np.array(list(product(lon, lat, ages)))
   630                             
   631                                 #query model & renormalize data
   632    694.7 MiB      0.0 MiB       y_pred, var = m.predict_f(xyt)
   633    694.7 MiB      0.0 MiB       y_pred_out = denormalize(y_pred, df_place.rsl_realresid)
   634                             
   635                                 #reshape output vectors
   636    694.7 MiB      0.0 MiB       Xlon = np.array(xyt[:, 0]).reshape((nout, nout, len(ages)))
   637    694.7 MiB      0.0 MiB       Xlat = np.array(xyt[:, 1]).reshape((nout, nout, len(ages)))
   638    695.2 MiB      0.5 MiB       Zp = np.array(y_pred_out).reshape(nout, nout, len(ages))
   639    695.7 MiB      0.5 MiB       varp = np.array(var).reshape(nout, nout, len(ages))
   640                             
   641                                 #print kernel details
   642    695.7 MiB      0.1 MiB       print_summary(m, fmt='notebook')
   643    695.7 MiB      0.0 MiB       print('time elapsed = ', time.time() - start)
   644                             
   645    695.7 MiB      0.0 MiB       print('negative log marginal likelihood =',
   646    704.8 MiB      0.0 MiB             m.neg_log_marginal_likelihood().numpy())
   647                             
   648                                 ##################	  INTERPOLATE MODELS 	#######################
   649                                 ##################  --------------------	 ######################
   650                             
   651                                 # turn GPR output into xarray dataarray
   652    704.8 MiB      0.0 MiB       da_zp = xr.DataArray(Zp, coords=[lon, lat, ages],
   653    704.8 MiB      0.0 MiB                            dims=['lon', 'lat',
   654    704.8 MiB      0.0 MiB                                  'age']).transpose('age', 'lat', 'lon')
   655    704.8 MiB      0.0 MiB       da_varp = xr.DataArray(varp,
   656    704.8 MiB      0.0 MiB                              coords=[lon, lat, ages],
   657    704.8 MiB      0.0 MiB                              dims=['lon', 'lat',
   658    704.8 MiB      0.0 MiB                                    'age']).transpose('age', 'lat', 'lon')
   659                             
   660                             
   661    704.8 MiB      0.0 MiB       def interp_likegpr(ds):
   662    651.0 MiB      0.0 MiB           return ds.rsl.load().transpose().interp_like(da_zp)
   663                             
   664                             
   665                                 #interpolate all models onto GPR grid
   666    646.2 MiB      0.0 MiB       da_trueinterp = interp_likegpr(ds_true)
   667    646.2 MiB      0.0 MiB       ds_trueinterp = ds_true.interp(age=ages)
   668    647.3 MiB      0.0 MiB       da_priorinterp = interp_likegpr(ds_prior)
   669    647.3 MiB      0.0 MiB       ds_priorinterp = ds_prior.interp(age=ages)
   670    648.4 MiB      0.0 MiB       da_priorinterpstd = interp_likegpr(ds_priorstd)
   671    648.9 MiB      0.0 MiB       da_giapriorinterp = interp_likegpr(ds_area)
   672    648.9 MiB      0.0 MiB       ds_giapriorinterp = ds_area.interp(age=ages)
   673    650.0 MiB      0.0 MiB       da_giapriorinterpstd = interp_likegpr(ds_areastd)
   674    650.5 MiB      0.0 MiB       da_readvpriorinterp = interp_likegpr(ds_readvprior)
   675    651.0 MiB      0.0 MiB       da_readvpriorinterpstd = interp_likegpr(ds_readvprior_std)
   676                             
   677                                 # add total prior RSL back into GPR
   678    651.0 MiB      0.0 MiB       da_priorplusgpr = da_zp + da_giapriorinterp
   679                             
   680                                 ##################	  	 SAVE NETCDFS 	 	#######################
   681                                 ##################  --------------------	 ######################
   682                             
   683    651.0 MiB      0.0 MiB       path = 'output/'
   684    652.1 MiB      1.1 MiB       da_zp.to_netcdf(path + ice_model + lith_thickness + '_' + place + '_da_zp')
   685    652.1 MiB      0.0 MiB       da_giapriorinterp.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   686    652.2 MiB      0.1 MiB                                   '_giaprior')
   687    652.2 MiB      0.0 MiB       da_priorplusgpr.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   688    652.2 MiB      0.0 MiB                                 '_posterior')
   689    652.2 MiB      0.0 MiB       da_varp.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   690    652.2 MiB      0.0 MiB                         '_gp_variance')
   691                             
   692                                 ##################		  PLOT  MODELS 		#######################
   693                                 ##################  --------------------	 ######################
   694                             
   695    652.2 MiB      0.0 MiB       dirName = f'figs/{place}/'
   696    652.2 MiB      0.0 MiB       if not os.path.exists(dirName):
   697                                     os.mkdir(dirName)
   698                                     print("Directory ", dirName, " Created ")
   699                                 else:
   700    652.2 MiB      0.0 MiB           print("Directory ", dirName, " already exists")
   701                             
   702    658.6 MiB      0.0 MiB       for i, age in enumerate(ages):
   703    655.8 MiB      0.0 MiB           if (age / 500).is_integer():
   704    655.8 MiB      0.0 MiB               step = (ages[0] - ages[1])
   705    655.8 MiB      0.0 MiB               df_it = df_place[(df_place.age < age) & (df_place.age > age - step)]
   706    655.8 MiB      0.0 MiB               resid_it = da_zp.sel(age=slice(age, age - step))
   707    655.8 MiB      0.0 MiB               rsl, var = df_it.rsl, df_it.rsl_er_max.values**2
   708    655.8 MiB      0.0 MiB               lat_it, lon_it = df_it.lat, df_it.lon
   709    655.8 MiB      0.0 MiB               vmin = ds_giapriorinterp.rsl.min().values  # + 10
   710    655.8 MiB      0.0 MiB               vmax = ds_giapriorinterp.rsl.max().values  # - 40
   711    655.8 MiB      0.0 MiB               vmin_std = 0
   712    655.8 MiB      0.0 MiB               vmax_std = 1
   713    655.8 MiB      0.0 MiB               tmin_it = np.round(age - step, 2)
   714    655.8 MiB      0.0 MiB               tmax_it = np.round(age, 2)
   715    655.8 MiB      0.0 MiB               cbarscale = 0.3
   716    655.8 MiB      0.0 MiB               fontsize = 20
   717    655.8 MiB      0.0 MiB               cmap = 'coolwarm'
   718    655.8 MiB      0.0 MiB               cbar_kwargs = {'shrink': cbarscale, 'label': 'RSL (m)'}
   719                             
   720    655.8 MiB      0.0 MiB               proj = ccrs.PlateCarree()
   721    655.8 MiB      0.0 MiB               projection = ccrs.PlateCarree()
   722                                         fig, (ax1, ax2, ax3,
   723    655.8 MiB      0.0 MiB                     ax4) = plt.subplots(1,
   724    655.8 MiB      0.0 MiB                                         4,
   725    655.8 MiB      0.0 MiB                                         figsize=(24, 16),
   726    656.6 MiB      0.9 MiB                                         subplot_kw=dict(projection=projection))
   727                             
   728                                         # total prior mean + "true" data
   729    656.6 MiB      0.0 MiB               ax1.coastlines(color='k')
   730    656.6 MiB      0.0 MiB               pc1 = ds_giapriorinterp.rsl[i].transpose().plot(ax=ax1,
   731    656.6 MiB      0.0 MiB                                                               transform=proj,
   732    656.6 MiB      0.0 MiB                                                               cmap=cmap,
   733    656.6 MiB      0.0 MiB                                                               norm=MidpointNormalize(
   734    656.6 MiB      0.0 MiB                                                                   vmin, vmax, 0),
   735    656.6 MiB      0.0 MiB                                                               add_colorbar=False,
   736    656.7 MiB      0.3 MiB                                                               extend='both')
   737    656.7 MiB      0.0 MiB               cbar = fig.colorbar(pc1,
   738    656.7 MiB      0.0 MiB                                   ax=ax1,
   739    656.7 MiB      0.0 MiB                                   shrink=.3,
   740    656.7 MiB      0.0 MiB                                   label='RSL (m)',
   741    656.9 MiB      0.0 MiB                                   extend='both')
   742    656.9 MiB      0.0 MiB               scat = ax1.scatter(lon_it,
   743    656.9 MiB      0.0 MiB                                  lat_it,
   744    656.9 MiB      0.0 MiB                                  s=80,
   745    656.9 MiB      0.0 MiB                                  c=rsl,
   746    656.9 MiB      0.0 MiB                                  edgecolor='k',
   747    656.9 MiB      0.0 MiB                                  vmin=vmin,
   748    656.9 MiB      0.0 MiB                                  vmax=vmax,
   749    656.9 MiB      0.0 MiB                                  norm=MidpointNormalize(vmin, vmax, 0),
   750    657.0 MiB      0.1 MiB                                  cmap=cmap)
   751    657.0 MiB      0.0 MiB               ax1.set_title(f'{np.round(ds_trueinterp.rsl[i].age.values, -1)} yrs',
   752    657.0 MiB      0.0 MiB                             fontsize=fontsize)
   753                                         #         ax1.set_extent(extent_)
   754                             
   755                                         # Learned difference between prior and "true" data
   756    657.0 MiB      0.0 MiB               ax2.coastlines(color='k')
   757    657.0 MiB      0.0 MiB               pc = da_zp[i, :, :].plot(ax=ax2,
   758    657.0 MiB      0.0 MiB                                        transform=proj,
   759    657.0 MiB      0.0 MiB                                        cmap=cmap,
   760    657.0 MiB      0.0 MiB                                        extend='both',
   761    657.0 MiB      0.0 MiB                                        norm=MidpointNormalize(
   762    657.0 MiB      0.0 MiB                                            resid_it.min(), resid_it.max(), 0),
   763    657.2 MiB      0.3 MiB                                        add_colorbar=False)
   764    657.2 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   765    657.2 MiB      0.0 MiB                                   ax=ax2,
   766    657.2 MiB      0.0 MiB                                   shrink=.3,
   767    657.2 MiB      0.0 MiB                                   label='RSL (m)',
   768    657.6 MiB      0.1 MiB                                   extend='both')
   769    657.6 MiB      0.0 MiB               scat = ax2.scatter(lon_it,
   770    657.6 MiB      0.0 MiB                                  lat_it,
   771    657.6 MiB      0.0 MiB                                  s=80,
   772    657.6 MiB      0.0 MiB                                  facecolors='k',
   773    657.6 MiB      0.0 MiB                                  cmap=cmap,
   774    657.6 MiB      0.0 MiB                                  edgecolor='k',
   775    657.6 MiB      0.0 MiB                                  transform=proj,
   776    657.6 MiB      0.0 MiB                                  norm=MidpointNormalize(resid_it.min(),
   777    657.7 MiB      0.0 MiB                                                         resid_it.max(), 0))
   778    657.7 MiB      0.0 MiB               ax2.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   779                                         #         ax2.set_extent(extent_)
   780                             
   781                                         # GP regression
   782    657.7 MiB      0.0 MiB               ax3.coastlines(color='k')
   783    657.7 MiB      0.0 MiB               pc = da_priorplusgpr[i].plot(ax=ax3,
   784    657.7 MiB      0.0 MiB                                            transform=proj,
   785    657.7 MiB      0.0 MiB                                            norm=MidpointNormalize(vmin, vmax, 0),
   786    657.7 MiB      0.0 MiB                                            cmap=cmap,
   787    657.7 MiB      0.0 MiB                                            extend='both',
   788    657.9 MiB      0.2 MiB                                            add_colorbar=False)
   789    657.9 MiB      0.0 MiB               scat = ax3.scatter(lon_it,
   790    657.9 MiB      0.0 MiB                                  lat_it,
   791    657.9 MiB      0.0 MiB                                  s=80,
   792    657.9 MiB      0.0 MiB                                  c=rsl,
   793    657.9 MiB      0.0 MiB                                  edgecolor='k',
   794    657.9 MiB      0.0 MiB                                  cmap=cmap,
   795    657.9 MiB      0.0 MiB                                  norm=MidpointNormalize(vmin, vmax, 0))
   796    657.9 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   797    657.9 MiB      0.0 MiB                                   ax=ax3,
   798    657.9 MiB      0.0 MiB                                   shrink=.3,
   799    657.9 MiB      0.0 MiB                                   label='RSL (m)',
   800    658.2 MiB      0.0 MiB                                   extend='both')
   801    658.2 MiB      0.0 MiB               ax3.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   802                                         #         ax3.set_extent(extent_)
   803                             
   804                                         #GP regression standard deviation
   805    658.2 MiB      0.0 MiB               ax4.coastlines(color='k')
   806    658.2 MiB      0.0 MiB               pc = (2 * np.sqrt(da_varp[i])).plot(
   807    658.2 MiB      0.0 MiB                   ax=ax4,
   808    658.2 MiB      0.0 MiB                   transform=proj,
   809    658.2 MiB      0.0 MiB                   vmin=vmin_std,
   810    658.2 MiB      0.0 MiB                   vmax=vmax_std * 2,
   811    658.2 MiB      0.0 MiB                   cmap='Reds',
   812    658.2 MiB      0.0 MiB                   extend='both',
   813    658.4 MiB      0.2 MiB                   add_colorbar=False,
   814                                         )
   815    658.4 MiB      0.0 MiB               scat = ax4.scatter(lon_it,
   816    658.4 MiB      0.0 MiB                                  lat_it,
   817    658.4 MiB      0.0 MiB                                  s=80,
   818    658.4 MiB      0.0 MiB                                  c=2 * np.sqrt(var),
   819    658.4 MiB      0.0 MiB                                  vmin=vmin_std,
   820    658.4 MiB      0.0 MiB                                  vmax=vmax_std * 2,
   821    658.4 MiB      0.0 MiB                                  cmap='Reds',
   822    658.4 MiB      0.0 MiB                                  edgecolor='k',
   823    658.4 MiB      0.0 MiB                                  transform=proj)
   824    658.4 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   825    658.4 MiB      0.0 MiB                                   ax=ax4,
   826    658.4 MiB      0.0 MiB                                   shrink=.3,
   827    658.4 MiB      0.0 MiB                                   extend='both',
   828    658.6 MiB      0.2 MiB                                   label='RSL (m) (2 $\sigma$)')
   829    658.6 MiB      0.0 MiB               ax4.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   830                                 #         ax4.set_extent(extent_)
   831                             
   832                                 ########## ----- Save figures -------- #######################
   833    680.5 MiB      1.6 MiB       fig.savefig(dirName + f'{ages[i]}_{place}_realdata_fig_3D', transparent=True)
   834                             
   835                                 ##################	CHOOSE LOCS W/NUF SAMPS #######################
   836                                 ##################  --------------------	 ######################
   837                             
   838                             
   839    680.5 MiB      0.0 MiB       def locs_with_enoughsamples(df_place, place, number):
   840                                     """make new dataframe, labeled, of sites with [> number] measurements"""
   841    680.5 MiB      0.0 MiB           df_lots = df_place.groupby(['lat',
   842    680.6 MiB      0.0 MiB                                       'lon']).filter(lambda x: len(x) > number)
   843                             
   844    680.6 MiB      0.0 MiB           df_locs = []
   845    680.6 MiB      0.0 MiB           for i, group in enumerate(df_lots.groupby(['lat', 'lon'])):
   846    680.6 MiB      0.0 MiB               singleloc = group[1].copy()
   847    680.6 MiB      0.0 MiB               singleloc['location'] = place
   848    680.6 MiB      0.0 MiB               singleloc['locnum'] = place + '_site' + str(
   849    680.6 MiB      0.0 MiB                   i)  # + singleloc.reset_index().index.astype('str')
   850    680.6 MiB      0.0 MiB               df_locs.append(singleloc)
   851    680.6 MiB      0.0 MiB           df_locs = pd.concat(df_locs)
   852                             
   853    680.6 MiB      0.0 MiB           return df_locs
   854                             
   855                             
   856    680.5 MiB      0.0 MiB       number = 6
   857    680.6 MiB      0.0 MiB       df_nufsamps = locs_with_enoughsamples(df_place, place, number)
   858    680.6 MiB      0.0 MiB       len(df_nufsamps.locnum.unique())
   859                             
   860                                 ##################	PLOT LOCS W/NUF SAMPS   #######################
   861                                 ##################  --------------------	 ######################
   862                             
   863                             
   864    683.3 MiB      0.0 MiB       def slice_dataarray(da):
   865    683.3 MiB      0.0 MiB           return da.sel(lat=site[1].lat.unique(),
   866    683.3 MiB      0.0 MiB                         lon=site[1].lon.unique(),
   867    683.3 MiB      0.0 MiB                         method='nearest')
   868                             
   869                             
   870    682.1 MiB      1.5 MiB       fig, ax = plt.subplots(1, len(df_nufsamps.locnum.unique()), figsize=(18, 4))
   871    682.1 MiB      0.0 MiB       ax = ax.ravel()
   872    682.1 MiB      0.0 MiB       colors = ['darkgreen', 'darkblue', 'darkred']
   873    682.1 MiB      0.0 MiB       fontsize = 18
   874                             
   875    683.5 MiB      0.0 MiB       for i, site in enumerate(df_nufsamps.groupby('locnum')):
   876                             
   877                                     #slice data for each site
   878    683.3 MiB      0.0 MiB           prior_it = slice_dataarray(da_priorinterp)
   879    683.3 MiB      0.0 MiB           priorvar_it = slice_dataarray(da_priorinterpstd)
   880    683.3 MiB      0.0 MiB           top_prior = prior_it + priorvar_it * 2
   881    683.3 MiB      0.0 MiB           bottom_prior = prior_it - priorvar_it * 2
   882                             
   883    683.3 MiB      0.0 MiB           var_it = slice_dataarray(np.sqrt(da_varp))
   884    683.3 MiB      0.0 MiB           post_it = slice_dataarray(da_priorplusgpr)
   885    683.3 MiB      0.0 MiB           top = post_it + var_it * 2
   886    683.3 MiB      0.0 MiB           bottom = post_it - var_it * 2
   887                             
   888    683.3 MiB      0.0 MiB           site_err = 2 * (site[1].rsl_er_max + site[1].rsl_giaprior_std)
   889                             
   890    683.3 MiB      0.0 MiB           ax[i].scatter(site[1].age, site[1].rsl, c=colors[0], label='"true" RSL')
   891    683.3 MiB      0.0 MiB           ax[i].errorbar(
   892    683.3 MiB      0.0 MiB               site[1].age,
   893    683.3 MiB      0.0 MiB               site[1].rsl,
   894    683.3 MiB      0.0 MiB               site_err,
   895    683.3 MiB      0.0 MiB               c=colors[0],
   896    683.3 MiB      0.0 MiB               fmt='none',
   897    683.3 MiB      0.0 MiB               capsize=1,
   898    683.4 MiB      0.1 MiB               lw=1,
   899                                     )
   900                             
   901    683.4 MiB      0.0 MiB           prior_it.plot(ax=ax[i], c=colors[2], label='Prior $\pm 2 \sigma$')
   902    683.4 MiB      0.0 MiB           ax[i].fill_between(prior_it.age,
   903    683.4 MiB      0.0 MiB                              bottom_prior.squeeze(),
   904    683.4 MiB      0.0 MiB                              top_prior.squeeze(),
   905    683.4 MiB      0.0 MiB                              color=colors[2],
   906    683.4 MiB      0.0 MiB                              alpha=0.3)
   907                             
   908    683.4 MiB      0.0 MiB           post_it.plot(ax=ax[i], c=colors[1], label='Posterior $\pm 2 \sigma$')
   909    683.4 MiB      0.0 MiB           ax[i].fill_between(post_it.age,
   910    683.4 MiB      0.0 MiB                              bottom.squeeze(),
   911    683.4 MiB      0.0 MiB                              top.squeeze(),
   912    683.4 MiB      0.0 MiB                              color=colors[1],
   913    683.4 MiB      0.0 MiB                              alpha=0.3)
   914                                     #     ax[i].set_title(f'{site[0]} RSL', fontsize=fontsize)
   915    683.4 MiB      0.0 MiB           ax[i].set_title('')
   916                             
   917    683.5 MiB      0.0 MiB           ax[i].legend(loc='lower left')
   918                             
   919                                 #path = f'/Users/rogercreel/Dropbox/Columbia_PhD/Research/Holocene/WAIS_readvance/scripts/figs/{place}/'
   920    683.5 MiB      0.0 MiB       path = 'figs/{place}'
   921    683.5 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_realdata_fig_1D',
   922    688.4 MiB      5.0 MiB                   transparent=True)
   923                             
   924                                 #plot locations of data
   925    688.4 MiB      0.0 MiB       fig, ax = plt.subplots(1,
   926    688.4 MiB      0.0 MiB                              len(df_nufsamps.locnum.unique()),
   927    688.4 MiB      0.0 MiB                              figsize=(18, 4),
   928    690.4 MiB      2.0 MiB                              subplot_kw=dict(projection=projection))
   929    690.4 MiB      0.0 MiB       ax = ax.ravel()
   930                             
   931    690.9 MiB      0.5 MiB       da_zeros = xr.zeros_like(da_zp)
   932                             
   933    692.0 MiB      0.0 MiB       for i, site in enumerate(df_nufsamps.groupby('locnum')):
   934    692.0 MiB      0.0 MiB           ax[i].coastlines(color='k')
   935    692.0 MiB      0.0 MiB           ax[i].plot(site[1].lon.unique(),
   936    692.0 MiB      0.0 MiB                      site[1].lat.unique(),
   937    692.0 MiB      0.0 MiB                      c=colors[0],
   938    692.0 MiB      0.0 MiB                      ms=7,
   939    692.0 MiB      0.0 MiB                      marker='o',
   940    692.0 MiB      0.0 MiB                      transform=proj)
   941    692.0 MiB      0.0 MiB           ax[i].plot(site[1].lon.unique(),
   942    692.0 MiB      0.0 MiB                      site[1].lat.unique(),
   943    692.0 MiB      0.0 MiB                      c=colors[0],
   944    692.0 MiB      0.0 MiB                      ms=25,
   945    692.0 MiB      0.0 MiB                      marker='o',
   946    692.0 MiB      0.0 MiB                      transform=proj,
   947    692.0 MiB      0.0 MiB                      mfc="None",
   948    692.0 MiB      0.0 MiB                      mec='red',
   949    692.0 MiB      0.0 MiB                      mew=4)
   950    692.0 MiB      0.3 MiB           da_zeros[0].plot(ax=ax[i], cmap='Greys', add_colorbar=False)
   951    692.0 MiB      0.0 MiB           ax[i].set_title(site[0], fontsize=fontsize)
   952                                 # plt.tight_layout()
   953    692.0 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_realdata_fig_1Dlocs',
   954    696.8 MiB      4.7 MiB                   transparent=True)
   955                             
   956                                 #################   DECOMPOSE GPR INTO KERNELS ####################
   957                                 ##################  --------------------	 ######################
   958                             
   959                             
   960    768.4 MiB      0.0 MiB       def predict_decomp_f(m,
   961                                                      custom_kernel,
   962                                                      predict_at: tf.Tensor,
   963                                                      full_cov: bool = False,
   964                                                      full_output_cov: bool = False,
   965    696.8 MiB      0.0 MiB                            var=None):
   966                                     """Decompose GP into individual kernels."""
   967                             
   968    768.4 MiB      0.0 MiB           x_data, y_data = m.data
   969    768.4 MiB      0.0 MiB           err = y_data - m.mean_function(x_data)
   970    768.4 MiB      0.0 MiB           kmm = m.kernel(x_data)
   971    768.4 MiB      1.0 MiB           knn = custom_kernel(predict_at, full=full_cov)
   972   1055.1 MiB    287.1 MiB           kmn = custom_kernel(x_data, predict_at)
   973   1055.1 MiB      0.0 MiB           num_data = x_data.shape[0]
   974   1055.1 MiB      0.0 MiB           s = tf.linalg.diag(tf.convert_to_tensor(var))  # added diagonal variance
   975   1055.1 MiB      0.0 MiB           conditional = gpf.conditionals.base_conditional
   976   1055.1 MiB      0.0 MiB           f_mean_zero, f_var = conditional(
   977   1055.1 MiB      0.0 MiB               kmn, kmm + s, knn, err, full_cov=full_cov,
   978   1055.1 MiB     40.1 MiB               white=False)  # [N, P], [N, P] or [P, N, N]
   979   1056.1 MiB      1.0 MiB           f_mean = np.array(f_mean_zero + m.mean_function(predict_at))
   980   1056.1 MiB      0.0 MiB           f_var = np.array(f_var)
   981   1056.1 MiB      0.0 MiB           return f_mean, f_var
   982                             
   983                             
   984    768.4 MiB      0.0 MiB       def reshape_decomp(k, var=None):
   985    769.4 MiB      0.0 MiB           A, var = predict_decomp_f(m, k, xyt, var=var)
   986    769.4 MiB      0.0 MiB           A = A.reshape(nout, nout, len(ages))
   987    769.4 MiB      0.0 MiB           var = var.reshape(nout, nout, len(ages))
   988    769.4 MiB      0.0 MiB           return A, var
   989                             
   990                             
   991    769.4 MiB      0.0 MiB       def make_dataarray(da):
   992    769.4 MiB      0.0 MiB           coords = [lon, lat, ages]
   993    769.4 MiB      0.0 MiB           dims = ['lon', 'lat', 'age']
   994    769.4 MiB      0.0 MiB           return xr.DataArray(da, coords=coords,
   995    769.4 MiB      0.0 MiB                               dims=dims).transpose('age', 'lat', 'lon')
   996                             
   997                             
   998    696.8 MiB      0.0 MiB       A1, var1 = reshape_decomp(k1,
   999    696.8 MiB      0.0 MiB                                 var=df_place.rsl_er_max.ravel()**2 +
  1000    761.3 MiB      0.0 MiB                                 df_place.rsl_giaprior_std.ravel()**2)  #gia spatial
  1001    761.3 MiB      0.0 MiB       A2, var2 = reshape_decomp(k2,
  1002    761.3 MiB      0.0 MiB                                 var=df_place.rsl_er_max.ravel()**2 +
  1003    762.3 MiB      0.0 MiB                                 df_place.rsl_giaprior_std.ravel()**2)  #gia temporal
  1004    762.3 MiB      0.0 MiB       A3, var3 = reshape_decomp(
  1005    762.3 MiB      0.0 MiB           k3,
  1006    762.3 MiB      0.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1007    767.4 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance spatial
  1008    767.4 MiB      0.0 MiB       A4, var4 = reshape_decomp(
  1009    767.4 MiB      0.0 MiB           k4,
  1010    767.4 MiB      0.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1011    768.4 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance temporal
  1012    768.4 MiB      0.0 MiB       A5, var5 = reshape_decomp(
  1013    768.4 MiB      0.0 MiB           k5,
  1014    768.4 MiB      0.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1015    769.4 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance spatial
  1016                                 # A6, var6 = reshape_decomp(k6, var=df_place.rsl_er_max.ravel()**2 + df_place.rsl_giaprior_std.ravel()**2) #readvance temporal
  1017                             
  1018    769.4 MiB      0.0 MiB       da_A1 = make_dataarray(A1)
  1019    769.4 MiB      0.0 MiB       da_var1 = make_dataarray(var1)
  1020                             
  1021    769.4 MiB      0.0 MiB       da_A2 = make_dataarray(A2)
  1022    769.4 MiB      0.0 MiB       da_var2 = make_dataarray(var2)
  1023                             
  1024    769.4 MiB      0.0 MiB       da_A3 = make_dataarray(A3)
  1025    769.4 MiB      0.0 MiB       da_var3 = make_dataarray(var3)
  1026                             
  1027    769.4 MiB      0.0 MiB       da_A4 = make_dataarray(A4)
  1028    769.4 MiB      0.0 MiB       da_var4 = make_dataarray(var4)
  1029                             
  1030    769.4 MiB      0.0 MiB       da_A5 = make_dataarray(A5)
  1031    769.4 MiB      0.0 MiB       da_var5 = make_dataarray(var5)
  1032                             
  1033                                 #################   PLOT DECOMPOSED KERNELS    ####################
  1034                                 ##################  --------------------	   ####################
  1035                             
  1036    770.1 MiB      0.7 MiB       fig, ax = plt.subplots(1, 6, figsize=(24, 4))
  1037    770.1 MiB      0.0 MiB       ax = ax.ravel()
  1038    770.4 MiB      0.3 MiB       da_A1[0, :, :].plot(ax=ax[0], cmap='RdBu_r')
  1039                             
  1040    770.7 MiB      0.3 MiB       da_A2[0, :, :].plot(ax=ax[1], cmap='RdBu_r')
  1041                             
  1042    770.9 MiB      0.3 MiB       da_A3[0, :, :].plot(ax=ax[2], cmap='RdBu_r')
  1043                             
  1044    771.0 MiB      0.0 MiB       da_A4[:, 0, 0].plot(ax=ax[3])
  1045                             
  1046    771.0 MiB      0.0 MiB       da_A5[:, 0, 0].plot(ax=ax[4])
  1047                             
  1048                                 # da_A6[:,0,0].plot(ax=ax[5])
  1049                             
  1050                                 # plt.tight_layout()
  1051                             
  1052    771.0 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_decompkernels',
  1053    772.8 MiB      1.9 MiB                   transparent=True)


