{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_model = ['d6g_h6g_', 'glac1d_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from memory_profiler import profile\n",
    "\n",
    "# generic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# import dask.array as da\n",
    "import scipy.io as io\n",
    "from itertools import product\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import Normalize\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "# gpflow\n",
    "import gpflow as gpf\n",
    "from gpflow.utilities import print_summary\n",
    "from gpflow.logdensities import multivariate_normal\n",
    "from gpflow.kernels import Kernel\n",
    "from gpflow.mean_functions import MeanFunction\n",
    "from typing import Optional, Tuple, List\n",
    "from gpflow.config import default_jitter\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "import argparse\n",
    "\n",
    "# set the colormap and centre the colorbar\n",
    "class MidpointNormalize(Normalize):\n",
    "    \"\"\"Normalise the colorbar.  e.g. norm=MidpointNormalize(mymin, mymax, 0.)\"\"\"\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))\n",
    "\n",
    "\n",
    "####################  Initialize parameters #######################\n",
    "#################### ---------------------- #######################\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='import vars via c-line')\n",
    "# parser.add_argument(\"--tmax\", default=\"12010\")\n",
    "# parser.add_argument(\"--tmin\", default=\"3990\")\n",
    "# parser.add_argument(\"--place\", default=\"fennoscandia\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# tmax = int(args.tmax)\n",
    "# tmin = int(args.tmin)\n",
    "# place = args.place\n",
    "\n",
    "tmax = 8010\n",
    "tmin = 5990\n",
    "place = 'fennoscandia'\n",
    "\n",
    "ice_model =  ['d6g_h6g_', 'glac1d_'] \n",
    "\n",
    "locs = {'europe': [-20, 15, 35, 70],\n",
    "        'atlantic':[-85,50, 25, 73],\n",
    "        'fennoscandia': [-15, 50, 45, 75]\n",
    "       }\n",
    "extent = locs[place]\n",
    "tmax, tmin, tstep = int(tmax), int(tmin), 100\n",
    "\n",
    "ages_lgm = np.arange(100, 26000, tstep)[::-1]\n",
    "\n",
    "#import khan dataset\n",
    "path = 'data/GSL_LGM_120519_.csv'\n",
    "\n",
    "df = pd.read_csv(path, encoding=\"ISO-8859-15\", engine='python')\n",
    "df = df.replace('\\s+', '_', regex=True).replace('-', '_', regex=True).\\\n",
    "        applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "df.columns = df.columns.str.lower()\n",
    "df.rename_axis('index', inplace=True)\n",
    "df = df.rename({'latitude': 'lat', 'longitude': 'lon'}, axis='columns')\n",
    "dfind, dfterr, dfmar = df[(df.type == 0)\n",
    "                          & (df.age > 0)], df[df.type == 1], df[df.type == -1]\n",
    "np.sort(list(set(dfind.regionname1)))\n",
    "\n",
    "#select location\n",
    "df_place = dfind[(dfind.age > tmin) & (dfind.age < tmax) &\n",
    "                 (dfind.lon > extent[0])\n",
    "                 & (dfind.lon < extent[1])\n",
    "                 & (dfind.lat > extent[2])\n",
    "                 & (dfind.lat < extent[3])][[\n",
    "                     'lat', 'lon', 'rsl', 'rsl_er_max', 'age']]\n",
    "\n",
    "####################  Make 3D fingerprint  #######################\n",
    "#################### ---------------------- #######################\n",
    "\n",
    "filename = 'data/WAISreadvance_VM5_6ka_1step.mat'\n",
    "\n",
    "waismask = io.loadmat(filename, squeeze_me=True)\n",
    "ds_mask = xr.Dataset({'rsl': (['lat', 'lon', 'age'], waismask['RSL'])},\n",
    "                     coords={\n",
    "                         'lon': waismask['lon_out'],\n",
    "                         'lat': waismask['lat_out'],\n",
    "                         'age': np.round(waismask['ice_time_new'])\n",
    "                     })\n",
    "fingerprint = ds_mask.sel(age=ds_mask.age[0])\n",
    "\n",
    "\n",
    "def make_fingerprint(start, end, maxscale):\n",
    "\n",
    "    #palindromic scaling vector\n",
    "    def palindrome(maxscale, ages):\n",
    "        \"\"\" Make palindrome scale 0-maxval with number of steps. \"\"\"\n",
    "        half = np.linspace(0, maxscale, 1 + (len(ages) - 1) // 2)\n",
    "        scalefactor = np.concatenate([half, half[::-1]])\n",
    "        return scalefactor\n",
    "\n",
    "    ages_readv = ages_lgm[(ages_lgm < start) & (ages_lgm >= end)]\n",
    "    scale = palindrome(maxscale, ages_readv)\n",
    "\n",
    "    #scale factor same size as ice model ages\n",
    "    pre = np.zeros(np.where(ages_lgm == start)[0])\n",
    "    post = np.zeros(len(ages_lgm) - len(pre) - len(scale))\n",
    "\n",
    "    readv_scale = np.concatenate([pre, scale, post])\n",
    "\n",
    "    #scale factor into dataarray\n",
    "    da_scale = xr.DataArray(readv_scale, coords=[('age', ages_lgm)])\n",
    "\n",
    "    # broadcast fingerprint & scale to same dimensions;\n",
    "    fingerprint_out, fing_scaled = xr.broadcast(fingerprint.rsl, da_scale)\n",
    "\n",
    "    # mask fingerprint with scale to get LGM-pres timeseries\n",
    "    ds_fingerprint = (fingerprint_out *\n",
    "                      fing_scaled).transpose().to_dataset(name='rsl')\n",
    "\n",
    "    # scale dataset with fingerprint to LGM-present length & 0-max-0 over x years\n",
    "    xrlist = []\n",
    "    for i, key in enumerate(da_scale):\n",
    "        mask = ds_fingerprint.sel(age=ds_fingerprint.age[i].values) * key\n",
    "        mask = mask.assign_coords(scale=key,\n",
    "                                  age=ages_lgm[i]).expand_dims(dim=['age'])\n",
    "        xrlist.append(mask)\n",
    "    ds_readv = xr.concat(xrlist, dim='age')\n",
    "\n",
    "    ds_readv.coords['lon'] = pd.DataFrame((ds_readv.lon[ds_readv.lon >= 180] - 360)- 0.12) \\\n",
    "                            .append(pd.DataFrame(ds_readv.lon[ds_readv.lon < 180]) + 0.58) \\\n",
    "                            .reset_index(drop=True).squeeze()\n",
    "    ds_readv = ds_readv.swap_dims({'dim_0': 'lon'}).drop('dim_0')\n",
    "\n",
    "    # Add readv to modeled RSL at locations with data\n",
    "    ##### Need to fix this, as currently slice does not acknowledge new coords #########\n",
    "    ds_readv = ds_readv.sel(age=slice(tmax, tmin),\n",
    "                            lon=slice(df_place.lon.min() + 180 - 2,\n",
    "                                      df_place.lon.max() + 180 + 2),\n",
    "                            lat=slice(df_place.lat.max() + 2,\n",
    "                                      df_place.lat.min() - 2))\n",
    "    return ds_readv\n",
    "\n",
    "#Make deterministic readvance fingerprint\n",
    "start, end = 6100, 3000\n",
    "maxscale = 2.25\n",
    "ds_readv = make_fingerprint(start, end, maxscale)\n",
    "\n",
    "\n",
    "####################  Build  GIA models \t#######################\n",
    "#################### ---------------------- #######################\n",
    "\n",
    "#Use either glac1d or ICE6G\n",
    "def build_dataset(path, ice_model):\n",
    "    \"\"\"download model runs from local directory.\"\"\"\n",
    "    files = f'{path + ice_model}/*.nc'\n",
    "    basefiles = glob.glob(files)\n",
    "    modelrun = [\n",
    "       key.split('output_', 1)[1][:-3].replace('.', '_')\n",
    "    for key in basefiles]\n",
    "    dss = xr.open_mfdataset(files,\n",
    "                        chunks=None,\n",
    "                        concat_dim='modelrun',\n",
    "                        combine='nested')\n",
    "    lats, lons, times = dss.LAT.values[0], dss.LON.values[\n",
    "       0], dss.TIME.values[0]\n",
    "    ds = dss.drop(['LAT', 'LON', 'TIME'])\n",
    "    ds = ds.assign_coords(lat=lats,\n",
    "                      lon=lons,\n",
    "                      time=times,\n",
    "                      modelrun=modelrun).rename({\n",
    "                          'time': 'age',\n",
    "                          'RSL': 'rsl'})\n",
    "    return ds\n",
    "\n",
    "def one_mod(path, ice_model):\n",
    "    \"\"\"Organize model runs into xarray dataset.\"\"\"\n",
    "    path1 = path + f'{ice_model[0]}/output_'\n",
    "    path2 = path + f'{ice_model[1]}/output_'\n",
    "\n",
    "    ds1 = build_dataset(path, ice_model[0])\n",
    "    ds2 = build_dataset(path, ice_model[1])\n",
    "    ds2 = ds2.interp(age=ds1.age, lat=ds1.lat, lon=ds1.lon)\n",
    "\n",
    "    ds = xr.concat([ds1, ds2], dim='modelrun')\n",
    "\n",
    "    ds['age'] = ds['age'] * 1000\n",
    "    ds = ds.roll(lon=256, roll_coords=True)\n",
    "    ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \\\n",
    "                        .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \\\n",
    "                        .reset_index(drop=True).squeeze()\n",
    "    ds.coords['lat'] = ds.lat[::-1]\n",
    "    ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')\n",
    "\n",
    "    return ds\n",
    "\n",
    "def build_dataset(path, model):\n",
    "            \"\"\"download model runs from local directory.\"\"\"\n",
    "            path = path\n",
    "            files = f'{path}*.nc'\n",
    "            basefiles = glob.glob(files)\n",
    "            modelrun = [\n",
    "                key.split('output_', 1)[1][:-3].replace('.', '_')\n",
    "                for key in basefiles\n",
    "            ]\n",
    "            dss = xr.open_mfdataset(files,\n",
    "                                    chunks=None,\n",
    "                                    concat_dim='modelrun',\n",
    "                                    combine='nested')\n",
    "            lats, lons, times = dss.LAT.values[0], dss.LON.values[\n",
    "                0], dss.TIME.values[0]\n",
    "            ds = dss.drop(['LAT', 'LON', 'TIME'])\n",
    "            ds = ds.assign_coords(lat=lats,\n",
    "                                  lon=lons,\n",
    "                                  time=times,\n",
    "                                  modelrun=modelrun).rename({\n",
    "                                      'time': 'age',\n",
    "                                      'RSL': 'rsl'\n",
    "                                  })\n",
    "            return ds\n",
    "\n",
    "def one_mod(path, names):\n",
    "    \"\"\"Organize model runs into xarray dataset.\"\"\"\n",
    "    ds1 = build_dataset(path, names[0])\n",
    "    names = names[1:]\n",
    "    ds = ds1.chunk({'lat': 10, 'lon': 10})\n",
    "    for i in range(len(names)):\n",
    "        temp = build_dataset(names[i])\n",
    "        temp1 = temp.interp_like(ds1)\n",
    "        temp1['modelrun'] = temp['modelrun']\n",
    "        ds = xr.concat([ds, temp1], dim='modelrun')\n",
    "    ds['age'] = ds['age'] * 1000\n",
    "    ds = ds.roll(lon=256, roll_coords=True)\n",
    "    ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \\\n",
    "                            .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \\\n",
    "                            .reset_index(drop=True).squeeze()\n",
    "    ds.coords['lat'] = ds.lat[::-1]\n",
    "    ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')\n",
    "    return ds\n",
    "\n",
    "\n",
    "#make composite of a bunch of GIA runs, i.e. GIA prior\n",
    "# path = f'data/{ice_model}/output_'\n",
    "#path1 = 'data/glac1d_/output_'\n",
    "path1 = '/Users/rogercreel/Desktop/readv_files/glac1d_/output_'\n",
    "# path2 = 'data/d6g_h6g_/output_'\n",
    "path2 = '/Users/rogercreel/Desktop/readv_files/d6g_h6g_/output_'\n",
    "\n",
    "\n",
    "ds_sliced1 = one_mod(path1,[ice_model[0]])\n",
    "ds_sliced2 = one_mod(path2, [ice_model[1]])\n",
    "ds_sliced2 = ds_sliced2.interp(age=ds_sliced1.age)\n",
    "\n",
    "ds_sliced_in = xr.concat([ds_sliced1, ds_sliced2], dim='modelrun')\n",
    "\n",
    "ds_sliced = ds_sliced_in.rsl.assign_coords({'lat':ds_sliced_in.lat.values[::-1]}).sel(\n",
    "        age=slice(tmax, tmin),\n",
    "        lon=slice(df_place.lon.min() - 2,\n",
    "                  df_place.lon.max() + 2),\n",
    "        lat=slice(df_place.lat.max() + 2,\n",
    "                  df_place.lat.min() - 2))\n",
    "\n",
    "ds_area = ds_sliced.mean(dim='modelrun').chunk((-1,-1,-1)).interp(\n",
    "                                        age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat).to_dataset()\n",
    "ds_areastd = ds_sliced.std(dim='modelrun').chunk((-1,-1,-1)).interp(\n",
    "                                        age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat).to_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (age: 21, lat: 40, lon: 76)\n",
       "Coordinates:\n",
       "  * age      (age) int64 8000 7900 7800 7700 7600 ... 6400 6300 6200 6100 6000\n",
       "  * lon      (lon) float32 -10.540013 -9.840001 ... 42.190002 42.890003\n",
       "  * lat      (lat) float32 71.23 70.53 69.82 69.12 ... 45.96 45.26 44.56 43.86\n",
       "    scale    (age) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.15\n",
       "Data variables:\n",
       "    rsl      (age, lon, lat) float64 dask.array<chunksize=(21, 76, 40), meta=np.ndarray>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_sliced_in.rsl.mean(dim='modelrun').chunk((-1,-1,-1)).interp(\n",
    "                                        age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat).to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'rsl' (age: 21, lon: 76, lat: 40)>\n",
       "dask.array<_interpnd, shape=(21, 76, 40), dtype=float64, chunksize=(21, 76, 40), chunktype=numpy.ndarray>\n",
       "Coordinates:\n",
       "  * age      (age) int64 8000 7900 7800 7700 7600 ... 6400 6300 6200 6100 6000\n",
       "    scale    (age) float64 dask.array<chunksize=(21,), meta=np.ndarray>\n",
       "  * lon      (lon) float32 -10.540013 -9.840001 ... 42.190002 42.890003\n",
       "  * lat      (lat) float32 71.23 70.53 69.82 69.12 ... 45.96 45.26 44.56 43.86"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = ds_sliced_in.rsl.mean(dim='modelrun').interp(\n",
    "                                        age=ds_readv.age).chunk((21,-1,-1))\n",
    "test2 = test1.interp(lon=ds_readv.lon, lat=ds_readv.lat)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample each model at points where we have RSL data\n",
    "def ds_select(ds):\n",
    "    return ds.rsl.sel(age=[row.age],\n",
    "                      lon=[row.lon],\n",
    "                      lat=[row.lat],\n",
    "                      method='nearest').squeeze().values\n",
    "\n",
    "#select points at which RSL data exists\n",
    "for i, row in df_place.iterrows():\n",
    "    df_place.loc[i, 'rsl_realresid'] = df_place.rsl[i] - ds_select(ds_area)\n",
    "    df_place.loc[i, 'rsl_giaprior'] = ds_select(ds_area)\n",
    "    df_place.loc[i, 'rsl_giaprior_std'] = ds_select(ds_areastd)\n",
    "\n",
    "print('number of datapoints = ', df_place.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rogercreel/Desktop/readv_files/glac1d_/output_*.nc'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/rogercreel/Desktop/readv_files/glac1d_/output_'\n",
    "path = path\n",
    "files = f'{path}*.nc'\n",
    "basefiles = glob.glob(files)\n",
    "modelrun = [\n",
    "    key.split('output_', 1)[1][:-3].replace('.', '_')\n",
    "    for key in basefiles\n",
    "]\n",
    "dss = xr.open_mfdataset(files,\n",
    "                        chunks=None,\n",
    "                        concat_dim='modelrun',\n",
    "                        combine='nested')\n",
    "\n",
    "# ds_sliced1 = one_mod(path1,[ice_model[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from memory_profiler import profile\n",
    "\n",
    "# generic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# import dask.array as da\n",
    "import scipy.io as io\n",
    "from itertools import product\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import Normalize\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "# gpflow\n",
    "import gpflow as gpf\n",
    "from gpflow.utilities import print_summary\n",
    "from gpflow.logdensities import multivariate_normal\n",
    "from gpflow.kernels import Kernel\n",
    "from gpflow.mean_functions import MeanFunction\n",
    "from typing import Optional, Tuple, List\n",
    "from gpflow.config import default_jitter\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "import argparse\n",
    "\n",
    "#Use either glac1d or ICE6G\n",
    "def build_dataset(path, model):\n",
    "    \"\"\"download model runs from local directory.\"\"\"\n",
    "    path = path\n",
    "    files = f'{path + model}*.nc'\n",
    "    basefiles = glob.glob(files)\n",
    "    modelrun = [\n",
    "        key.split('output_', 1)[1][:-3].replace('.', '_')\n",
    "        for key in basefiles]\n",
    "    dss = xr.open_mfdataset(files,\n",
    "                            chunks=None,\n",
    "                            concat_dim='modelrun',\n",
    "                            combine='nested')\n",
    "    lats, lons, times = dss.LAT.values[0], dss.LON.values[\n",
    "        0], dss.TIME.values[0]\n",
    "    ds = dss.drop(['LAT', 'LON', 'TIME'])\n",
    "    ds = ds.assign_coords(lat=lats,\n",
    "                          lon=lons,\n",
    "                          time=times,\n",
    "                          modelrun=modelrun).rename({\n",
    "                              'time': 'age',\n",
    "                              'RSL': 'rsl'})\n",
    "    return ds\n",
    "\n",
    "def one_mod(path, names):\n",
    "    \"\"\"Organize model runs into xarray dataset.\"\"\"\n",
    "    ds1 = build_dataset(path, names[0])\n",
    "    names = names[1:]\n",
    "    ds = ds1.chunk({'lat': 10, 'lon': 10})\n",
    "    for i in range(len(names)):\n",
    "        temp = build_dataset(path, names[i])\n",
    "        temp1 = temp.interp(age=ds1.age, lat=ds1.lat, lon=ds1.lon)\n",
    "#         temp1['modelrun'] = temp['modelrun']\n",
    "        ds = xr.concat([ds, temp1], dim='modelrun')\n",
    "    ds['age'] = ds['age'] * 1000\n",
    "    ds = ds.roll(lon=256, roll_coords=True)\n",
    "    ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \\\n",
    "                            .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \\\n",
    "                            .reset_index(drop=True).squeeze()\n",
    "    ds.coords['lat'] = ds.lat[::-1]\n",
    "    ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')\n",
    "    return ds\n",
    "    \n",
    "# path = f'data/{ice_model}/output_'\n",
    "# path = f'/Users/rogercreel/Desktop/readv_files/output_'\n",
    "\n",
    "\n",
    "# ds_sliced_in = one_mod(path,ice_model).rsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dataset(path):\n",
    "    \"\"\"download model runs from local directory.\"\"\"\n",
    "    path = path\n",
    "    files = f'{path}*.nc'\n",
    "    basefiles = glob.glob(files)\n",
    "    modelrun = [\n",
    "        key.split('output_', 1)[1][:-3].replace('.', '_')\n",
    "        for key in basefiles\n",
    "    ]\n",
    "    dss = xr.open_mfdataset(files,\n",
    "                            chunks=None,\n",
    "                            concat_dim='modelrun',\n",
    "                            combine='nested')\n",
    "    lats, lons, times = dss.LAT.values[0], dss.LON.values[\n",
    "        0], dss.TIME.values[0]\n",
    "    ds = dss.drop(['LAT', 'LON', 'TIME'])\n",
    "    ds = ds.assign_coords(lat=lats,\n",
    "                          lon=lons,\n",
    "                          time=times,\n",
    "                          modelrun=modelrun).rename({\n",
    "                              'time': 'age',\n",
    "                              'RSL': 'rsl'\n",
    "                          })\n",
    "    return ds\n",
    "\n",
    "def one_mod(path, names):\n",
    "    \"\"\"Organize model runs into xarray dataset.\"\"\"\n",
    "    ds1 = build_dataset(path)\n",
    "    names = names[1:]\n",
    "    ds = ds1.chunk({'lat': 20, 'lon': 20})\n",
    "    for i in range(len(names)):\n",
    "        temp = build_dataset(names[i])\n",
    "        temp1 = temp.interp_like(ds1)\n",
    "        temp1['modelrun'] = temp['modelrun']\n",
    "        ds = xr.concat([ds, temp1], dim='modelrun')\n",
    "    ds['age'] = ds['age'] * 1000\n",
    "    ds = ds.roll(lon=256, roll_coords=True)\n",
    "    ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \\\n",
    "                            .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \\\n",
    "                            .reset_index(drop=True).squeeze()\n",
    "    ds.coords['lat'] = ds.lat[::-1]\n",
    "    ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d6g_h6g_'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ice_model[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/gpflow6_0/lib/python3.6/site-packages/dask/array/core.py:3877: PerformanceWarning: Increasing number of chunks by factor of 26\n",
      "  **blockwise_kwargs,\n",
      "/anaconda3/envs/gpflow6_0/lib/python3.6/site-packages/dask/array/core.py:3877: PerformanceWarning: Increasing number of chunks by factor of 13\n",
      "  **blockwise_kwargs,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (age: 261, lat: 256, lon: 512)\n",
       "Coordinates:\n",
       "  * lon      (lon) float64 -180.1 -179.4 -178.7 -178.0 ... 178.5 179.2 179.9\n",
       "  * lat      (lat) float64 -89.46 -88.77 -88.07 -87.37 ... 88.07 88.77 89.46\n",
       "  * age      (age) float64 -0.0 100.0 200.0 300.0 ... 2.58e+04 2.59e+04 2.6e+04\n",
       "Data variables:\n",
       "    rsl      (age, lon, lat) float64 dask.array<chunksize=(261, 20, 20), meta=np.ndarray>\n",
       "    ESL      (age) float64 dask.array<chunksize=(261,), meta=np.ndarray>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = 'data/glac1d_/output_'\n",
    "\n",
    "files = f'{path1}*.nc'\n",
    "basefiles = glob.glob(files)\n",
    "\n",
    "path2 = 'data/d6g_h6g_/output_'\n",
    "ds_sliced1 = one_mod(path1,[ice_model[0]])\n",
    "ds_sliced2 = one_mod(path2, [ice_model[1]])\n",
    "\n",
    "ds_sliced = xr.concat([ds_sliced1, ds_sliced2], dim='modelrun')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #Use either glac1d or ICE6G\n",
    "def build_dataset(path, ice_model):\n",
    "    \"\"\"download model runs from local directory.\"\"\"\n",
    "    files = f'{path + ice_model}/*.nc'\n",
    "    basefiles = glob.glob(files)\n",
    "    modelrun = [\n",
    "       key.split('output_', 1)[1][:-3].replace('.', '_')\n",
    "    for key in basefiles]\n",
    "    dss = xr.open_mfdataset(files,\n",
    "                        chunks=None,\n",
    "                        concat_dim='modelrun',\n",
    "                        combine='nested')\n",
    "    lats, lons, times = dss.LAT.values[0], dss.LON.values[\n",
    "       0], dss.TIME.values[0]\n",
    "    ds = dss.drop(['LAT', 'LON', 'TIME'])\n",
    "    ds = ds.assign_coords(lat=lats,\n",
    "                      lon=lons,\n",
    "                      time=times,\n",
    "                      modelrun=modelrun).rename({\n",
    "                          'time': 'age',\n",
    "                          'RSL': 'rsl'})\n",
    "    return ds\n",
    "\n",
    "def one_mod(path, ice_model):\n",
    "    \"\"\"Organize model runs into xarray dataset.\"\"\"\n",
    "    path1 = path + f'{ice_model[0]}/output_'\n",
    "    path2 = path + f'{ice_model[1]}/output_'\n",
    "\n",
    "    ds1 = build_dataset(path, ice_model[0])\n",
    "    ds2 = build_dataset(path, ice_model[1])\n",
    "    ds2 = ds2.interp(age=ds1.age, lat=ds1.lat, lon=ds1.lon)\n",
    "\n",
    "    ds = xr.concat([ds1, ds2], dim='modelrun')\n",
    "\n",
    "    ds['age'] = ds['age'] * 1000\n",
    "    ds = ds.roll(lon=256, roll_coords=True)\n",
    "    ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \\\n",
    "                        .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \\\n",
    "                        .reset_index(drop=True).squeeze()\n",
    "    ds.coords['lat'] = ds.lat[::-1]\n",
    "    ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')\n",
    "\n",
    "    return ds\n",
    "ice_model = ['d6g_h6g_', 'glac1d_']\n",
    "path = f'data/'\n",
    "\n",
    "    #make composite of a bunch of GIA runs, i.e. GIA prior\n",
    "    #         path = f'data/{ice_model}/output_'\n",
    "#     path = f'/Users/rogercreel/Desktop/readv_files/output_'\n",
    "\n",
    "\n",
    "ds_sliced_in = one_mod(path,ice_model).rsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = build_dataset(path, ice_model[0])\n",
    "ds2 = build_dataset(path, ice_model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sliced_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = f'{path + ice_model[0]}*.nc'\n",
    "basefiles = glob.glob(files)\n",
    "modelrun = [\n",
    "    key.split('output_', 1)[1][:-3].replace('.', '_')\n",
    "    for key in basefiles]\n",
    "dss = xr.open_mfdataset(files,\n",
    "                        chunks=None,\n",
    "                        concat_dim='modelrun',\n",
    "                        combine='nested')\n",
    "lats, lons, times = dss.LAT.values[0], dss.LON.values[\n",
    "    0], dss.TIME.values[0]\n",
    "ds = dss.drop(['LAT', 'LON', 'TIME'])\n",
    "ds = ds.assign_coords(lat=lats,\n",
    "                      lon=lons,\n",
    "                      time=times,\n",
    "                      modelrun=modelrun).rename({\n",
    "                          'time': 'age',\n",
    "                          'RSL': 'rsl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses conda environment gpflow6_0\n",
    "\n",
    "# #!/anaconda3/envs/gpflow6_0/env/bin/python\n",
    "\n",
    "from memory_profiler import profile\n",
    "\n",
    "# generic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# import dask.array as da\n",
    "import scipy.io as io\n",
    "from itertools import product\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import Normalize\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "# gpflow\n",
    "import gpflow as gpf\n",
    "from gpflow.utilities import print_summary\n",
    "from gpflow.logdensities import multivariate_normal\n",
    "from gpflow.kernels import Kernel\n",
    "from gpflow.mean_functions import MeanFunction\n",
    "from typing import Optional, Tuple, List\n",
    "from gpflow.config import default_jitter\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "import argparse\n",
    "\n",
    "# @profile\n",
    "\n",
    "# def readv():\n",
    "\n",
    "# set the colormap and centre the colorbar\n",
    "class MidpointNormalize(Normalize):\n",
    "    \"\"\"Normalise the colorbar.  e.g. norm=MidpointNormalize(mymin, mymax, 0.)\"\"\"\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))\n",
    "\n",
    "\n",
    "####################  Initialize parameters #######################\n",
    "#################### ---------------------- #######################\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='import vars via c-line')\n",
    "#     parser.add_argument(\"--mod\", default='d6g_h6g_')\n",
    "#     parser.add_argument(\"--lith\", default='l71C')\n",
    "#     parser.add_argument(\"--um\", default=\"p2\")\n",
    "#     parser.add_argument(\"--lm\", default=\"3\")\n",
    "#     parser.add_argument(\"--tmax\", default=\"4010\")\n",
    "#     parser.add_argument(\"--tmin\", default=\"2990\")\n",
    "#     parser.add_argument(\"--place\", default=\"fennoscandia\")\n",
    "\n",
    "ice_models =  ['glac1d_']#['d6g_h6g_'] # \n",
    "lith_thicknesses = ['l96C']\n",
    "um = 'p5'\n",
    "lm = '30'\n",
    "tmax = '12010'\n",
    "tmin = '7990'\n",
    "place = 'fennoscandia'\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# ice_models = [args.mod]\n",
    "# lith_thicknesses = [args.lith]\n",
    "# um = args.um\n",
    "# lm = args.lm\n",
    "# tmax = int(args.tmax)\n",
    "# tmin = int(args.tmin)\n",
    "# place = args.place\n",
    "\n",
    "for i, ice_model in enumerate(ice_models):\n",
    "    for k, lith_thickness in enumerate(lith_thicknesses):\n",
    "        plotting = 'true'\n",
    "        plot_heatmap = \"false\"\n",
    "        plot_nufsamps = 'false'\n",
    "        decomp = 'false'\n",
    "        ice_model = ice_model \n",
    "        lith_thickness = lith_thickness \n",
    "        model = ice_model + lith_thickness\n",
    "        place = place\n",
    "        mantle = f'um{um}_lm{lm}'\n",
    "\n",
    "        locs = {\n",
    "            'england': [-12, 2, 50, 60],\n",
    "            'easternhem': [50, 178, -45, 80],\n",
    "            'westernhem': [-175, 30, -80, 75],\n",
    "            'world': [-179.8, 179.8, -89.8, 89.8],\n",
    "            'namerica': [-150, -20, 10, 75],\n",
    "            'eastcoast': [-88, -65, 15, 40],\n",
    "            'europe': [-20, 15, 35, 70],\n",
    "            'atlantic':[-85,50, 25, 73],\n",
    "            'fennoscandia': [-15, 50, 45, 75],\n",
    "        }\n",
    "        extent = locs[place]\n",
    "        tmax, tmin, tstep = int(tmax), int(tmin), 100\n",
    "\n",
    "        ages_lgm = np.arange(100, 26000, tstep)[::-1]\n",
    "\n",
    "        #import khan dataset\n",
    "        path = 'data/GSL_LGM_120519_.csv'\n",
    "\n",
    "        df = pd.read_csv(path, encoding=\"ISO-8859-15\", engine='python')\n",
    "        df = df.replace('\\s+', '_', regex=True).replace('-', '_', regex=True).\\\n",
    "                applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "        df.columns = df.columns.str.lower()\n",
    "        df.rename_axis('index', inplace=True)\n",
    "        df = df.rename({'latitude': 'lat', 'longitude': 'lon'}, axis='columns')\n",
    "        dfind, dfterr, dfmar = df[(df.type == 0)\n",
    "                                  & (df.age > 0)], df[df.type == 1], df[df.type == -1]\n",
    "        np.sort(list(set(dfind.regionname1)))\n",
    "\n",
    "        #select location\n",
    "        df_place = dfind[(dfind.age > tmin) & (dfind.age < tmax) &\n",
    "                         (dfind.lon > extent[0])\n",
    "                         & (dfind.lon < extent[1])\n",
    "                         & (dfind.lat > extent[2])\n",
    "                         & (dfind.lat < extent[3])][[\n",
    "                             'lat', 'lon', 'rsl', 'rsl_er_max', 'age'\n",
    "                         ]]\n",
    "        df_place.shape\n",
    "\n",
    "        ####################  \tPlot locations  \t#######################\n",
    "        #################### ---------------------- #######################\n",
    "\n",
    "        #get counts by location rounded to nearest 0.1 degree\n",
    "        if plotting == 'true':\n",
    "            df_rnd = df_place.copy()\n",
    "            df_rnd.lat = np.round(df_rnd.lat, 1)\n",
    "            df_rnd.lon = np.round(df_rnd.lon, 1)\n",
    "            dfcounts_place = df_rnd.groupby(\n",
    "                ['lat', 'lon']).count().reset_index()[['lat', 'lon', 'rsl', 'age']]\n",
    "\n",
    "            #plot\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "            ax.set_extent(extent)\n",
    "            ax.coastlines(resolution='110m', linewidth=1, zorder=2)\n",
    "            ax.add_feature(cfeature.OCEAN, zorder=0)\n",
    "            ax.add_feature(cfeature.LAND, color='palegreen', zorder=1)\n",
    "            ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=3)\n",
    "            ax.gridlines(linewidth=1, color='white', alpha=0.5, zorder=4)\n",
    "            scat = ax.scatter(dfcounts_place.lon,\n",
    "                              dfcounts_place.lat,\n",
    "                              s=dfcounts_place.rsl * 70,\n",
    "                              c='lightsalmon',\n",
    "                              vmin=-20,\n",
    "                              vmax=20,\n",
    "                              cmap='coolwarm',\n",
    "                              edgecolor='k',\n",
    "                              linewidths=1,\n",
    "                              transform=ccrs.PlateCarree(),\n",
    "                              zorder=5)\n",
    "            size = Line2D(range(4),\n",
    "                          range(4),\n",
    "                          color=\"black\",\n",
    "                          marker='o',\n",
    "                          linewidth=0,\n",
    "                          linestyle='none',\n",
    "                          markersize=16,\n",
    "                          markerfacecolor=\"lightsalmon\")\n",
    "            labels = ['RSL datapoint location']\n",
    "            leg = plt.legend([size],\n",
    "                             labels,\n",
    "                             loc='lower left',\n",
    "                             bbox_to_anchor=(0.00, 0.00),\n",
    "                             prop={'size': 20},\n",
    "                             fancybox=True)\n",
    "            leg.get_frame().set_edgecolor('k')\n",
    "            ax.set_title('')\n",
    "\n",
    "        ####################  Make 3D fingerprint  #######################\n",
    "        #################### ---------------------- #######################\n",
    "\n",
    "        filename = 'data/WAISreadvance_VM5_6ka_1step.mat'\n",
    "\n",
    "        waismask = io.loadmat(filename, squeeze_me=True)\n",
    "        ds_mask = xr.Dataset({'rsl': (['lat', 'lon', 'age'], waismask['RSL'])},\n",
    "                             coords={\n",
    "                                 'lon': waismask['lon_out'],\n",
    "                                 'lat': waismask['lat_out'],\n",
    "                                 'age': np.round(waismask['ice_time_new'])\n",
    "                             })\n",
    "        fingerprint = ds_mask.sel(age=ds_mask.age[0])\n",
    "\n",
    "\n",
    "        def make_fingerprint(start, end, maxscale):\n",
    "\n",
    "            #palindromic scaling vector\n",
    "            def palindrome(maxscale, ages):\n",
    "                \"\"\" Make palindrome scale 0-maxval with number of steps. \"\"\"\n",
    "                half = np.linspace(0, maxscale, 1 + (len(ages) - 1) // 2)\n",
    "                scalefactor = np.concatenate([half, half[::-1]])\n",
    "                return scalefactor\n",
    "\n",
    "            ages_readv = ages_lgm[(ages_lgm < start) & (ages_lgm >= end)]\n",
    "            scale = palindrome(maxscale, ages_readv)\n",
    "\n",
    "            #scale factor same size as ice model ages\n",
    "            pre = np.zeros(np.where(ages_lgm == start)[0])\n",
    "            post = np.zeros(len(ages_lgm) - len(pre) - len(scale))\n",
    "\n",
    "            readv_scale = np.concatenate([pre, scale, post])\n",
    "\n",
    "            #scale factor into dataarray\n",
    "            da_scale = xr.DataArray(readv_scale, coords=[('age', ages_lgm)])\n",
    "\n",
    "            # broadcast fingerprint & scale to same dimensions;\n",
    "            fingerprint_out, fing_scaled = xr.broadcast(fingerprint.rsl, da_scale)\n",
    "\n",
    "            # mask fingerprint with scale to get LGM-pres timeseries\n",
    "            ds_fingerprint = (fingerprint_out *\n",
    "                              fing_scaled).transpose().to_dataset(name='rsl')\n",
    "\n",
    "            # scale dataset with fingerprint to LGM-present length & 0-max-0 over x years\n",
    "            xrlist = []\n",
    "            for i, key in enumerate(da_scale):\n",
    "                mask = ds_fingerprint.sel(age=ds_fingerprint.age[i].values) * key\n",
    "                mask = mask.assign_coords(scale=key,\n",
    "                                          age=ages_lgm[i]).expand_dims(dim=['age'])\n",
    "                xrlist.append(mask)\n",
    "            ds_readv = xr.concat(xrlist, dim='age')\n",
    "\n",
    "            ds_readv.coords['lon'] = pd.DataFrame((ds_readv.lon[ds_readv.lon >= 180] - 360)- 0.12) \\\n",
    "                                    .append(pd.DataFrame(ds_readv.lon[ds_readv.lon < 180]) + 0.58) \\\n",
    "                                    .reset_index(drop=True).squeeze()\n",
    "            ds_readv = ds_readv.swap_dims({'dim_0': 'lon'}).drop('dim_0')\n",
    "\n",
    "            # Add readv to modeled RSL at locations with data\n",
    "            ##### Need to fix this, as currently slice does not acknowledge new coords #########\n",
    "            ds_readv = ds_readv.sel(age=slice(tmax, tmin),\n",
    "                                    lon=slice(df_place.lon.min() + 180 - 2,\n",
    "                                              df_place.lon.max() + 180 + 2),\n",
    "                                    lat=slice(df_place.lat.max() + 2,\n",
    "                                              df_place.lat.min() - 2))\n",
    "            return ds_readv\n",
    "\n",
    "\n",
    "        #Make deterministic readvance fingerprint\n",
    "        start, end = 6100, 3000\n",
    "        maxscale = 2.25\n",
    "        ds_readv = make_fingerprint(start, end, maxscale)\n",
    "\n",
    "\n",
    "        ####################  Build  GIA models \t#######################\n",
    "        #################### ---------------------- #######################\n",
    "\n",
    "        #Use either glac1d or ICE6G\n",
    "\n",
    "\n",
    "        def build_dataset(path, model):\n",
    "            \"\"\"download model runs from local directory.\"\"\"\n",
    "            path = path\n",
    "            files = f'{path}*.nc'\n",
    "            basefiles = glob.glob(files)\n",
    "            modelrun = [\n",
    "                key.split('output_', 1)[1][:-3].replace('.', '_')\n",
    "                for key in basefiles\n",
    "            ]\n",
    "            dss = xr.open_mfdataset(files,\n",
    "                                    chunks=None,\n",
    "                                    concat_dim='modelrun',\n",
    "                                    combine='nested')\n",
    "            lats, lons, times = dss.LAT.values[0], dss.LON.values[\n",
    "                0], dss.TIME.values[0]\n",
    "            ds = dss.drop(['LAT', 'LON', 'TIME'])\n",
    "            ds = ds.assign_coords(lat=lats,\n",
    "                                  lon=lons,\n",
    "                                  time=times,\n",
    "                                  modelrun=modelrun).rename({\n",
    "                                      'time': 'age',\n",
    "                                      'RSL': 'rsl'\n",
    "                                  })\n",
    "            return ds\n",
    "\n",
    "        def one_mod(path, names):\n",
    "            \"\"\"Organize model runs into xarray dataset.\"\"\"\n",
    "            ds1 = build_dataset(path, names[0])\n",
    "            names = names[1:]\n",
    "            ds = ds1.chunk({'lat': 10, 'lon': 10})\n",
    "            for i in range(len(names)):\n",
    "                temp = build_dataset(names[i])\n",
    "                temp1 = temp.interp_like(ds1)\n",
    "                temp1['modelrun'] = temp['modelrun']\n",
    "                ds = xr.concat([ds, temp1], dim='modelrun')\n",
    "            ds['age'] = ds['age'] * 1000\n",
    "            ds = ds.roll(lon=256, roll_coords=True)\n",
    "            ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \\\n",
    "                                    .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \\\n",
    "                                    .reset_index(drop=True).squeeze()\n",
    "            ds.coords['lat'] = ds.lat[::-1]\n",
    "            ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')\n",
    "            return ds\n",
    "\n",
    "        #make composite of a bunch of GIA runs, i.e. GIA prior\n",
    "\n",
    "        if ice_model == 'glac1d_':\n",
    "#             path = f'data/glac1d_/output_{model}'\n",
    "            path = f'/Users/rogercreel/Desktop/readv_files/output_{model}'\n",
    "            \n",
    "            #make composite of a bunch of GIA runs, i.e. GIA prior\n",
    "            ds = one_mod(path, [model])\n",
    "            ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),\n",
    "                                   lon=slice(df_place.lon.min() - 2,\n",
    "                                             df_place.lon.max() + 2),\n",
    "                                   lat=slice(df_place.lat.min() - 2,\n",
    "                                             df_place.lat.max() + 2))\n",
    "\n",
    "        elif ice_model == 'd6g_h6g_':\n",
    "            path = f'data/d6g_h6g_/output_{model}'\n",
    "\n",
    "            #make GIA prior std.\n",
    "            ds = one_mod(path, [model])\n",
    "            ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),\n",
    "                                   lon=slice(df_place.lon.min() - 2,\n",
    "                                             df_place.lon.max() + 2),\n",
    "                                   lat=slice(df_place.lat.min() - 2,\n",
    "                                             df_place.lat.max() + 2))\n",
    "            \n",
    "        elif ice_model == 'd6g_h6gt_':\n",
    "            path = f'data/d6g_h6gt_/output_{model}'\n",
    "\n",
    "            #make GIA prior std.\n",
    "            ds = one_mod(path, [model])\n",
    "            ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),\n",
    "                               lon=slice(df_place.lon.min() - 2,\n",
    "                                         df_place.lon.max() + 2),\n",
    "                               lat=slice(df_place.lat.min() - 2,\n",
    "                                         df_place.lat.max() + 2))\n",
    "        ds_areastd = ds_sliced.mean(dim='modelrun').load().to_dataset().interp(\n",
    "            age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)\n",
    "        ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(\n",
    "            age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)\n",
    "\n",
    "        # loop through all models to calculate GPR log likelihood\n",
    "        # runs = ds.modelrun.values.tolist()\n",
    "        runs = [f'{ice_model}{lith_thickness}_um{um}_lm{lm}']\n",
    "\n",
    "        modrunlist = []\n",
    "        loglikelist = []\n",
    "        for i, modelrun in enumerate(runs):\n",
    "\n",
    "            print('---------------')\n",
    "            print(f'{modelrun} run number {i}')\n",
    "\n",
    "            ds_area = one_mod(path, [ice_model + lith_thickness]).sel(modelrun=modelrun).rsl\n",
    "            ds_area = ds_area.assign_coords({'lat':ds_area.lat.values[::-1]}).sel(\n",
    "                                    age=slice(tmax, tmin),\n",
    "                                    lon=slice(df_place.lon.min() - 2,\n",
    "                                              df_place.lon.max() + 2),\n",
    "                                    lat=slice(df_place.lat.max() + 2,\n",
    "                                              df_place.lat.min() - 2)).load().to_dataset().interp(\n",
    "                                    age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)\n",
    "\n",
    "            #sample each model at points where we have RSL data\n",
    "            def ds_select(ds):\n",
    "                return ds.rsl.sel(age=[row.age],\n",
    "                                  lon=[row.lon],\n",
    "                                  lat=[row.lat],\n",
    "                                  method='nearest').squeeze().values\n",
    "\n",
    "            #select points at which RSL data exists\n",
    "            for i, row in df_place.iterrows():\n",
    "                df_place.loc[i, 'rsl_realresid'] = df_place.rsl[i] - ds_select(ds_area)\n",
    "                df_place.loc[i, 'rsl_giaprior'] = ds_select(ds_area)\n",
    "                df_place.loc[i, 'rsl_giaprior_std'] = ds_select(ds_areastd)\n",
    "\n",
    "            print('number of datapoints = ', df_place.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path\n",
    "files = f'{path}*.nc'\n",
    "basefiles = glob.glob(files)\n",
    "modelrun = [key.split('output_', 1)[1][:-3].replace('.', '_') for key in basefiles]\n",
    "dss = xr.open_mfdataset(files,\n",
    "                        chunks=None,\n",
    "                        concat_dim='modelrun',\n",
    "                        combine='nested')\n",
    "lats, lons, times = dss.LAT.values[0], dss.LON.values[0], dss.TIME.values[0]\n",
    "ds = dss.drop(['LAT', 'LON', 'TIME'])\n",
    "ds = ds.assign_coords(lat=lats,\n",
    "                      lon=lons,\n",
    "                      time=times,\n",
    "                      modelrun=modelrun).rename({'time': 'age', 'RSL': 'rsl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_area = one_mod(path,\n",
    "                [ice_model + lith_thickness]).sel(modelrun=modelrun).rsl\n",
    "ds_area_out = ds_area.assign_coords({'lat':ds_area.lat.values[::-1]}).sel(\n",
    "    age=slice(tmax, tmin),\n",
    "    lon=slice(df_place.lon.min() - 2,\n",
    "              df_place.lon.max() + 2),\n",
    "    lat=slice(df_place.lat.max() + 2,\n",
    "              df_place.lat.min() - 2)).load().to_dataset()\n",
    "\n",
    "#.interp(\n",
    "                #  age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_area_out.rsl[0][10].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "            ##################\t  RUN GP REGRESSION \t#######################\n",
    "            ##################  --------------------\t ######################\n",
    "            start = time.time()\n",
    "\n",
    "            def run_gpr():\n",
    "\n",
    "                Data = Tuple[tf.Tensor, tf.Tensor]\n",
    "                likelihood = df_place.rsl_er_max.ravel()**2 # + df_place.rsl_giaprior_std.ravel()**2  # here we define likelihood\n",
    "\n",
    "                class GPR_diag(gpf.models.GPModel):\n",
    "                    r\"\"\"\n",
    "                    Gaussian Process Regression.\n",
    "                    This is a vanilla implementation of GP regression with a pointwise Gaussian\n",
    "                    likelihood.  Multiple columns of Y are treated independently.\n",
    "                    The log likelihood of this models is sometimes referred to as the 'marginal log likelihood',\n",
    "                    and is given by\n",
    "                    .. math::\n",
    "                       \\log p(\\mathbf y \\,|\\, \\mathbf f) =\n",
    "                            \\mathcal N\\left(\\mathbf y\\,|\\, 0, \\mathbf K + \\sigma_n \\mathbf I\\right)\n",
    "                    \"\"\"\n",
    "                    def __init__(self,\n",
    "                                 data: Data,\n",
    "                                 kernel: Kernel,\n",
    "                                 mean_function: Optional[MeanFunction] = None,\n",
    "                                 likelihood=likelihood):\n",
    "                        likelihood = gpf.likelihoods.Gaussian(variance=likelihood)\n",
    "                        _, y_data = data\n",
    "                        super().__init__(kernel,\n",
    "                                         likelihood,\n",
    "                                         mean_function,\n",
    "                                         num_latent=y_data.shape[-1])\n",
    "                        self.data = data\n",
    "\n",
    "                    def log_likelihood(self):\n",
    "                        \"\"\"\n",
    "                        Computes the log likelihood.\n",
    "                        \"\"\"\n",
    "                        x, y = self.data\n",
    "                        K = self.kernel(x)\n",
    "                        num_data = x.shape[0]\n",
    "                        k_diag = tf.linalg.diag_part(K)\n",
    "                        s_diag = tf.convert_to_tensor(self.likelihood.variance)\n",
    "                        jitter = tf.cast(tf.fill([num_data], default_jitter()),\n",
    "                                         'float64')  # stabilize K matrix w/jitter\n",
    "                        ks = tf.linalg.set_diag(K, k_diag + s_diag + jitter)\n",
    "                        L = tf.linalg.cholesky(ks)\n",
    "                        m = self.mean_function(x)\n",
    "\n",
    "                        # [R,] log-likelihoods for each independent dimension of Y\n",
    "                        log_prob = multivariate_normal(y, m, L)\n",
    "                        return tf.reduce_sum(log_prob)\n",
    "\n",
    "                    def predict_f(self,\n",
    "                                  predict_at: tf.Tensor,\n",
    "                                  full_cov: bool = False,\n",
    "                                  full_output_cov: bool = False):\n",
    "                        r\"\"\"\n",
    "                        This method computes predictions at X \\in R^{N \\x D} input points\n",
    "                        .. math::\n",
    "                            p(F* | Y)\n",
    "                        where F* are points on the GP at new data points, Y are noisy observations at training data points.\n",
    "                        \"\"\"\n",
    "                        x_data, y_data = self.data\n",
    "                        err = y_data - self.mean_function(x_data)\n",
    "\n",
    "                        kmm = self.kernel(x_data)\n",
    "                        knn = self.kernel(predict_at, full=full_cov)\n",
    "                        kmn = self.kernel(x_data, predict_at)\n",
    "\n",
    "                        num_data = x_data.shape[0]\n",
    "                        s = tf.linalg.diag(tf.convert_to_tensor(\n",
    "                            self.likelihood.variance))  #changed from normal GPR\n",
    "\n",
    "                        conditional = gpf.conditionals.base_conditional\n",
    "                        f_mean_zero, f_var = conditional(\n",
    "                            kmn, kmm + s, knn, err, full_cov=full_cov,\n",
    "                            white=False)  # [N, P], [N, P] or [P, N, N]\n",
    "                        f_mean = f_mean_zero + self.mean_function(predict_at)\n",
    "                        return f_mean, f_var\n",
    "\n",
    "\n",
    "                def normalize(df):\n",
    "                    return np.array((df - df.mean()) / df.std()).reshape(len(df), 1)\n",
    "\n",
    "\n",
    "                def denormalize(y_pred, df):\n",
    "                    return np.array((y_pred * df.std()) + df.mean())\n",
    "\n",
    "\n",
    "                def bounded_parameter(low, high, param):\n",
    "                    \"\"\"Make parameter tfp Parameter with optimization bounds.\"\"\"\n",
    "                    affine = tfb.AffineScalar(shift=tf.cast(low, tf.float64),\n",
    "                                              scale=tf.cast(high - low, tf.float64))\n",
    "                    sigmoid = tfb.Sigmoid()\n",
    "                    logistic = tfb.Chain([affine, sigmoid])\n",
    "                    parameter = gpf.Parameter(param, transform=logistic, dtype=tf.float64)\n",
    "                    return parameter\n",
    "\n",
    "\n",
    "                class HaversineKernel_Matern52(gpf.kernels.Matern52):\n",
    "                    \"\"\"\n",
    "                    Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.\n",
    "                    Assumes n dimensional data, with columns [latitude, longitude] in degrees.\n",
    "                    \"\"\"\n",
    "                    def __init__(\n",
    "                        self,\n",
    "                        lengthscale=1.0,\n",
    "                        variance=1.0,\n",
    "                        active_dims=None,\n",
    "                    ):\n",
    "                        super().__init__(\n",
    "                            active_dims=active_dims,\n",
    "                            variance=variance,\n",
    "                            lengthscale=lengthscale,\n",
    "                        )\n",
    "\n",
    "                    def haversine_dist(self, X, X2):\n",
    "                        pi = np.pi / 180\n",
    "                        f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D\n",
    "                        f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D\n",
    "                        d = tf.sin((f - f2) / 2)**2\n",
    "                        lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \\\n",
    "                                    tf.expand_dims(X2[:, 0] * pi, -2)\n",
    "                        cos_prod = tf.cos(lat2) * tf.cos(lat1)\n",
    "                        a = d[:, :, 0] + cos_prod * d[:, :, 1]\n",
    "                        c = tf.asin(tf.sqrt(a)) * 6371 * 2\n",
    "                        return c\n",
    "\n",
    "                    def scaled_squared_euclid_dist(self, X, X2):\n",
    "                        \"\"\"\n",
    "                        Returns (dist(X, X2ᵀ)/lengthscales)².\n",
    "                        \"\"\"\n",
    "                        if X2 is None:\n",
    "                            X2 = X\n",
    "                        dist = da.square(self.haversine_dist(X, X2) / self.lengthscale)\n",
    "                #             dist = tf.convert_to_tensor(dist)\n",
    "                        return dist\n",
    "\n",
    "\n",
    "                class HaversineKernel_Matern32(gpf.kernels.Matern32):\n",
    "                    \"\"\"\n",
    "                    Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.\n",
    "                    Assumes n dimensional data, with columns [latitude, longitude] in degrees.\n",
    "                    \"\"\"\n",
    "                    def __init__(\n",
    "                        self,\n",
    "                        lengthscale=1.0,\n",
    "                        variance=1.0,\n",
    "                        active_dims=None,\n",
    "                    ):\n",
    "                        super().__init__(\n",
    "                            active_dims=active_dims,\n",
    "                            variance=variance,\n",
    "                            lengthscale=lengthscale,\n",
    "                        )\n",
    "\n",
    "                    def haversine_dist(self, X, X2):\n",
    "                        pi = np.pi / 180\n",
    "                        f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D\n",
    "                        f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D\n",
    "                        d = tf.sin((f - f2) / 2)**2\n",
    "                        lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \\\n",
    "                                    tf.expand_dims(X2[:, 0] * pi, -2)\n",
    "                        cos_prod = tf.cos(lat2) * tf.cos(lat1)\n",
    "                        a = d[:, :, 0] + cos_prod * d[:, :, 1]\n",
    "                        c = tf.asin(tf.sqrt(a)) * 6371 * 2\n",
    "                        return c\n",
    "\n",
    "                    def scaled_squared_euclid_dist(self, X, X2):\n",
    "                        \"\"\"\n",
    "                        Returns (dist(X, X2ᵀ)/lengthscales)².\n",
    "                        \"\"\"\n",
    "                        if X2 is None:\n",
    "                            X2 = X\n",
    "                        dist = tf.square(self.haversine_dist(X, X2) / self.lengthscale)\n",
    "                #             dist = tf.convert_to_tensor(dist) # return to tensorflow\n",
    "                        return dist\n",
    "\n",
    "\n",
    "                ########### Section to Run GPR######################\n",
    "                ##################################3#################\n",
    "\n",
    "                # Input space, rsl normalized to zero mean, unit variance\n",
    "                X = np.stack((df_place['lon'], df_place['lat'], df_place['age']), 1)\n",
    "                RSL = normalize(df_place.rsl_realresid)\n",
    "\n",
    "                #define kernels  with bounds\n",
    "\n",
    "                k1 = HaversineKernel_Matern32(active_dims=[0, 1])\n",
    "                k1.lengthscale = bounded_parameter(5000, 30000, 10000)  #hemispheric space\n",
    "                k1.variance = bounded_parameter(0.1, 100, 2)\n",
    "\n",
    "                # k1 = gpf.kernels.Matern32(active_dims=[0, 1])\n",
    "                # k1.lengthscale = bounded_parameter(50, 500, 60)  #hemispheric space\n",
    "                # k1.variance = bounded_parameter(0.05, 100, 2)\n",
    "\n",
    "                k2 = HaversineKernel_Matern32(active_dims=[0, 1])\n",
    "                k2.lengthscale = bounded_parameter(1, 5000, 1000)  #GIA space\n",
    "                k2.variance = bounded_parameter(0.1, 100, 2)\n",
    "\n",
    "                # k2 = gpf.kernels.Matern32(active_dims=[0,1])\n",
    "                # k2.lengthscale = bounded_parameter(1, 50, 5)  #GIA space\n",
    "                # k2.variance = bounded_parameter(0.05, 100, 2)\n",
    "\n",
    "                k3 = gpf.kernels.Matern32(active_dims=[2])  #GIA time\n",
    "                k3.lengthscale = bounded_parameter(8000, 20000, 10000)\n",
    "                k3.variance = bounded_parameter(0.1, 100, 1)\n",
    "\n",
    "                k4 = gpf.kernels.Matern32(active_dims=[2])  #shorter time\n",
    "                k4.lengthscale = bounded_parameter(1, 8000, 1000)\n",
    "                k4.variance = bounded_parameter(0.1, 100, 1)\n",
    "\n",
    "                k5 = gpf.kernels.White(active_dims=[2])\n",
    "                k5.variance = bounded_parameter(0.01, 100, 1)\n",
    "\n",
    "                kernel = (k1 * k3) + (k2 * k4) + k5\n",
    "\n",
    "                #build & train model\n",
    "                m = GPR_diag((X, RSL), kernel=kernel, likelihood=likelihood)\n",
    "                print('model built, time=', time.time() - start)\n",
    "\n",
    "\n",
    "                @tf.function(autograph=False)\n",
    "                def objective():\n",
    "                    return - m.log_marginal_likelihood()\n",
    "\n",
    "                o = gpf.optimizers.Scipy()\n",
    "                o.minimize(objective, variables=m.trainable_variables)\n",
    "                print('model minimized, time=', time.time() - start)\n",
    "\n",
    "                # output space\n",
    "                nout = 70\n",
    "                lat = np.linspace(min(ds_area.lat), max(ds_area.lat), nout)\n",
    "                lon = np.linspace(min(ds_area.lon), max(ds_area.lon), nout)\n",
    "                ages = ages_lgm[(ages_lgm < tmax) & (ages_lgm > tmin)]\n",
    "                xyt = np.array(list(product(lon, lat, ages)))\n",
    "\n",
    "                #query model & renormalize data\n",
    "                y_pred, var = m.predict_f(xyt)\n",
    "                y_pred_out = denormalize(y_pred, df_place.rsl_realresid)\n",
    "\n",
    "                #reshape output vectors\n",
    "                Zp = np.array(y_pred_out).reshape(nout, nout, len(ages))\n",
    "                varp = np.array(var).reshape(nout, nout, len(ages))\n",
    "\n",
    "                #print kernel details\n",
    "                print_summary(m, fmt='notebook')\n",
    "                print('time elapsed = ', time.time() - start)\n",
    "\n",
    "                print('negative log marginal likelihood =',\n",
    "                      m.neg_log_marginal_likelihood().numpy())\n",
    "\n",
    "\n",
    "                modrunlist.append(modelrun)\n",
    "                loglikelist.append(m.neg_log_marginal_likelihood().numpy())\n",
    "\n",
    "\n",
    "\n",
    "                ##################\t  INTERPOLATE MODELS \t#######################\n",
    "                ##################  --------------------\t ######################\n",
    "\n",
    "                # turn GPR output into xarray dataarray\n",
    "                da_zp = xr.DataArray(Zp, coords=[lon, lat, ages],\n",
    "                                     dims=['lon', 'lat',\n",
    "                                           'age']).transpose('age', 'lat', 'lon')\n",
    "                da_varp = xr.DataArray(varp,\n",
    "                                       coords=[lon, lat, ages],\n",
    "                                       dims=['lon', 'lat',\n",
    "                                             'age']).transpose('age', 'lat', 'lon')\n",
    "\n",
    "\n",
    "                def interp_likegpr(ds):\n",
    "                    return ds.rsl.load().transpose().interp_like(da_zp)\n",
    "\n",
    "\n",
    "                #interpolate all models onto GPR grid\n",
    "                da_giapriorinterp = interp_likegpr(ds_area)\n",
    "                ds_giapriorinterp = ds_area.interp(age=ages)\n",
    "                da_giapriorinterpstd = interp_likegpr(ds_areastd)\n",
    "\n",
    "                # add total prior RSL back into GPR\n",
    "                da_priorplusgpr = da_zp + da_giapriorinterp\n",
    "\n",
    "                return k1, k2, k3, k4, k5, nout, xyt, m, ages, da_zp, ds_giapriorinterp, da_giapriorinterpstd, da_giapriorinterp, da_priorplusgpr, da_varp, modrunlist, loglikelist\n",
    "\n",
    "            k1, k2, k3, k4, k5, nout, xyt, m, ages, da_zp, ds_giapriorinterp, da_giapriorinterpstd, da_giapriorinterp, da_priorplusgpr, da_varp, modrunlist, loglikelist = run_gpr()\n",
    "            ##################\t  \t SAVE NETCDFS \t \t#######################\n",
    "            ##################  --------------------\t ######################\n",
    "\n",
    "            path_gen = f'{ages[0]}_{ages[-1]}_{model}_{mantle}_{place}'\n",
    "#                 da_zp.to_netcdf('output/' + path_gen + '_dazp')\n",
    "#                 da_giapriorinterp.to_netcdf('output/' + path_gen + '_giaprior')\n",
    "#                 da_priorplusgpr.to_netcdf('output/' + path_gen + '_posterior')\n",
    "#                 da_varp.to_netcdf('output/' + path_gen + '_gpvariance')\n",
    "\n",
    "            ##################\t\t  PLOT  MODELS \t\t#######################\n",
    "            ##################  --------------------\t ######################\n",
    "            dirName = f'figs/{place}/'\n",
    "            if not os.path.exists(dirName):\n",
    "                os.mkdir(dirName)\n",
    "                print(\"Directory \", dirName, \" Created \")\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if plotting == 'true':\n",
    "                for i, age in enumerate(ages):\n",
    "                    if (age / 500).is_integer():\n",
    "                        step = (ages[0] - ages[1])\n",
    "                        df_it = df_place[(df_place.age < age) & (df_place.age > age - step)]\n",
    "                        resid_it = da_zp.sel(age=slice(age, age - step))\n",
    "                        rsl, var = df_it.rsl, df_it.rsl_er_max.values**2\n",
    "                        lat_it, lon_it = df_it.lat, df_it.lon\n",
    "\n",
    "                        if ice_model == 'glac1d_':\n",
    "                            vmin = ds_giapriorinterp.rsl.min().values  \n",
    "                            vmax = ds_giapriorinterp.rsl.max().values  \n",
    "                        elif ice_model =='d6g_h6g_':\n",
    "                            vmin = ds_giapriorinterp.rsl.min().values  + 20\n",
    "                            vmax = ds_giapriorinterp.rsl.max().values  + 10\n",
    "\n",
    "                        vmin_std = 0\n",
    "                        vmax_std = 1\n",
    "                        tmin_it = np.round(age - step, 2)\n",
    "                        tmax_it = np.round(age, 2)\n",
    "                        cbarscale = 0.3\n",
    "                        fontsize = 20\n",
    "                        cmap = 'coolwarm'\n",
    "                        cbar_kwargs = {'shrink': cbarscale, 'label': 'RSL (m)'}\n",
    "\n",
    "                        proj = ccrs.PlateCarree()\n",
    "                        projection = ccrs.PlateCarree()\n",
    "                        fig, (ax1, ax2, ax3,\n",
    "                              ax4) = plt.subplots(1,\n",
    "                                                  4,\n",
    "                                                  figsize=(24, 16),\n",
    "                                                  subplot_kw=dict(projection=projection))\n",
    "\n",
    "                        # total prior mean + \"true\" data\n",
    "                        ax1.coastlines(color='k')\n",
    "                        pc1 = ds_giapriorinterp.rsl[i].transpose().plot(ax=ax1,\n",
    "                                                                        transform=proj,\n",
    "                                                                        cmap=cmap,\n",
    "                                                                        norm=MidpointNormalize(\n",
    "                                                                            vmin, vmax, 0),\n",
    "                                                                        add_colorbar=False,\n",
    "                                                                        extend='both')\n",
    "                        cbar = fig.colorbar(pc1,\n",
    "                                            ax=ax1,\n",
    "                                            shrink=.3,\n",
    "                                            label='RSL (m)',\n",
    "                                            extend='both')\n",
    "                        scat = ax1.scatter(lon_it,\n",
    "                                           lat_it,\n",
    "                                           s=80,\n",
    "                                           c=rsl,\n",
    "                                           edgecolor='k',\n",
    "                                           vmin=vmin,\n",
    "                                           vmax=vmax,\n",
    "                                           norm=MidpointNormalize(vmin, vmax, 0),\n",
    "                                           cmap=cmap)\n",
    "                        ax1.set_title(f'{np.round(ds_giapriorinterp.rsl[i].age.values, -1)} yrs',\n",
    "                                      fontsize=fontsize)\n",
    "                        #         ax1.set_extent(extent_)\n",
    "\n",
    "                        # Learned difference between prior and \"true\" data\n",
    "                        ax2.coastlines(color='k')\n",
    "                        pc = da_zp[i, :, :].plot(ax=ax2,\n",
    "                                                 transform=proj,\n",
    "                                                 cmap=cmap,\n",
    "                                                 extend='both',\n",
    "                                                 norm=MidpointNormalize(\n",
    "                                                     resid_it.min(), resid_it.max(), 0),\n",
    "                                                 add_colorbar=False)\n",
    "                        cbar = fig.colorbar(pc,\n",
    "                                            ax=ax2,\n",
    "                                            shrink=.3,\n",
    "                                            label='RSL (m)',\n",
    "                                            extend='both')\n",
    "                        scat = ax2.scatter(lon_it,\n",
    "                                           lat_it,\n",
    "                                           s=80,\n",
    "                                           facecolors='k',\n",
    "                                           cmap=cmap,\n",
    "                                           edgecolor='k',\n",
    "                                           transform=proj,\n",
    "                                           norm=MidpointNormalize(resid_it.min(),\n",
    "                                                                  resid_it.max(), 0))\n",
    "                        ax2.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)\n",
    "                        #         ax2.set_extent(extent_)\n",
    "\n",
    "                        # GP regression\n",
    "                        ax3.coastlines(color='k')\n",
    "                        pc = da_priorplusgpr[i].plot(ax=ax3,\n",
    "                                                     transform=proj,\n",
    "                                                     norm=MidpointNormalize(vmin, vmax, 0),\n",
    "                                                     cmap=cmap,\n",
    "                                                     extend='both',\n",
    "                                                     add_colorbar=False)\n",
    "                        scat = ax3.scatter(lon_it,\n",
    "                                           lat_it,\n",
    "                                           s=80,\n",
    "                                           c=rsl,\n",
    "                                           edgecolor='k',\n",
    "                                           cmap=cmap,\n",
    "                                           norm=MidpointNormalize(vmin, vmax, 0))\n",
    "                        cbar = fig.colorbar(pc,\n",
    "                                            ax=ax3,\n",
    "                                            shrink=.3,\n",
    "                                            label='RSL (m)',\n",
    "                                            extend='both')\n",
    "                        ax3.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)\n",
    "                        #         ax3.set_extent(extent_)\n",
    "\n",
    "                        #GP regression standard deviation\n",
    "                        ax4.coastlines(color='k')\n",
    "                        pc = (2 * np.sqrt(da_varp[i])).plot(\n",
    "                            ax=ax4,\n",
    "                            transform=proj,\n",
    "                            vmin=vmin_std,\n",
    "                            vmax=vmax_std * 2,\n",
    "                            cmap='Reds',\n",
    "                            extend='both',\n",
    "                            add_colorbar=False,\n",
    "                        )\n",
    "                        scat = ax4.scatter(lon_it,\n",
    "                                           lat_it,\n",
    "                                           s=80,\n",
    "                                           c=2 * np.sqrt(var),\n",
    "                                           vmin=vmin_std,\n",
    "                                           vmax=vmax_std * 2,\n",
    "                                           cmap='Reds',\n",
    "                                           edgecolor='k',\n",
    "                                           transform=proj)\n",
    "                        cbar = fig.colorbar(pc,\n",
    "                                            ax=ax4,\n",
    "                                            shrink=.3,\n",
    "                                            extend='both',\n",
    "                                            label='RSL (m) (2 $\\sigma$)')\n",
    "                        ax4.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)\n",
    "                #         ax4.set_extent(extent_)\n",
    "\n",
    "                ########## ----- Save figures -------- #######################\n",
    "#                             fig.savefig(dirName + f'{path_gen}_{age}_3Dfig', transparent=True)\n",
    "\n",
    "                ##################\tCHOOSE LOCS W/NUF SAMPS #######################\n",
    "                ##################  --------------------\t ######################\n",
    "\n",
    "                if plot_nufsamps == 'true':\n",
    "                    def locs_with_enoughsamples(df_place, place, number):\n",
    "                        \"\"\"make new dataframe, labeled, of sites with [> number] measurements\"\"\"\n",
    "                        df_lots = df_place.groupby(['lat',\n",
    "                                                    'lon']).filter(lambda x: len(x) > number)\n",
    "\n",
    "                        df_locs = []\n",
    "                        for i, group in enumerate(df_lots.groupby(['lat', 'lon'])):\n",
    "                            singleloc = group[1].copy()\n",
    "                            singleloc['location'] = place\n",
    "                            singleloc['locnum'] = place + '_site' + str(\n",
    "                                i)  # + singleloc.reset_index().index.astype('str')\n",
    "                            df_locs.append(singleloc)\n",
    "                        df_locs = pd.concat(df_locs)\n",
    "\n",
    "                        return df_locs\n",
    "\n",
    "\n",
    "                    number = 3\n",
    "                    df_nufsamps = locs_with_enoughsamples(df_place, place, number)\n",
    "                    len(df_nufsamps.locnum.unique())\n",
    "\n",
    "                    ##################\tPLOT LOCS W/NUF SAMPS   #######################\n",
    "                    ##################  --------------------\t ######################\n",
    "\n",
    "\n",
    "                    def slice_dataarray(da):\n",
    "                        return da.sel(lat=site[1].lat.unique(),\n",
    "                                      lon=site[1].lon.unique(),\n",
    "                                      method='nearest')\n",
    "\n",
    "\n",
    "                    fig, ax = plt.subplots(4, len(df_nufsamps.locnum.unique()), figsize=(18, 16))\n",
    "                    ax = ax.ravel()\n",
    "                    colors = ['darkgreen', 'darkblue', 'darkred']\n",
    "                    fontsize = 18\n",
    "\n",
    "                    for i, site in enumerate(df_nufsamps.groupby('locnum')):\n",
    "\n",
    "                        #slice data for each site\n",
    "                        prior_it = slice_dataarray(da_giapriorinterp)\n",
    "                        priorvar_it = slice_dataarray(da_giapriorinterpstd)\n",
    "                        top_prior = prior_it + priorvar_it * 2\n",
    "                        bottom_prior = prior_it - priorvar_it * 2\n",
    "\n",
    "                        var_it = slice_dataarray(np.sqrt(da_varp))\n",
    "                        post_it = slice_dataarray(da_priorplusgpr)\n",
    "                        top = post_it + var_it * 2\n",
    "                        bottom = post_it - var_it * 2\n",
    "\n",
    "                        site_err = 2 * (site[1].rsl_er_max)\n",
    "\n",
    "                        ax[i].scatter(site[1].age, site[1].rsl, c=colors[0], label='\"true\" RSL')\n",
    "                        ax[i].errorbar(\n",
    "                            site[1].age,\n",
    "                            site[1].rsl,\n",
    "                            site_err,\n",
    "                            c=colors[0],\n",
    "                            fmt='none',\n",
    "                            capsize=1,\n",
    "                            lw=1,\n",
    "                        )\n",
    "\n",
    "                        prior_it.plot(ax=ax[i], c=colors[2], label='Prior $\\pm 2 \\sigma$')\n",
    "                        ax[i].fill_between(prior_it.age,\n",
    "                                           bottom_prior.squeeze(),\n",
    "                                           top_prior.squeeze(),\n",
    "                                           color=colors[2],\n",
    "                                           alpha=0.3)\n",
    "\n",
    "                        post_it.plot(ax=ax[i], c=colors[1], label='Posterior $\\pm 2 \\sigma$')\n",
    "                        ax[i].fill_between(post_it.age,\n",
    "                                           bottom.squeeze(),\n",
    "                                           top.squeeze(),\n",
    "                                           color=colors[1],\n",
    "                                           alpha=0.3)\n",
    "                        #     ax[i].set_title(f'{site[0]} RSL', fontsize=fontsize)\n",
    "                        ax[i].set_title('')\n",
    "\n",
    "                        ax[i].legend(loc='lower left')\n",
    "\n",
    "#                         fig.savefig(dirName + f'{path_gen}_1Dfig',\n",
    "#                                     transparent=True)\n",
    "\n",
    "                    #plot locations of data\n",
    "                    fig, ax = plt.subplots(2,len(df_nufsamps.locnum.unique()),\n",
    "                                           figsize=(18, 4), subplot_kw=dict(projection=projection))\n",
    "                    ax = ax.ravel()\n",
    "\n",
    "                    da_zeros = xr.zeros_like(da_zp)\n",
    "\n",
    "                    for i, site in enumerate(df_nufsamps.groupby('locnum')):\n",
    "                        ax[i].coastlines(color='k')\n",
    "                        ax[i].plot(site[1].lon.unique(),\n",
    "                                   site[1].lat.unique(),\n",
    "                                   c=colors[0],\n",
    "                                   ms=7,\n",
    "                                   marker='o',\n",
    "                                   transform=proj)\n",
    "                        ax[i].plot(site[1].lon.unique(),\n",
    "                                   site[1].lat.unique(),\n",
    "                                   c=colors[0],\n",
    "                                   ms=25,\n",
    "                                   marker='o',\n",
    "                                   transform=proj,\n",
    "                                   mfc=\"None\",\n",
    "                                   mec='red',\n",
    "                                   mew=4)\n",
    "                        da_zeros[0].plot(ax=ax[i], cmap='Greys', add_colorbar=False)\n",
    "                        ax[i].set_title(site[0], fontsize=fontsize)\n",
    "\n",
    "#                         fig.savefig(dirName + f'{path_gen}_1Dfig_locs', transparent=True)\n",
    "\n",
    "                #################   DECOMPOSE GPR INTO KERNELS ####################\n",
    "                ##################  --------------------\t ######################\n",
    "\n",
    "                if decomp == 'true':\n",
    "\n",
    "                    def predict_decomp_f(m,\n",
    "                                         custom_kernel,\n",
    "                                         predict_at: tf.Tensor,\n",
    "                                         full_cov: bool = False,\n",
    "                                         full_output_cov: bool = False,\n",
    "                                         var=None):\n",
    "                        \"\"\"Decompose GP into individual kernels.\"\"\"\n",
    "\n",
    "                        x_data, y_data = m.data\n",
    "                        err = y_data - m.mean_function(x_data)\n",
    "                        kmm = m.kernel(x_data)\n",
    "                        knn = custom_kernel(predict_at, full=full_cov)\n",
    "                        kmn = custom_kernel(x_data, predict_at)\n",
    "                        num_data = x_data.shape[0]\n",
    "                        s = tf.linalg.diag(tf.convert_to_tensor(var))  # added diagonal variance\n",
    "                        conditional = gpf.conditionals.base_conditional\n",
    "                        f_mean_zero, f_var = conditional(\n",
    "                            kmn, kmm + s, knn, err, full_cov=full_cov,\n",
    "                            white=False)  # [N, P], [N, P] or [P, N, N]\n",
    "                        f_mean = np.array(f_mean_zero + m.mean_function(predict_at))\n",
    "                        f_var = np.array(f_var)\n",
    "                        return f_mean, f_var\n",
    "\n",
    "\n",
    "                    def reshape_decomp(k, var=None):\n",
    "                        A, var = predict_decomp_f(m, k, xyt, var=var)\n",
    "                        A = A.reshape(nout, nout, len(ages))\n",
    "                        var = var.reshape(nout, nout, len(ages))\n",
    "                        return A, var\n",
    "\n",
    "\n",
    "                    def make_dataarray(da):\n",
    "                        coords = [lon, lat, ages]\n",
    "                        dims = ['lon', 'lat', 'age']\n",
    "                        return xr.DataArray(da, coords=coords,\n",
    "                                            dims=dims).transpose('age', 'lat', 'lon')\n",
    "\n",
    "\n",
    "                    A1, var1 = reshape_decomp(k1,var=df_place.rsl_er_max.ravel()**2)  #gia spatial\n",
    "                    A2, var2 = reshape_decomp(k2,var=df_place.rsl_er_max.ravel()**2)  #gia temporal\n",
    "                    A3, var3 = reshape_decomp(k3,var=df_place.rsl_er_max.ravel()**2)  #readvance spatial\n",
    "                    A4, var4 = reshape_decomp(k4,var=df_place.rsl_er_max.ravel()**2)  #readvance temporal\n",
    "                    A5, var5 = reshape_decomp(k5, var=df_place.rsl_er_max.ravel()**2)  #readvance spatial\n",
    "\n",
    "                    da_A1 = make_dataarray(A1)\n",
    "                    da_var1 = make_dataarray(var1)\n",
    "\n",
    "                    da_A2 = make_dataarray(A2)\n",
    "                    da_var2 = make_dataarray(var2)\n",
    "\n",
    "                    da_A3 = make_dataarray(A3)\n",
    "                    da_var3 = make_dataarray(var3)\n",
    "\n",
    "                    da_A4 = make_dataarray(A4)\n",
    "                    da_var4 = make_dataarray(var4)\n",
    "\n",
    "                    da_A5 = make_dataarray(A5)\n",
    "                    da_var5 = make_dataarray(var5)\n",
    "\n",
    "                    da_A1.to_netcdf(f'output/{path_gen}_da_A1')\n",
    "                    da_var1.to_netcdf(f'output/{path_gen}_da_var1')\n",
    "                    da_A2.to_netcdf(f'output/{path_gen}_da_A2')\n",
    "                    da_var2.to_netcdf(f'output/{path_gen}_da_var2')\n",
    "                    da_A3.to_netcdf(f'output/{path_gen}_da_A3')\n",
    "                    da_var3.to_netcdf(f'output/{path_gen}_da_var3')\n",
    "                    da_A4.to_netcdf(f'output/{path_gen}_da_A4')\n",
    "                    da_var4.to_netcdf(f'output/{path_gen}_da_var4')\n",
    "                    da_A5.to_netcdf(f'output/{path_gen}_da_A5')\n",
    "                    da_var5.to_netcdf(f'output/{path_gen}_da_var5')\n",
    "\n",
    "\n",
    "                    #################   PLOT DECOMPOSED KERNELS    ####################\n",
    "                    ##################  --------------------\t   ####################\n",
    "\n",
    "                    fig, ax = plt.subplots(1, 6, figsize=(24, 4))\n",
    "                    ax = ax.ravel()\n",
    "                    da_A1[0, :, :].plot(ax=ax[0], cmap='RdBu_r')\n",
    "                    da_A2[0, :, :].plot(ax=ax[1], cmap='RdBu_r')\n",
    "                    da_A3[0, :, :].plot(ax=ax[2], cmap='RdBu_r')\n",
    "                    da_A4[:, 0, 0].plot(ax=ax[3])\n",
    "                    da_A5[:, 0, 0].plot(ax=ax[4])\n",
    "\n",
    "#                         fig.savefig(dirName + f'{path_gen}_decompkernels', transparent=True)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #store log likelihood in dataframe\n",
    "            df_out = pd.DataFrame({'modelrun': modrunlist,\n",
    "                             'log_marginal_likelihood': loglikelist})\n",
    "\n",
    "\n",
    "            writepath = f'output/{path_gen}_loglikelihood'\n",
    "            df_out.to_csv(writepath, index=False)\n",
    "            df_likes = pd.read_csv(writepath)\n",
    "\n",
    "        # make heatmap for upper vs. lower mantle viscosities at one lithosphere thickness\n",
    "if plot_heatmap == \"true\":\n",
    "    if ice_model =='glac1d_':\n",
    "        df_likes['um'] = [key.split('_')[2][3:] for key in df_likes.modelrun]\n",
    "        df_likes['lm'] = [key.split('_')[3][2:] for key in df_likes.modelrun]\n",
    "        df_likes['lith'] = [key.split('_')[1][1:3] for key in df_likes.modelrun]\n",
    "        df_likes['icemodel'] = [key.split('_')[0] for key in df_likes.modelrun]\n",
    "    elif ice_model == 'd6g_h6g_':\n",
    "    #                 df_likes = df_likes.drop([36])\n",
    "        df_likes['um'] = [key.split('_')[3][3:] for key in df_likes.modelrun]\n",
    "        df_likes['lm'] = [key.split('_')[4][2:] for key in df_likes.modelrun]\n",
    "        df_likes['lith'] = [key.split('_')[2][1:3] for key in df_likes.modelrun]\n",
    "        df_likes['icemodel'] = [key.split('_l')[0] for key in df_likes.modelrun]\n",
    "\n",
    "    df_likes.lm = df_likes.lm.astype(float)\n",
    "    df_likes.um = df_likes.um.astype(float)\n",
    "    heatmap = df_likes.pivot_table(index='um', columns='lm', values='log_marginal_likelihood')\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    sns.heatmap(heatmap,  cmap='coolwarm', ax=ax,  cbar_kws={'label': 'negative log likelihood'})\n",
    "    ax.set_title(f'{place} {ages[0]} - {ages[-1]} yrs \\n {ice_model} : {df_likes.lith[0]} km lithosphere'); # (havsine)\n",
    "\n",
    "#                 fig.savefig(dirName + f'{path_gen}_likelihood_heatmap', transparent=True)   # _havsine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
