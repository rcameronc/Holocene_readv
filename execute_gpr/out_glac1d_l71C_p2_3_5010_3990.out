2020-02-20 17:59:44.298510: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/17.11.2/lib64/slurm:/cm/shared/apps/slurm/17.11.2/lib64:/cm/local/apps/gcc/6.1.0/lib:/cm/local/apps/gcc/6.1.0/lib64
2020-02-20 17:59:44.298905: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/17.11.2/lib64/slurm:/cm/shared/apps/slurm/17.11.2/lib64:/cm/local/apps/gcc/6.1.0/lib:/cm/local/apps/gcc/6.1.0/lib64
2020-02-20 17:59:44.298951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-02-20 18:03:00.801728: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/17.11.2/lib64/slurm:/cm/shared/apps/slurm/17.11.2/lib64:/cm/local/apps/gcc/6.1.0/lib:/cm/local/apps/gcc/6.1.0/lib64
2020-02-20 18:03:00.802303: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-02-20 18:03:00.802410: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node248): /proc/driver/nvidia/version does not exist
2020-02-20 18:03:00.804550: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-20 18:03:01.182775: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194770000 Hz
2020-02-20 18:03:01.203153: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55555d49a570 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-20 18:03:01.203195: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From readv_it.py:468: AffineScalar.__init__ (from tensorflow_probability.python.bijectors.affine_scalar) is deprecated and will be removed after 2020-01-01.
Instructions for updating:
`AffineScalar` bijector is deprecated; please use `tfb.Shift(loc)(tfb.Scale(...))` instead.
2020-02-20 18:03:02.460230: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
---------------
glac1d_l71C_ump2_lm3 run number 0
number of datapoints =  (227, 8)
model built, time= 1.6624908447265625
model minimized, time= 18.5529465675354
time elapsed =  21.354751348495483
negative log marginal likelihood = 14.834613817427424
Filename: readv_it.py

Line #    Mem usage    Increment   Line Contents
================================================
    41    311.7 MiB    311.7 MiB   @profile
    42                             
    43                             def readv():
    44                             
    45                                 # set the colormap and centre the colorbar
    46    311.7 MiB      0.0 MiB       class MidpointNormalize(Normalize):
    47    311.7 MiB      0.0 MiB           """Normalise the colorbar.  e.g. norm=MidpointNormalize(mymin, mymax, 0.)"""
    48    311.7 MiB      0.0 MiB           def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
    49                                         self.midpoint = midpoint
    50                                         Normalize.__init__(self, vmin, vmax, clip)
    51                             
    52    311.7 MiB      0.0 MiB           def __call__(self, value, clip=None):
    53                                         x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
    54                                         return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))
    55                             
    56                             
    57                                 ####################  Initialize parameters #######################
    58                                 #################### ---------------------- #######################
    59                             
    60    311.7 MiB      0.0 MiB       parser = argparse.ArgumentParser(description='import vars via c-line')
    61    311.7 MiB      0.0 MiB       parser.add_argument("--mod", default='d6g_h6g_')
    62    311.7 MiB      0.0 MiB       parser.add_argument("--lith", default='l71C')
    63    311.7 MiB      0.0 MiB       parser.add_argument("--um", default="p2")
    64    311.7 MiB      0.0 MiB       parser.add_argument("--lm", default="3")
    65    311.7 MiB      0.0 MiB       parser.add_argument("--tmax", default="4010")
    66    311.7 MiB      0.0 MiB       parser.add_argument("--tmin", default="2990")
    67                             
    68    311.7 MiB      0.0 MiB       args = parser.parse_args()
    69    311.7 MiB      0.0 MiB       ice_models = [args.mod]
    70    311.7 MiB      0.0 MiB       lith_thicknesses = [args.lith]
    71    311.7 MiB      0.0 MiB       um = args.um
    72    311.7 MiB      0.0 MiB       lm = args.lm
    73    311.7 MiB      0.0 MiB       tmax = int(args.tmax)
    74    311.7 MiB      0.0 MiB       tmin = int(args.tmin)
    75                             
    76                                 #ice_models = ['d6g_h6g_']# , 'glac1d_']
    77                                 #lith_thicknesses = ['l96C']# , 'l71C']
    78                             
    79    954.1 MiB      0.0 MiB       for i, ice_model in enumerate(ice_models):
    80    954.1 MiB      0.0 MiB           for k, lith_thickness in enumerate(lith_thicknesses):
    81    311.7 MiB      0.0 MiB               plotting = 'false'
    82    311.7 MiB      0.0 MiB               decomp = 'false'
    83    311.7 MiB      0.0 MiB               ice_model = ice_model # 'd6g_h6g_' # 'glac1d_' #   #
    84    311.7 MiB      0.0 MiB               lith_thickness = lith_thickness # 'l96'  # 'l90C'
    85    311.7 MiB      0.0 MiB               model = ice_model + lith_thickness
    86    311.7 MiB      0.0 MiB               place = 'fennoscandia'
    87                             
    88                                         locs = {
    89    311.7 MiB      0.0 MiB                   'england': [-12, 2, 50, 60],
    90    311.7 MiB      0.0 MiB                   'easternhem': [50, 178, -45, 80],
    91    311.7 MiB      0.0 MiB                   'westernhem': [-175, 30, -80, 75],
    92    311.7 MiB      0.0 MiB                   'world': [-179.8, 179.8, -89.8, 89.8],
    93    311.7 MiB      0.0 MiB                   'namerica': [-150, -20, 10, 75],
    94    311.7 MiB      0.0 MiB                   'eastcoast': [-88, -65, 15, 40],
    95    311.7 MiB      0.0 MiB                   'europe': [-20, 15, 35, 70],
    96    311.7 MiB      0.0 MiB                   'atlantic':[-85,10, 25, 60],
    97    311.7 MiB      0.0 MiB                   'fennoscandia': [-15, 50, 45, 73],
    98                                         }
    99    311.7 MiB      0.0 MiB               extent = locs[place]
   100    311.7 MiB      0.0 MiB               tmax, tmin, tstep = tmax, tmin, 100
   101                             
   102    311.7 MiB      0.0 MiB               ages_lgm = np.arange(100, 26000, tstep)[::-1]
   103                             
   104                                         #import khan dataset
   105    311.7 MiB      0.0 MiB               path = 'data/GSL_LGM_120519_.csv'
   106                             
   107    333.0 MiB     21.3 MiB               df = pd.read_csv(path, encoding="ISO-8859-15", engine='python')
   108    338.1 MiB      5.0 MiB               df = df.replace('\s+', '_', regex=True).replace('-', '_', regex=True).\
   109    339.5 MiB      0.2 MiB                       applymap(lambda s:s.lower() if type(s) == str else s)
   110    334.4 MiB      0.0 MiB               df.columns = df.columns.str.lower()
   111    334.4 MiB      0.0 MiB               df.rename_axis('index', inplace=True)
   112    335.3 MiB      0.9 MiB               df = df.rename({'latitude': 'lat', 'longitude': 'lon'}, axis='columns')
   113    335.4 MiB      0.1 MiB               dfind, dfterr, dfmar = df[(df.type == 0)
   114    335.4 MiB      0.0 MiB                                         & (df.age > 0)], df[df.type == 1], df[df.type == -1]
   115    335.5 MiB      0.0 MiB               np.sort(list(set(dfind.regionname1)))
   116                             
   117                                         #select location
   118    335.5 MiB      0.0 MiB               df_place = dfind[(dfind.age > tmin) & (dfind.age < tmax) &
   119                                                          (dfind.lon > extent[0])
   120                                                          & (dfind.lon < extent[1])
   121                                                          & (dfind.lat > extent[2])
   122    335.5 MiB      0.0 MiB                                & (dfind.lat < extent[3])][[
   123    335.5 MiB      0.0 MiB                                    'lat', 'lon', 'rsl', 'rsl_er_max', 'age'
   124                                                          ]]
   125                                         # & (df_place.rsl_er_max < 1)
   126    335.5 MiB      0.0 MiB               df_place.shape
   127                             
   128                                         ####################  	Plot locations  	#######################
   129                                         #################### ---------------------- #######################
   130                             
   131                                         #get counts by location rounded to nearest 0.1 degree
   132    335.5 MiB      0.0 MiB               df_rnd = df_place.copy()
   133    335.5 MiB      0.1 MiB               df_rnd.lat = np.round(df_rnd.lat, 1)
   134    335.5 MiB      0.0 MiB               df_rnd.lon = np.round(df_rnd.lon, 1)
   135    335.5 MiB      0.0 MiB               dfcounts_place = df_rnd.groupby(
   136    335.7 MiB      0.1 MiB                   ['lat', 'lon']).count().reset_index()[['lat', 'lon', 'rsl', 'age']]
   137                             
   138                                         #plot
   139    335.7 MiB      0.1 MiB               fig = plt.figure(figsize=(10, 7))
   140    335.9 MiB      0.1 MiB               ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())
   141                             
   142    336.0 MiB      0.2 MiB               ax.set_extent(extent)
   143    336.1 MiB      0.1 MiB               ax.coastlines(resolution='110m', linewidth=1, zorder=2)
   144    336.1 MiB      0.0 MiB               ax.add_feature(cfeature.OCEAN, zorder=0)
   145    336.1 MiB      0.0 MiB               ax.add_feature(cfeature.LAND, color='palegreen', zorder=1)
   146    336.1 MiB      0.0 MiB               ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=3)
   147    336.1 MiB      0.0 MiB               ax.gridlines(linewidth=1, color='white', alpha=0.5, zorder=4)
   148    336.1 MiB      0.0 MiB               scat = ax.scatter(dfcounts_place.lon,
   149    336.1 MiB      0.0 MiB                                 dfcounts_place.lat,
   150    336.1 MiB      0.0 MiB                                 s=dfcounts_place.rsl * 70,
   151    336.1 MiB      0.0 MiB                                 c='lightsalmon',
   152    336.1 MiB      0.0 MiB                                 vmin=-20,
   153    336.1 MiB      0.0 MiB                                 vmax=20,
   154    336.1 MiB      0.0 MiB                                 cmap='coolwarm',
   155    336.1 MiB      0.0 MiB                                 edgecolor='k',
   156    336.1 MiB      0.0 MiB                                 linewidths=1,
   157    336.1 MiB      0.0 MiB                                 transform=ccrs.PlateCarree(),
   158    336.1 MiB      0.0 MiB                                 zorder=5)
   159    336.1 MiB      0.0 MiB               size = Line2D(range(4),
   160    336.1 MiB      0.0 MiB                             range(4),
   161    336.1 MiB      0.0 MiB                             color="black",
   162    336.1 MiB      0.0 MiB                             marker='o',
   163    336.1 MiB      0.0 MiB                             linewidth=0,
   164    336.1 MiB      0.0 MiB                             linestyle='none',
   165    336.1 MiB      0.0 MiB                             markersize=16,
   166    336.1 MiB      0.0 MiB                             markerfacecolor="lightsalmon")
   167    336.1 MiB      0.0 MiB               labels = ['RSL datapoint location']
   168    336.1 MiB      0.0 MiB               leg = plt.legend([size],
   169    336.1 MiB      0.0 MiB                                labels,
   170    336.1 MiB      0.0 MiB                                loc='lower left',
   171    336.1 MiB      0.0 MiB                                bbox_to_anchor=(0.00, 0.00),
   172    336.1 MiB      0.0 MiB                                prop={'size': 20},
   173    336.2 MiB      0.1 MiB                                fancybox=True)
   174    336.2 MiB      0.0 MiB               leg.get_frame().set_edgecolor('k')
   175    336.2 MiB      0.0 MiB               ax.set_title('')
   176                             
   177                                         ####################  Make 3D fingerprint  #######################
   178                                         #################### ---------------------- #######################
   179                             
   180    336.2 MiB      0.0 MiB               filename = 'data/WAISreadvance_VM5_6ka_1step.mat'
   181                             
   182    339.8 MiB      3.6 MiB               waismask = io.loadmat(filename, squeeze_me=True)
   183    339.8 MiB      0.0 MiB               ds_mask = xr.Dataset({'rsl': (['lat', 'lon', 'age'], waismask['RSL'])},
   184                                                              coords={
   185    339.8 MiB      0.0 MiB                                        'lon': waismask['lon_out'],
   186    339.8 MiB      0.0 MiB                                        'lat': waismask['lat_out'],
   187    340.0 MiB      0.1 MiB                                        'age': np.round(waismask['ice_time_new'])
   188                                                              })
   189    340.0 MiB      0.0 MiB               fingerprint = ds_mask.sel(age=ds_mask.age[0])
   190                             
   191                             
   192    340.0 MiB      0.0 MiB               def make_fingerprint(start, end, maxscale):
   193                             
   194                                             #palindromic scaling vector
   195    340.0 MiB      0.0 MiB                   def palindrome(maxscale, ages):
   196                                                 """ Make palindrome scale 0-maxval with number of steps. """
   197    340.0 MiB      0.0 MiB                       half = np.linspace(0, maxscale, 1 + (len(ages) - 1) // 2)
   198    340.0 MiB      0.0 MiB                       scalefactor = np.concatenate([half, half[::-1]])
   199    340.0 MiB      0.0 MiB                       return scalefactor
   200                             
   201    340.0 MiB      0.0 MiB                   ages_readv = ages_lgm[(ages_lgm < start) & (ages_lgm >= end)]
   202    340.0 MiB      0.0 MiB                   scale = palindrome(maxscale, ages_readv)
   203                             
   204                                             #scale factor same size as ice model ages
   205    340.0 MiB      0.0 MiB                   pre = np.zeros(np.where(ages_lgm == start)[0])
   206    340.0 MiB      0.0 MiB                   post = np.zeros(len(ages_lgm) - len(pre) - len(scale))
   207                             
   208    340.0 MiB      0.0 MiB                   readv_scale = np.concatenate([pre, scale, post])
   209                             
   210                                             #scale factor into dataarray
   211    340.0 MiB      0.0 MiB                   da_scale = xr.DataArray(readv_scale, coords=[('age', ages_lgm)])
   212                             
   213                                             # broadcast fingerprint & scale to same dimensions;
   214    340.0 MiB      0.0 MiB                   fingerprint_out, fing_scaled = xr.broadcast(fingerprint.rsl, da_scale)
   215                             
   216                                             # mask fingerprint with scale to get LGM-pres timeseries
   217    340.0 MiB      0.0 MiB                   ds_fingerprint = (fingerprint_out *
   218    598.8 MiB    258.9 MiB                                     fing_scaled).transpose().to_dataset(name='rsl')
   219                             
   220                                             # scale dataset with fingerprint to LGM-present length & 0-max-0 over x years
   221    598.8 MiB      0.0 MiB                   xrlist = []
   222    855.3 MiB      0.0 MiB                   for i, key in enumerate(da_scale):
   223    855.3 MiB      1.0 MiB                       mask = ds_fingerprint.sel(age=ds_fingerprint.age[i].values) * key
   224    855.3 MiB      0.0 MiB                       mask = mask.assign_coords(scale=key,
   225    855.3 MiB      0.0 MiB                                                 age=ages_lgm[i]).expand_dims(dim=['age'])
   226    855.3 MiB      0.0 MiB                       xrlist.append(mask)
   227   1114.5 MiB    259.2 MiB                   ds_readv = xr.concat(xrlist, dim='age')
   228                             
   229   1114.5 MiB      0.0 MiB                   ds_readv.coords['lon'] = pd.DataFrame((ds_readv.lon[ds_readv.lon >= 180] - 360)- 0.12) \
   230   1114.5 MiB      0.0 MiB                                           .append(pd.DataFrame(ds_readv.lon[ds_readv.lon < 180]) + 0.58) \
   231   1114.5 MiB      0.0 MiB                                           .reset_index(drop=True).squeeze()
   232   1114.5 MiB      0.0 MiB                   ds_readv = ds_readv.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   233                             
   234                                             # Add readv to modeled RSL at locations with data
   235                                             ##### Need to fix this, as currently slice does not acknowledge new coords #########
   236   1114.5 MiB      0.0 MiB                   ds_readv = ds_readv.sel(age=slice(tmax, tmin),
   237   1114.5 MiB      0.0 MiB                                           lon=slice(df_place.lon.min() + 180 - 2,
   238   1114.5 MiB      0.0 MiB                                                     df_place.lon.max() + 180 + 2),
   239   1114.5 MiB      0.0 MiB                                           lat=slice(df_place.lat.max() + 2,
   240   1114.5 MiB      0.0 MiB                                                     df_place.lat.min() - 2))
   241   1114.5 MiB      0.0 MiB                   return ds_readv
   242                             
   243                             
   244                                         #Make deterministic readvance fingerprint
   245    340.0 MiB      0.0 MiB               start, end = 6100, 3000
   246    340.0 MiB      0.0 MiB               maxscale = 2.25
   247    597.7 MiB      0.0 MiB               ds_readv = make_fingerprint(start, end, maxscale)
   248                             
   249                             
   250                                         ####################  Build  GIA models 	#######################
   251                                         #################### ---------------------- #######################
   252                             
   253                                         #Use either glac1d or ICE6G
   254                             
   255                             
   256    885.7 MiB      0.0 MiB               def build_dataset(path, model):
   257                                             """download model runs from local directory."""
   258    885.7 MiB      0.0 MiB                   path = path
   259    885.7 MiB      0.0 MiB                   files = f'{path}*.nc'
   260    885.7 MiB      0.0 MiB                   basefiles = glob.glob(files)
   261                                             modelrun = [
   262    885.7 MiB      0.0 MiB                       key.split('output_', 1)[1][:-3].replace('.', '_')
   263    885.7 MiB      0.0 MiB                       for key in basefiles
   264                                             ]
   265    885.7 MiB      0.0 MiB                   dss = xr.open_mfdataset(files,
   266    885.7 MiB      0.0 MiB                                           chunks=None,
   267    885.7 MiB      0.0 MiB                                           concat_dim='modelrun',
   268    885.7 MiB     38.1 MiB                                           combine='nested')
   269    885.7 MiB      2.2 MiB                   lats, lons, times = dss.LAT.values[0], dss.LON.values[
   270    885.7 MiB      0.8 MiB                       0], dss.TIME.values[0]
   271    885.7 MiB      0.0 MiB                   ds = dss.drop(['LAT', 'LON', 'TIME'])
   272    885.7 MiB      0.0 MiB                   ds = ds.assign_coords(lat=lats,
   273    885.7 MiB      0.0 MiB                                         lon=lons,
   274    885.7 MiB      0.0 MiB                                         time=times,
   275    885.7 MiB      0.1 MiB                                         modelrun=modelrun).rename({
   276    885.7 MiB      0.0 MiB                                             'time': 'age',
   277    885.7 MiB      0.0 MiB                                             'RSL': 'rsl'
   278                                                                   })
   279    885.7 MiB      0.0 MiB                   return ds
   280                             
   281    885.7 MiB      0.0 MiB               def one_mod(path, names):
   282                                             """Organize model runs into xarray dataset."""
   283    885.7 MiB      0.0 MiB                   ds1 = build_dataset(path, names[0])
   284    885.7 MiB      0.0 MiB                   names = names[1:]
   285    910.3 MiB     24.6 MiB                   ds = ds1.chunk({'lat': 10, 'lon': 10})
   286    910.3 MiB      0.0 MiB                   for i in range(len(names)):
   287                                                 temp = build_dataset(names[i])
   288                                                 temp1 = temp.interp_like(ds1)
   289                                                 temp1['modelrun'] = temp['modelrun']
   290                                                 ds = xr.concat([ds, temp1], dim='modelrun')
   291    910.3 MiB      0.0 MiB                   ds['age'] = ds['age'] * 1000
   292   1022.7 MiB    112.5 MiB                   ds = ds.roll(lon=256, roll_coords=True)
   293   1022.9 MiB      0.2 MiB                   ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \
   294   1022.9 MiB      0.0 MiB                                           .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \
   295   1022.9 MiB      0.0 MiB                                           .reset_index(drop=True).squeeze()
   296   1022.9 MiB      0.0 MiB                   ds.coords['lat'] = ds.lat[::-1]
   297   1022.9 MiB      0.0 MiB                   ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   298   1022.9 MiB      0.0 MiB                   return ds
   299                             
   300                                         #make composite of a bunch of GIA runs, i.e. GIA prior
   301                             
   302    597.7 MiB      0.0 MiB               if ice_model == 'glac1d_':
   303    597.7 MiB      0.0 MiB                   path = f'data/glac1d_/output_{model}'
   304                             
   305                                             #make composite of a bunch of GIA runs, i.e. GIA prior
   306    767.3 MiB      0.0 MiB                   ds = one_mod(path, [model])
   307    767.3 MiB      0.0 MiB                   ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   308    767.3 MiB      0.0 MiB                                          lon=slice(df_place.lon.min() - 2,
   309    767.3 MiB      0.0 MiB                                                    df_place.lon.max() + 2),
   310    767.3 MiB      0.0 MiB                                          lat=slice(df_place.lat.min() - 2,
   311    767.6 MiB      0.3 MiB                                                    df_place.lat.max() + 2))
   312                             
   313                                         elif ice_model == 'd6g_h6g_':
   314                                             path = f'data/d6g_h6g_/output_{model}'
   315                             
   316                                             #make GIA prior std.
   317                                             ds = one_mod(path, [model])
   318                                             ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   319                                                                    lon=slice(df_place.lon.min() - 2,
   320                                                                              df_place.lon.max() + 2),
   321                                                                    lat=slice(df_place.lat.min() - 2,
   322                                                                              df_place.lat.max() + 2))
   323                             
   324    884.1 MiB    116.5 MiB               ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(
   325    885.6 MiB      1.4 MiB                   age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   326                             
   327                                         # loop through all models to calculate GPR log likelihood
   328                                         # runs = ds.modelrun.values.tolist()
   329    885.6 MiB      0.0 MiB               runs = [f'{ice_model}{lith_thickness}_um{um}_lm{lm}']
   330                             
   331    885.6 MiB      0.0 MiB               modrunlist = []
   332    885.6 MiB      0.0 MiB               loglikelist = []
   333    952.8 MiB      0.0 MiB               for i, modelrun in enumerate(runs):
   334                             
   335    885.7 MiB      0.1 MiB                   print('---------------')
   336    885.7 MiB      0.0 MiB                   print(f'{modelrun} run number {i}')
   337                             
   338    885.7 MiB      0.0 MiB                   if ice_model == 'glac1d_':
   339                                                 # make prior RSL
   340    885.7 MiB      0.0 MiB                       ds_area = one_mod(path,
   341   1023.0 MiB      0.1 MiB                           [ice_model + lith_thickness]).sel(modelrun=modelrun).rsl.sel(
   342   1023.0 MiB      0.0 MiB                               age=slice(tmax, tmin),
   343   1023.0 MiB      0.0 MiB                               lon=slice(df_place.lon.min() - 2,
   344   1023.0 MiB      0.0 MiB                                         df_place.lon.max() + 2),
   345   1023.0 MiB      0.0 MiB                               lat=slice(df_place.lat.min() - 2,
   346    905.4 MiB      0.0 MiB                                         df_place.lat.max() + 2)).load().to_dataset().interp(
   347    905.4 MiB      0.0 MiB                                             age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   348                             
   349                                             else:
   350                                                 # make prior RSL
   351                                                 ds_area = one_mod(path,
   352                                                     [ice_model + lith_thickness]).sel(modelrun=modelrun).rsl.sel(
   353                                                         age=slice(tmax, tmin),
   354                                                         lon=slice(df_place.lon.min() - 2,
   355                                                                   df_place.lon.max() + 2),
   356                                                         lat=slice(df_place.lat.min() - 2,
   357                                                                   df_place.lat.max() + 2)).load().to_dataset().interp(
   358                                                                       age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   359                             
   360                             
   361                                             #sample each model at points where we have RSL data
   362    905.4 MiB      0.0 MiB                   def ds_select(ds):
   363    905.4 MiB      0.0 MiB                       return ds.rsl.sel(age=[row.age],
   364    905.4 MiB      0.0 MiB                                         lon=[row.lon],
   365    905.4 MiB      0.0 MiB                                         lat=[row.lat],
   366    905.4 MiB      0.0 MiB                                         method='nearest').squeeze().values
   367                             
   368                                             #select points at which RSL data exists
   369    905.4 MiB      0.0 MiB                   for i, row in df_place.iterrows():
   370    905.4 MiB      0.0 MiB                       df_place.loc[i, 'rsl_realresid'] = df_place.rsl[i] - ds_select(ds_area)
   371    905.4 MiB      0.0 MiB                       df_place.loc[i, 'rsl_giaprior'] = ds_select(ds_area)
   372    905.4 MiB      0.0 MiB                       df_place.loc[i, 'rsl_giaprior_std'] = ds_select(ds_areastd)
   373                             
   374    905.4 MiB      0.0 MiB                   print('number of datapoints = ', df_place.shape)
   375                             
   376                                             ##################	  RUN GP REGRESSION 	#######################
   377                                             ##################  --------------------	 ######################
   378    905.4 MiB      0.0 MiB                   start = time.time()
   379                             
   380    905.4 MiB      0.0 MiB                   def run_gpr():
   381                             
   382    905.4 MiB      0.0 MiB                       Data = Tuple[tf.Tensor, tf.Tensor]
   383    905.4 MiB      0.0 MiB                       likelihood = df_place.rsl_er_max.ravel()**2 # + df_place.rsl_giaprior_std.ravel()**2  # here we define likelihood
   384                             
   385    905.4 MiB      0.0 MiB                       class GPR_diag(gpf.models.GPModel):
   386                                                     r"""
   387                                                     Gaussian Process Regression.
   388                                                     This is a vanilla implementation of GP regression with a pointwise Gaussian
   389                                                     likelihood.  Multiple columns of Y are treated independently.
   390                                                     The log likelihood of this models is sometimes referred to as the 'marginal log likelihood',
   391                                                     and is given by
   392                                                     .. math::
   393                                                        \log p(\mathbf y \,|\, \mathbf f) =
   394                                                             \mathcal N\left(\mathbf y\,|\, 0, \mathbf K + \sigma_n \mathbf I\right)
   395    905.4 MiB      0.0 MiB                           """
   396    907.6 MiB      0.0 MiB                           def __init__(self,
   397                                                                  data: Data,
   398                                                                  kernel: Kernel,
   399    905.4 MiB      0.0 MiB                                        mean_function: Optional[MeanFunction] = None,
   400    905.4 MiB      0.0 MiB                                        likelihood=likelihood):
   401    907.6 MiB      0.0 MiB                               likelihood = gpf.likelihoods.Gaussian(variance=likelihood)
   402    907.6 MiB      0.0 MiB                               _, y_data = data
   403    907.6 MiB      0.0 MiB                               super().__init__(kernel,
   404    907.6 MiB      0.0 MiB                                                likelihood,
   405    907.6 MiB      0.0 MiB                                                mean_function,
   406    907.6 MiB      0.0 MiB                                                num_latent=y_data.shape[-1])
   407    907.6 MiB      0.0 MiB                               self.data = data
   408                             
   409    952.0 MiB      0.0 MiB                           def log_likelihood(self):
   410                                                         """
   411                                                         Computes the log likelihood.
   412                                                         """
   413    952.0 MiB      0.0 MiB                               x, y = self.data
   414    952.0 MiB      0.1 MiB                               K = self.kernel(x)
   415    952.0 MiB      0.0 MiB                               num_data = x.shape[0]
   416    952.0 MiB      0.0 MiB                               k_diag = tf.linalg.diag_part(K)
   417    952.0 MiB      0.0 MiB                               s_diag = tf.convert_to_tensor(self.likelihood.variance)
   418    952.0 MiB      0.0 MiB                               jitter = tf.cast(tf.fill([num_data], default_jitter()),
   419    952.0 MiB      0.0 MiB                                                'float64')  # stabilize K matrix w/jitter
   420    952.0 MiB      0.0 MiB                               ks = tf.linalg.set_diag(K, k_diag + s_diag + jitter)
   421    952.0 MiB      0.0 MiB                               L = tf.linalg.cholesky(ks)
   422    952.0 MiB      0.0 MiB                               m = self.mean_function(x)
   423                             
   424                                                         # [R,] log-likelihoods for each independent dimension of Y
   425    952.0 MiB      0.0 MiB                               log_prob = multivariate_normal(y, m, L)
   426    952.0 MiB      0.0 MiB                               return tf.reduce_sum(log_prob)
   427                             
   428    935.3 MiB      0.0 MiB                           def predict_f(self,
   429                                                                   predict_at: tf.Tensor,
   430                                                                   full_cov: bool = False,
   431    905.4 MiB      0.0 MiB                                         full_output_cov: bool = False):
   432                                                         r"""
   433                                                         This method computes predictions at X \in R^{N \x D} input points
   434                                                         .. math::
   435                                                             p(F* | Y)
   436                                                         where F* are points on the GP at new data points, Y are noisy observations at training data points.
   437                                                         """
   438    935.3 MiB      0.0 MiB                               x_data, y_data = self.data
   439    935.3 MiB      0.0 MiB                               err = y_data - self.mean_function(x_data)
   440                             
   441    935.3 MiB      0.0 MiB                               kmm = self.kernel(x_data)
   442    935.3 MiB      0.0 MiB                               knn = self.kernel(predict_at, full=full_cov)
   443    982.9 MiB     47.6 MiB                               kmn = self.kernel(x_data, predict_at)
   444                             
   445    982.9 MiB      0.0 MiB                               num_data = x_data.shape[0]
   446    982.9 MiB      0.0 MiB                               s = tf.linalg.diag(tf.convert_to_tensor(
   447    982.9 MiB      0.0 MiB                                   self.likelihood.variance))  #changed from normal GPR
   448                             
   449    982.9 MiB      0.0 MiB                               conditional = gpf.conditionals.base_conditional
   450    982.9 MiB      0.0 MiB                               f_mean_zero, f_var = conditional(
   451    982.9 MiB      0.0 MiB                                   kmn, kmm + s, knn, err, full_cov=full_cov,
   452    999.6 MiB     16.7 MiB                                   white=False)  # [N, P], [N, P] or [P, N, N]
   453    999.6 MiB      0.0 MiB                               f_mean = f_mean_zero + self.mean_function(predict_at)
   454    999.6 MiB      0.0 MiB                               return f_mean, f_var
   455                             
   456                             
   457    905.4 MiB      0.0 MiB                       def normalize(df):
   458    905.4 MiB      0.0 MiB                           return np.array((df - df.mean()) / df.std()).reshape(len(df), 1)
   459                             
   460                             
   461    952.0 MiB      0.0 MiB                       def denormalize(y_pred, df):
   462    952.0 MiB      0.0 MiB                           return np.array((y_pred * df.std()) + df.mean())
   463                             
   464                             
   465    907.6 MiB      0.0 MiB                       def bounded_parameter(low, high, param):
   466                                                     """Make parameter tfp Parameter with optimization bounds."""
   467    907.6 MiB      0.0 MiB                           affine = tfb.AffineScalar(shift=tf.cast(low, tf.float64),
   468    907.6 MiB      0.0 MiB                                                     scale=tf.cast(high - low, tf.float64))
   469    907.6 MiB      0.0 MiB                           sigmoid = tfb.Sigmoid()
   470    907.6 MiB      0.0 MiB                           logistic = tfb.Chain([affine, sigmoid])
   471    907.6 MiB      0.1 MiB                           parameter = gpf.Parameter(param, transform=logistic, dtype=tf.float64)
   472    907.6 MiB      0.0 MiB                           return parameter
   473                             
   474                             
   475    905.4 MiB      0.0 MiB                       class HaversineKernel_Matern52(gpf.kernels.Matern52):
   476                                                     """
   477                                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   478                                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   479    905.4 MiB      0.0 MiB                           """
   480                                                     def __init__(
   481                                                         self,
   482                                                         lengthscale=1.0,
   483                                                         variance=1.0,
   484    905.4 MiB      0.0 MiB                               active_dims=None,
   485                                                     ):
   486                                                         super().__init__(
   487                                                             active_dims=active_dims,
   488                                                             variance=variance,
   489                                                             lengthscale=lengthscale,
   490                                                         )
   491                             
   492    905.4 MiB      0.0 MiB                           def haversine_dist(self, X, X2):
   493                                                         pi = np.pi / 180
   494                                                         f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   495                                                         f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   496                                                         d = tf.sin((f - f2) / 2)**2
   497                                                         lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   498                                                                     tf.expand_dims(X2[:, 0] * pi, -2)
   499                                                         cos_prod = tf.cos(lat2) * tf.cos(lat1)
   500                                                         a = d[:, :, 0] + cos_prod * d[:, :, 1]
   501                                                         c = tf.asin(tf.sqrt(a)) * 6371 * 2
   502                                                         return c
   503                             
   504    905.4 MiB      0.0 MiB                           def scaled_squared_euclid_dist(self, X, X2):
   505                                                         """
   506                                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   507                                                         """
   508                                                         if X2 is None:
   509                                                             X2 = X
   510                                                         dist = da.square(self.haversine_dist(X, X2) / self.lengthscale)
   511                                                 #             dist = tf.convert_to_tensor(dist)
   512                                                         return dist
   513                             
   514                             
   515    905.4 MiB      0.0 MiB                       class HaversineKernel_Matern32(gpf.kernels.Matern32):
   516                                                     """
   517                                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   518                                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   519    905.4 MiB      0.0 MiB                           """
   520                                                     def __init__(
   521                                                         self,
   522                                                         lengthscale=1.0,
   523                                                         variance=1.0,
   524    905.4 MiB      0.0 MiB                               active_dims=None,
   525                                                     ):
   526                                                         super().__init__(
   527                                                             active_dims=active_dims,
   528                                                             variance=variance,
   529                                                             lengthscale=lengthscale,
   530                                                         )
   531                             
   532    905.4 MiB      0.0 MiB                           def haversine_dist(self, X, X2):
   533                                                         pi = np.pi / 180
   534                                                         f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   535                                                         f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   536                                                         d = tf.sin((f - f2) / 2)**2
   537                                                         lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   538                                                                     tf.expand_dims(X2[:, 0] * pi, -2)
   539                                                         cos_prod = tf.cos(lat2) * tf.cos(lat1)
   540                                                         a = d[:, :, 0] + cos_prod * d[:, :, 1]
   541                                                         c = tf.asin(tf.sqrt(a)) * 6371 * 2
   542                                                         return c
   543                             
   544    905.4 MiB      0.0 MiB                           def scaled_squared_euclid_dist(self, X, X2):
   545                                                         """
   546                                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   547                                                         """
   548                                                         if X2 is None:
   549                                                             X2 = X
   550                                                         dist = tf.square(self.haversine_dist(X, X2) / self.lengthscale)
   551                                                 #             dist = tf.convert_to_tensor(dist) # return to tensorflow
   552                                                         return dist
   553                             
   554                             
   555                                                 ########### Section to Run GPR######################
   556                                                 ##################################3#################
   557                             
   558                                                 # Input space, rsl normalized to zero mean, unit variance
   559    905.4 MiB      0.0 MiB                       X = np.stack((df_place['lon'], df_place['lat'], df_place['age']), 1)
   560    905.4 MiB      0.0 MiB                       RSL = normalize(df_place.rsl_realresid)
   561                             
   562                                                 #define kernels  with bounds
   563                             
   564                                 #                 k1 = HaversineKernel_Matern32(active_dims=[0, 1])
   565                                 #                 k1.lengthscale = bounded_parameter(5000, 30000, 10000)  #hemispheric space
   566                                 #                 k1.variance = bounded_parameter(0.1, 100, 2)
   567                             
   568    907.5 MiB      2.1 MiB                       k1 = gpf.kernels.Matern32(active_dims=[0, 1])
   569    907.6 MiB      0.0 MiB                       k1.lengthscale = bounded_parameter(50, 500, 60)  #hemispheric space
   570    907.6 MiB      0.0 MiB                       k1.variance = bounded_parameter(0.05, 100, 2)
   571                             
   572                                 #                 k2 = HaversineKernel_Matern32(active_dims=[0, 1])
   573                                 #                 k2.lengthscale = bounded_parameter(10, 5000, 100)  #GIA space
   574                                 #                 k2.variance = bounded_parameter(0.1, 100, 2)
   575                             
   576    907.6 MiB      0.0 MiB                       k2 = gpf.kernels.Matern32(active_dims=[0,1])
   577    907.6 MiB      0.0 MiB                       k2.lengthscale = bounded_parameter(1, 50, 5)  #GIA space
   578    907.6 MiB      0.0 MiB                       k2.variance = bounded_parameter(0.05, 100, 2)
   579                             
   580    907.6 MiB      0.0 MiB                       k3 = gpf.kernels.Matern32(active_dims=[2])  #GIA time
   581    907.6 MiB      0.0 MiB                       k3.lengthscale = bounded_parameter(8000, 20000, 10000)
   582    907.6 MiB      0.0 MiB                       k3.variance = bounded_parameter(0.1, 100, 1)
   583                             
   584    907.6 MiB      0.0 MiB                       k4 = gpf.kernels.Matern32(active_dims=[2])  #shorter time
   585    907.6 MiB      0.0 MiB                       k4.lengthscale = bounded_parameter(1, 8000, 1000)
   586    907.6 MiB      0.0 MiB                       k4.variance = bounded_parameter(0.1, 100, 1)
   587                             
   588    907.6 MiB      0.0 MiB                       k5 = gpf.kernels.White(active_dims=[2])
   589    907.6 MiB      0.0 MiB                       k5.variance = bounded_parameter(0.1, 100, 1)
   590                             
   591    907.6 MiB      0.0 MiB                       kernel = (k1 * k3) + (k2 * k4) + k5
   592                             
   593                                                 #build & train model
   594    907.6 MiB      0.0 MiB                       m = GPR_diag((X, RSL), kernel=kernel, likelihood=likelihood)
   595    907.6 MiB      0.0 MiB                       print('model built, time=', time.time() - start)
   596                             
   597                             
   598    908.5 MiB      0.9 MiB                       @tf.function(autograph=False)
   599                                                 def objective():
   600    908.7 MiB      0.0 MiB                           return - m.log_marginal_likelihood()
   601                             
   602    907.6 MiB      0.0 MiB                       o = gpf.optimizers.Scipy()
   603    935.3 MiB     26.6 MiB                       o.minimize(objective, variables=m.trainable_variables)
   604    935.3 MiB      0.0 MiB                       print('model minimized, time=', time.time() - start)
   605                             
   606                                                 # output space
   607    935.3 MiB      0.0 MiB                       nout = 50
   608    935.3 MiB      0.0 MiB                       lat = np.linspace(min(ds_area.lat), max(ds_area.lat), nout)
   609    935.3 MiB      0.0 MiB                       lon = np.linspace(min(ds_area.lon), max(ds_area.lon), nout)
   610    935.3 MiB      0.0 MiB                       ages = ages_lgm[(ages_lgm < tmax) & (ages_lgm > tmin)]
   611    935.3 MiB      0.0 MiB                       xyt = np.array(list(product(lon, lat, ages)))
   612                             
   613                                                 #query model & renormalize data
   614    952.0 MiB      0.0 MiB                       y_pred, var = m.predict_f(xyt)
   615    952.0 MiB      0.0 MiB                       y_pred_out = denormalize(y_pred, df_place.rsl_realresid)
   616                             
   617                                                 #reshape output vectors
   618                             #                    Xlon = np.array(xyt[:, 0]).reshape((nout, nout, len(ages)))
   619                              #                   Xlat = np.array(xyt[:, 1]).reshape((nout, nout, len(ages)))
   620    952.0 MiB      0.0 MiB                       Zp = np.array(y_pred_out).reshape(nout, nout, len(ages))
   621    952.0 MiB      0.0 MiB                       varp = np.array(var).reshape(nout, nout, len(ages))
   622                             
   623                                                 #print kernel details
   624                                             #     print_summary(m, fmt='notebook')
   625    952.0 MiB      0.0 MiB                       print('time elapsed = ', time.time() - start)
   626                             
   627    952.0 MiB      0.0 MiB                       print('negative log marginal likelihood =',
   628    952.0 MiB      0.0 MiB                             m.neg_log_marginal_likelihood().numpy())
   629                             
   630                             
   631    952.0 MiB      0.0 MiB                       modrunlist.append(modelrun)
   632    952.0 MiB      0.0 MiB                       loglikelist.append(m.neg_log_marginal_likelihood().numpy())
   633                             
   634                             
   635                             
   636                                                 ##################	  INTERPOLATE MODELS 	#######################
   637                                                 ##################  --------------------	 ######################
   638                             
   639                                                 # turn GPR output into xarray dataarray
   640    952.0 MiB      0.0 MiB                       da_zp = xr.DataArray(Zp, coords=[lon, lat, ages],
   641    952.0 MiB      0.0 MiB                                            dims=['lon', 'lat',
   642    952.0 MiB      0.0 MiB                                                  'age']).transpose('age', 'lat', 'lon')
   643    952.0 MiB      0.0 MiB                       da_varp = xr.DataArray(varp,
   644    952.0 MiB      0.0 MiB                                              coords=[lon, lat, ages],
   645    952.0 MiB      0.0 MiB                                              dims=['lon', 'lat',
   646    952.0 MiB      0.0 MiB                                                    'age']).transpose('age', 'lat', 'lon')
   647                             
   648                             
   649    952.0 MiB      0.0 MiB                       def interp_likegpr(ds):
   650    952.0 MiB      0.0 MiB                           return ds.rsl.load().transpose().interp_like(da_zp)
   651                             
   652                             
   653                                                 #interpolate all models onto GPR grid
   654    952.0 MiB      0.0 MiB                       da_giapriorinterp = interp_likegpr(ds_area)
   655    952.0 MiB      0.0 MiB                       ds_giapriorinterp = ds_area.interp(age=ages)
   656    952.0 MiB      0.0 MiB                       da_giapriorinterpstd = interp_likegpr(ds_areastd)
   657                             
   658                                                 # add total prior RSL back into GPR
   659    952.0 MiB      0.0 MiB                       da_priorplusgpr = da_zp + da_giapriorinterp
   660                             
   661    952.0 MiB      0.0 MiB                       return ages, da_zp, da_giapriorinterp, da_priorplusgpr, da_varp, modrunlist, loglikelist
   662                             
   663    952.0 MiB      0.0 MiB                   ages, da_zp, da_giapriorinterp, da_priorplusgpr, da_varp, modrunlist, loglikelist = run_gpr()
   664                                             ##################	  	 SAVE NETCDFS 	 	#######################
   665                                             ##################  --------------------	 ######################
   666                             
   667    952.0 MiB      0.0 MiB                   path_gen = f'{ages[0]}_{ages[-1]}_{model}_{place}'
   668    952.4 MiB      0.4 MiB                   da_zp.to_netcdf('output/' + path_gen + '_da_zp')
   669    952.5 MiB      0.1 MiB                   da_giapriorinterp.to_netcdf('output/' + path_gen + '_giaprior')
   670    952.5 MiB      0.0 MiB                   da_priorplusgpr.to_netcdf('output/' + path_gen + '_posterior')
   671    952.5 MiB      0.0 MiB                   da_varp.to_netcdf('output/' + path_gen + '_gp_variance')
   672                             
   673                                             ##################		  PLOT  MODELS 		#######################
   674                                             ##################  --------------------	 ######################
   675    952.5 MiB      0.0 MiB                   dirName = f'figs/{place}/'
   676    952.5 MiB      0.0 MiB                   if not os.path.exists(dirName):
   677                                                 os.mkdir(dirName)
   678                                                 print("Directory ", dirName, " Created ")
   679                                             else:
   680                                                 pass
   681                                     #             print("Directory ", dirName, " already exists")
   682                             
   683    952.5 MiB      0.0 MiB                   if plotting == 'true':
   684                                                 for i, age in enumerate(ages):
   685                                                     if (age / 500).is_integer():
   686                                                         step = (ages[0] - ages[1])
   687                                                         df_it = df_place[(df_place.age < age) & (df_place.age > age - step)]
   688                                                         resid_it = da_zp.sel(age=slice(age, age - step))
   689                                                         rsl, var = df_it.rsl, df_it.rsl_er_max.values**2
   690                                                         lat_it, lon_it = df_it.lat, df_it.lon
   691                                                         vmin = ds_giapriorinterp.rsl.min().values  # + 10
   692                                                         vmax = ds_giapriorinterp.rsl.max().values  # - 40
   693                                                         vmin_std = 0
   694                                                         vmax_std = 1
   695                                                         tmin_it = np.round(age - step, 2)
   696                                                         tmax_it = np.round(age, 2)
   697                                                         cbarscale = 0.3
   698                                                         fontsize = 20
   699                                                         cmap = 'coolwarm'
   700                                                         cbar_kwargs = {'shrink': cbarscale, 'label': 'RSL (m)'}
   701                             
   702                                                         proj = ccrs.PlateCarree()
   703                                                         projection = ccrs.PlateCarree()
   704                                                         fig, (ax1, ax2, ax3,
   705                                                               ax4) = plt.subplots(1,
   706                                                                                   4,
   707                                                                                   figsize=(24, 16),
   708                                                                                   subplot_kw=dict(projection=projection))
   709                             
   710                                                         # total prior mean + "true" data
   711                                                         ax1.coastlines(color='k')
   712                                                         pc1 = ds_giapriorinterp.rsl[i].transpose().plot(ax=ax1,
   713                                                                                                         transform=proj,
   714                                                                                                         cmap=cmap,
   715                                                                                                         norm=MidpointNormalize(
   716                                                                                                             vmin, vmax, 0),
   717                                                                                                         add_colorbar=False,
   718                                                                                                         extend='both')
   719                                                         cbar = fig.colorbar(pc1,
   720                                                                             ax=ax1,
   721                                                                             shrink=.3,
   722                                                                             label='RSL (m)',
   723                                                                             extend='both')
   724                                                         scat = ax1.scatter(lon_it,
   725                                                                            lat_it,
   726                                                                            s=80,
   727                                                                            c=rsl,
   728                                                                            edgecolor='k',
   729                                                                            vmin=vmin,
   730                                                                            vmax=vmax,
   731                                                                            norm=MidpointNormalize(vmin, vmax, 0),
   732                                                                            cmap=cmap)
   733                                                         ax1.set_title(f'{np.round(ds_giapriorinterp.rsl[i].age.values, -1)} yrs',
   734                                                                       fontsize=fontsize)
   735                                                         #         ax1.set_extent(extent_)
   736                             
   737                                                         # Learned difference between prior and "true" data
   738                                                         ax2.coastlines(color='k')
   739                                                         pc = da_zp[i, :, :].plot(ax=ax2,
   740                                                                                  transform=proj,
   741                                                                                  cmap=cmap,
   742                                                                                  extend='both',
   743                                                                                  norm=MidpointNormalize(
   744                                                                                      resid_it.min(), resid_it.max(), 0),
   745                                                                                  add_colorbar=False)
   746                                                         cbar = fig.colorbar(pc,
   747                                                                             ax=ax2,
   748                                                                             shrink=.3,
   749                                                                             label='RSL (m)',
   750                                                                             extend='both')
   751                                                         scat = ax2.scatter(lon_it,
   752                                                                            lat_it,
   753                                                                            s=80,
   754                                                                            facecolors='k',
   755                                                                            cmap=cmap,
   756                                                                            edgecolor='k',
   757                                                                            transform=proj,
   758                                                                            norm=MidpointNormalize(resid_it.min(),
   759                                                                                                   resid_it.max(), 0))
   760                                                         ax2.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   761                                                         #         ax2.set_extent(extent_)
   762                             
   763                                                         # GP regression
   764                                                         ax3.coastlines(color='k')
   765                                                         pc = da_priorplusgpr[i].plot(ax=ax3,
   766                                                                                      transform=proj,
   767                                                                                      norm=MidpointNormalize(vmin, vmax, 0),
   768                                                                                      cmap=cmap,
   769                                                                                      extend='both',
   770                                                                                      add_colorbar=False)
   771                                                         scat = ax3.scatter(lon_it,
   772                                                                            lat_it,
   773                                                                            s=80,
   774                                                                            c=rsl,
   775                                                                            edgecolor='k',
   776                                                                            cmap=cmap,
   777                                                                            norm=MidpointNormalize(vmin, vmax, 0))
   778                                                         cbar = fig.colorbar(pc,
   779                                                                             ax=ax3,
   780                                                                             shrink=.3,
   781                                                                             label='RSL (m)',
   782                                                                             extend='both')
   783                                                         ax3.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   784                                                         #         ax3.set_extent(extent_)
   785                             
   786                                                         #GP regression standard deviation
   787                                                         ax4.coastlines(color='k')
   788                                                         pc = (2 * np.sqrt(da_varp[i])).plot(
   789                                                             ax=ax4,
   790                                                             transform=proj,
   791                                                             vmin=vmin_std,
   792                                                             vmax=vmax_std * 2,
   793                                                             cmap='Reds',
   794                                                             extend='both',
   795                                                             add_colorbar=False,
   796                                                         )
   797                                                         scat = ax4.scatter(lon_it,
   798                                                                            lat_it,
   799                                                                            s=80,
   800                                                                            c=2 * np.sqrt(var),
   801                                                                            vmin=vmin_std,
   802                                                                            vmax=vmax_std * 2,
   803                                                                            cmap='Reds',
   804                                                                            edgecolor='k',
   805                                                                            transform=proj)
   806                                                         cbar = fig.colorbar(pc,
   807                                                                             ax=ax4,
   808                                                                             shrink=.3,
   809                                                                             extend='both',
   810                                                                             label='RSL (m) (2 $\sigma$)')
   811                                                         ax4.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   812                                                 #         ax4.set_extent(extent_)
   813                             
   814                                                 ########## ----- Save figures -------- #######################
   815                                                     fig.savefig(dirName + f'{path_gen}_{age}_3D_fig', transparent=True)
   816                             
   817                                                 ##################	CHOOSE LOCS W/NUF SAMPS #######################
   818                                                 ##################  --------------------	 ######################
   819                             
   820                             
   821                                                 def locs_with_enoughsamples(df_place, place, number):
   822                                                     """make new dataframe, labeled, of sites with [> number] measurements"""
   823                                                     df_lots = df_place.groupby(['lat',
   824                                                                                 'lon']).filter(lambda x: len(x) > number)
   825                             
   826                                                     df_locs = []
   827                                                     for i, group in enumerate(df_lots.groupby(['lat', 'lon'])):
   828                                                         singleloc = group[1].copy()
   829                                                         singleloc['location'] = place
   830                                                         singleloc['locnum'] = place + '_site' + str(
   831                                                             i)  # + singleloc.reset_index().index.astype('str')
   832                                                         df_locs.append(singleloc)
   833                                                     df_locs = pd.concat(df_locs)
   834                             
   835                                                     return df_locs
   836                             
   837                             
   838                                                 number = 6
   839                                                 df_nufsamps = locs_with_enoughsamples(df_place, place, number)
   840                                                 len(df_nufsamps.locnum.unique())
   841                             
   842                                                 ##################	PLOT LOCS W/NUF SAMPS   #######################
   843                                                 ##################  --------------------	 ######################
   844                             
   845                             
   846                                                 def slice_dataarray(da):
   847                                                     return da.sel(lat=site[1].lat.unique(),
   848                                                                   lon=site[1].lon.unique(),
   849                                                                   method='nearest')
   850                             
   851                             
   852                                                 fig, ax = plt.subplots(1, len(df_nufsamps.locnum.unique()), figsize=(18, 4))
   853                                                 ax = ax.ravel()
   854                                                 colors = ['darkgreen', 'darkblue', 'darkred']
   855                                                 fontsize = 18
   856                             
   857                                                 for i, site in enumerate(df_nufsamps.groupby('locnum')):
   858                             
   859                                                     #slice data for each site
   860                                                     prior_it = slice_dataarray(da_giapriorinterp)
   861                                                     priorvar_it = slice_dataarray(da_giapriorinterpstd)
   862                                                     top_prior = prior_it + priorvar_it * 2
   863                                                     bottom_prior = prior_it - priorvar_it * 2
   864                             
   865                                                     var_it = slice_dataarray(np.sqrt(da_varp))
   866                                                     post_it = slice_dataarray(da_priorplusgpr)
   867                                                     top = post_it + var_it * 2
   868                                                     bottom = post_it - var_it * 2
   869                             
   870                                                     site_err = 2 * (site[1].rsl_er_max)
   871                             
   872                                                     ax[i].scatter(site[1].age, site[1].rsl, c=colors[0], label='"true" RSL')
   873                                                     ax[i].errorbar(
   874                                                         site[1].age,
   875                                                         site[1].rsl,
   876                                                         site_err,
   877                                                         c=colors[0],
   878                                                         fmt='none',
   879                                                         capsize=1,
   880                                                         lw=1,
   881                                                     )
   882                             
   883                                                     prior_it.plot(ax=ax[i], c=colors[2], label='Prior $\pm 2 \sigma$')
   884                                                     ax[i].fill_between(prior_it.age,
   885                                                                        bottom_prior.squeeze(),
   886                                                                        top_prior.squeeze(),
   887                                                                        color=colors[2],
   888                                                                        alpha=0.3)
   889                             
   890                                                     post_it.plot(ax=ax[i], c=colors[1], label='Posterior $\pm 2 \sigma$')
   891                                                     ax[i].fill_between(post_it.age,
   892                                                                        bottom.squeeze(),
   893                                                                        top.squeeze(),
   894                                                                        color=colors[1],
   895                                                                        alpha=0.3)
   896                                                     #     ax[i].set_title(f'{site[0]} RSL', fontsize=fontsize)
   897                                                     ax[i].set_title('')
   898                             
   899                                                     ax[i].legend(loc='lower left')
   900                             
   901                                                 fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_realdata_fig_1D',
   902                                                             transparent=True)
   903                             
   904                                                 #plot locations of data
   905                                                 fig, ax = plt.subplots(1,
   906                                                                        len(df_nufsamps.locnum.unique()),
   907                                                                        figsize=(18, 4),
   908                                                                        subplot_kw=dict(projection=projection))
   909                                                 ax = ax.ravel()
   910                             
   911                                                 da_zeros = xr.zeros_like(da_zp)
   912                             
   913                                                 for i, site in enumerate(df_nufsamps.groupby('locnum')):
   914                                                     ax[i].coastlines(color='k')
   915                                                     ax[i].plot(site[1].lon.unique(),
   916                                                                site[1].lat.unique(),
   917                                                                c=colors[0],
   918                                                                ms=7,
   919                                                                marker='o',
   920                                                                transform=proj)
   921                                                     ax[i].plot(site[1].lon.unique(),
   922                                                                site[1].lat.unique(),
   923                                                                c=colors[0],
   924                                                                ms=25,
   925                                                                marker='o',
   926                                                                transform=proj,
   927                                                                mfc="None",
   928                                                                mec='red',
   929                                                                mew=4)
   930                                                     da_zeros[0].plot(ax=ax[i], cmap='Greys', add_colorbar=False)
   931                                                     ax[i].set_title(site[0], fontsize=fontsize)
   932                             
   933                                                 fig.savefig(dirName + f'{path_gen}_1Dlocs_fig', transparent=True)
   934                             
   935                                                 #################   DECOMPOSE GPR INTO KERNELS ####################
   936                                                 ##################  --------------------	 ######################
   937                             
   938                                                 if decomp == 'true':
   939                             
   940                                                     def predict_decomp_f(m,
   941                                                                          custom_kernel,
   942                                                                          predict_at: tf.Tensor,
   943                                                                          full_cov: bool = False,
   944                                                                          full_output_cov: bool = False,
   945                                                                          var=None):
   946                                                         """Decompose GP into individual kernels."""
   947                             
   948                                                         x_data, y_data = m.data
   949                                                         err = y_data - m.mean_function(x_data)
   950                                                         kmm = m.kernel(x_data)
   951                                                         knn = custom_kernel(predict_at, full=full_cov)
   952                                                         kmn = custom_kernel(x_data, predict_at)
   953                                                         num_data = x_data.shape[0]
   954                                                         s = tf.linalg.diag(tf.convert_to_tensor(var))  # added diagonal variance
   955                                                         conditional = gpf.conditionals.base_conditional
   956                                                         f_mean_zero, f_var = conditional(
   957                                                             kmn, kmm + s, knn, err, full_cov=full_cov,
   958                                                             white=False)  # [N, P], [N, P] or [P, N, N]
   959                                                         f_mean = np.array(f_mean_zero + m.mean_function(predict_at))
   960                                                         f_var = np.array(f_var)
   961                                                         return f_mean, f_var
   962                             
   963                             
   964                                                     def reshape_decomp(k, var=None):
   965                                                         A, var = predict_decomp_f(m, k, xyt, var=var)
   966                                                         A = A.reshape(nout, nout, len(ages))
   967                                                         var = var.reshape(nout, nout, len(ages))
   968                                                         return A, var
   969                             
   970                             
   971                                                     def make_dataarray(da):
   972                                                         coords = [lon, lat, ages]
   973                                                         dims = ['lon', 'lat', 'age']
   974                                                         return xr.DataArray(da, coords=coords,
   975                                                                             dims=dims).transpose('age', 'lat', 'lon')
   976                             
   977                             
   978                                                     A1, var1 = reshape_decomp(k1,
   979                                                                               var=df_place.rsl_er_max.ravel()**2)  #gia spatial
   980                                                     A2, var2 = reshape_decomp(k2,
   981                                                                               var=df_place.rsl_er_max.ravel()**2)  #gia temporal
   982                                                     A3, var3 = reshape_decomp(
   983                                                         k3,
   984                                                         var=df_place.rsl_er_max.ravel()**2)  #readvance spatial
   985                                                     A4, var4 = reshape_decomp(
   986                                                         k4,
   987                                                         var=df_place.rsl_er_max.ravel()**2)  #readvance temporal
   988                                                     A5, var5 = reshape_decomp(
   989                                                         k5,
   990                                                         var=df_place.rsl_er_max.ravel()**2)  #readvance spatial
   991                             
   992                                                     da_A1 = make_dataarray(A1)
   993                                                     da_var1 = make_dataarray(var1)
   994                             
   995                                                     da_A2 = make_dataarray(A2)
   996                                                     da_var2 = make_dataarray(var2)
   997                             
   998                                                     da_A3 = make_dataarray(A3)
   999                                                     da_var3 = make_dataarray(var3)
  1000                             
  1001                                                     da_A4 = make_dataarray(A4)
  1002                                                     da_var4 = make_dataarray(var4)
  1003                             
  1004                                                     da_A5 = make_dataarray(A5)
  1005                                                     da_var5 = make_dataarray(var5)
  1006                             
  1007                                                     #################   PLOT DECOMPOSED KERNELS    ####################
  1008                                                     ##################  --------------------	   ####################
  1009                             
  1010                                                     fig, ax = plt.subplots(1, 6, figsize=(24, 4))
  1011                                                     ax = ax.ravel()
  1012                                                     da_A1[0, :, :].plot(ax=ax[0], cmap='RdBu_r')
  1013                             
  1014                                                     da_A2[0, :, :].plot(ax=ax[1], cmap='RdBu_r')
  1015                             
  1016                                                     da_A3[0, :, :].plot(ax=ax[2], cmap='RdBu_r')
  1017                             
  1018                                                     da_A4[:, 0, 0].plot(ax=ax[3])
  1019                             
  1020                                                     da_A5[:, 0, 0].plot(ax=ax[4])
  1021                             
  1022                                                     fig.savefig(dirName + f'{path_gen}_decompkernels', transparent=True)
  1023                                                 else:
  1024                                                     pass
  1025                             
  1026                                             else:
  1027                                                 pass
  1028                             
  1029                                         #store log likelihood in dataframe
  1030    952.5 MiB      0.0 MiB                   df_out = pd.DataFrame({'modelrun': modrunlist,
  1031    952.5 MiB      0.0 MiB                                    'log_marginal_likelihood': loglikelist})
  1032                             
  1033                             
  1034    952.5 MiB      0.0 MiB                   writepath = f'output/{path_gen}_loglikelihood'
  1035    952.5 MiB      0.0 MiB                   df_out.to_csv(writepath, index=False)
  1036    952.8 MiB      0.3 MiB                   df_likes = pd.read_csv(writepath)
  1037                             
  1038                                         # make heatmap for upper vs. lower mantle viscosities at one lithosphere thickness
  1039                             
  1040    952.8 MiB      0.0 MiB               if ice_model =='glac1d_':
  1041    952.8 MiB      0.0 MiB                   df_likes['um'] = [key.split('_')[2][3:] for key in df_likes.modelrun]
  1042    952.8 MiB      0.0 MiB                   df_likes['lm'] = [key.split('_')[3][2:] for key in df_likes.modelrun]
  1043    952.8 MiB      0.0 MiB                   df_likes['lith'] = [key.split('_')[1][1:3] for key in df_likes.modelrun]
  1044    952.8 MiB      0.0 MiB                   df_likes['icemodel'] = [key.split('_')[0] for key in df_likes.modelrun]
  1045                                         elif ice_model == 'd6g_h6g_':
  1046                             #                 df_likes = df_likes.drop([36])
  1047                                             df_likes['um'] = [key.split('_')[3][3:] for key in df_likes.modelrun]
  1048                                             df_likes['lm'] = [key.split('_')[4][2:] for key in df_likes.modelrun]
  1049                                             df_likes['lith'] = [key.split('_')[2][1:3] for key in df_likes.modelrun]
  1050                                             df_likes['icemodel'] = [key.split('_l')[0] for key in df_likes.modelrun]
  1051                             
  1052    952.8 MiB      0.0 MiB               df_likes.lm = df_likes.lm.astype(float)
  1053    952.8 MiB      0.0 MiB               df_likes.um = df_likes.um.astype(float)
  1054    952.9 MiB      0.2 MiB               heatmap = df_likes.pivot_table(index='um', columns='lm', values='log_marginal_likelihood')
  1055                             
  1056                             
  1057    952.9 MiB      0.0 MiB               fig, ax = plt.subplots(1, 1, figsize=(6, 6))
  1058    954.0 MiB      1.1 MiB               sns.heatmap(heatmap,  cmap='coolwarm', ax=ax,  cbar_kws={'label': 'negative log likelihood'})
  1059    954.1 MiB      0.1 MiB               ax.set_title(f'{place} {ages[0]} - {ages[-1]} yrs \n {ice_model} : {df_likes.lith[0]} km lithosphere'); # (havsine)
  1060                             
  1061    954.1 MiB      0.0 MiB               fig.savefig(dirName + f'{path_gen}_likelihood_heatmap', transparent=True)   # _havsine


