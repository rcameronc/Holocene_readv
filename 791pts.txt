number of datapoints =  (791, 14)
model built, time= 0.15238094329833984
model minimized, time= 163.6482458114624
<IPython.core.display.HTML object>
time elapsed =  183.94808673858643
negative log marginal likelihood = 378.0493690911435
Directory  figs/europe/  already exists
Filename: readv_012920_realdata_europe_glac1d.py

Line #    Mem usage    Increment   Line Contents
================================================
    39    294.6 MiB    294.6 MiB   @profile
    40                             def readv():
    41
    42                                 # set the colormap and centre the colorbar
    43    294.6 MiB      0.0 MiB       class MidpointNormalize(Normalize):
    44    294.6 MiB      0.0 MiB           """Normalise the colorbar.  e.g. norm=MidpointNormalize(mymin, mymax, 0.)"""
    45    657.6 MiB      0.0 MiB           def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
    46    657.6 MiB      0.0 MiB               self.midpoint = midpoint
    47    657.6 MiB      0.0 MiB               Normalize.__init__(self, vmin, vmax, clip)
    48
    49    679.0 MiB     15.3 MiB           def __call__(self, value, clip=None):
    50    679.0 MiB      0.0 MiB               x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
    51    679.0 MiB      0.0 MiB               return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))
    52
    53
    54                                 ####################  Initialize parameters #######################
    55                                 #################### ---------------------- #######################
    56
    57    294.6 MiB      0.0 MiB       ice_model = 'd6g_h6g_'  #'glac1d_'
    58    294.6 MiB      0.0 MiB       lith_thickness = 'l7'  # 'l90C'
    59    294.6 MiB      0.0 MiB       place = 'europe'
    60
    61                                 locs = {
    62    294.6 MiB      0.0 MiB           'england': [-12, 2, 50, 60],
    63    294.6 MiB      0.0 MiB           'southchina': [110, 117, 19, 23],
    64    294.6 MiB      0.0 MiB           'easternhem': [50, 178, -45, 80],
    65    294.6 MiB      0.0 MiB           'westernhem': [-175, 30, -80, 75],
    66    294.6 MiB      0.0 MiB           'world': [-179.8, 179.8, -89.8, 89.8],
    67    294.6 MiB      0.0 MiB           'namerica': [-150, -20, 10, 75],
    68    294.6 MiB      0.0 MiB           'eastcoast': [-88, -65, 15, 40],
    69    294.6 MiB      0.0 MiB           'europe': [-20, 15, 35, 70]
    70                                 }
    71    294.6 MiB      0.0 MiB       extent = locs[place]
    72    294.6 MiB      0.0 MiB       tmax, tmin, tstep = 7050, 3450, 100
    73
    74    294.6 MiB      0.0 MiB       ages_lgm = np.arange(100, 26000, tstep)[::-1]
    75
    76                                 #import khan dataset
    77    294.6 MiB      0.0 MiB       path = 'data/GSL_LGM_120519_.csv'
    78
    79    312.9 MiB     18.3 MiB       df = pd.read_csv(path, encoding="ISO-8859-15", engine='python')
    80    317.9 MiB      5.0 MiB       df = df.replace('\s+', '_', regex=True).replace('-', '_', regex=True).\
    81    321.5 MiB      0.5 MiB               applymap(lambda s:s.lower() if type(s) == str else s)
    82    321.5 MiB      0.0 MiB       df.columns = df.columns.str.lower()
    83    321.5 MiB      0.0 MiB       df.rename_axis('index', inplace=True)
    84    321.5 MiB      0.1 MiB       df = df.rename({'latitude': 'lat', 'longitude': 'lon'}, axis='columns')
    85    321.6 MiB      0.0 MiB       dfind, dfterr, dfmar = df[(df.type == 0)
    86    321.7 MiB      0.2 MiB                                 & (df.age > 0)], df[df.type == 1], df[df.type == -1]
    87    321.7 MiB      0.0 MiB       np.sort(list(set(dfind.regionname1)))
    88
    89                                 #select location
    90    321.7 MiB      0.0 MiB       df_place = dfind[(dfind.age > tmin) & (dfind.age < tmax) &
    91                                                  (dfind.lon > extent[0])
    92                                                  & (dfind.lon < extent[1])
    93                                                  & (dfind.lat > extent[2])
    94                                                  & (dfind.lat < extent[3])
    95    321.8 MiB      0.1 MiB                        & (dfind.rsl_er_max < 1)][[
    96    321.9 MiB      0.1 MiB                            'lat', 'lon', 'rsl', 'rsl_er_max', 'age'
    97                                                  ]]
    98                                 # & (df_place.rsl_er_max < 1)
    99    321.9 MiB      0.0 MiB       df_place.shape
   100
   101                                 ####################  	Plot locations  	#######################
   102                                 #################### ---------------------- #######################
   103
   104                                 #get counts by location rounded to nearest 0.1 degree
   105    321.9 MiB      0.0 MiB       df_rnd = df_place.copy()
   106    321.9 MiB      0.0 MiB       df_rnd.lat = np.round(df_rnd.lat, 1)
   107    321.9 MiB      0.0 MiB       df_rnd.lon = np.round(df_rnd.lon, 1)
   108    321.9 MiB      0.0 MiB       dfcounts_place = df_rnd.groupby(
   109    322.0 MiB      0.1 MiB           ['lat', 'lon']).count().reset_index()[['lat', 'lon', 'rsl', 'age']]
   110
   111                                 #plot
   112    327.9 MiB      5.9 MiB       fig = plt.figure(figsize=(10, 7))
   113    328.7 MiB      0.7 MiB       ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())
   114
   115    328.9 MiB      0.2 MiB       ax.set_extent(extent)
   116    328.9 MiB      0.0 MiB       ax.coastlines(resolution='110m', linewidth=1, zorder=2)
   117    328.9 MiB      0.0 MiB       ax.add_feature(cfeature.OCEAN, zorder=0)
   118    328.9 MiB      0.0 MiB       ax.add_feature(cfeature.LAND, color='palegreen', zorder=1)
   119    328.9 MiB      0.0 MiB       ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=3)
   120    328.9 MiB      0.0 MiB       ax.gridlines(linewidth=1, color='white', alpha=0.5, zorder=4)
   121    328.9 MiB      0.0 MiB       scat = ax.scatter(dfcounts_place.lon,
   122    328.9 MiB      0.0 MiB                         dfcounts_place.lat,
   123    328.9 MiB      0.0 MiB                         s=dfcounts_place.rsl * 70,
   124    328.9 MiB      0.0 MiB                         c='lightsalmon',
   125    328.9 MiB      0.0 MiB                         vmin=-20,
   126    328.9 MiB      0.0 MiB                         vmax=20,
   127    328.9 MiB      0.0 MiB                         cmap='coolwarm',
   128    328.9 MiB      0.0 MiB                         edgecolor='k',
   129    328.9 MiB      0.0 MiB                         linewidths=1,
   130    328.9 MiB      0.0 MiB                         transform=ccrs.PlateCarree(),
   131    329.4 MiB      0.5 MiB                         zorder=5)
   132    329.4 MiB      0.0 MiB       size = Line2D(range(4),
   133    329.4 MiB      0.0 MiB                     range(4),
   134    329.4 MiB      0.0 MiB                     color="black",
   135    329.4 MiB      0.0 MiB                     marker='o',
   136    329.4 MiB      0.0 MiB                     linewidth=0,
   137    329.4 MiB      0.0 MiB                     linestyle='none',
   138    329.4 MiB      0.0 MiB                     markersize=16,
   139    329.4 MiB      0.0 MiB                     markerfacecolor="lightsalmon")
   140    329.4 MiB      0.0 MiB       labels = ['RSL datapoint location']
   141    329.4 MiB      0.0 MiB       leg = plt.legend([size],
   142    329.4 MiB      0.0 MiB                        labels,
   143    329.4 MiB      0.0 MiB                        loc='lower left',
   144    329.4 MiB      0.0 MiB                        bbox_to_anchor=(0.00, 0.00),
   145    329.4 MiB      0.0 MiB                        prop={'size': 20},
   146    329.4 MiB      0.1 MiB                        fancybox=True)
   147    329.4 MiB      0.0 MiB       leg.get_frame().set_edgecolor('k')
   148    329.4 MiB      0.0 MiB       ax.set_title('')
   149
   150                                 ####################  Make 3D fingerprint  #######################
   151                                 #################### ---------------------- #######################
   152
   153    329.4 MiB      0.0 MiB       filename = 'data/WAISreadvance_VM5_6ka_1step.mat'
   154
   155    328.6 MiB      0.0 MiB       waismask = io.loadmat(filename, squeeze_me=True)
   156    328.6 MiB      0.0 MiB       ds_mask = xr.Dataset({'rsl': (['lat', 'lon', 'age'], waismask['RSL'])},
   157                                                      coords={
   158    328.6 MiB      0.0 MiB                                'lon': waismask['lon_out'],
   159    328.6 MiB      0.0 MiB                                'lat': waismask['lat_out'],
   160    328.7 MiB      0.0 MiB                                'age': np.round(waismask['ice_time_new'])
   161                                                      })
   162    328.7 MiB      0.0 MiB       fingerprint = ds_mask.sel(age=ds_mask.age[0])
   163
   164
   165    602.1 MiB      0.0 MiB       def make_fingerprint(start, end, maxscale):
   166
   167                                     #palindromic scaling vector
   168    602.1 MiB      0.0 MiB           def palindrome(maxscale, ages):
   169                                         """ Make palindrome scale 0-maxval with number of steps. """
   170    602.1 MiB      0.0 MiB               half = np.linspace(0, maxscale, 1 + (len(ages) - 1) // 2)
   171    602.1 MiB      0.0 MiB               scalefactor = np.concatenate([half, half[::-1]])
   172    602.1 MiB      0.0 MiB               return scalefactor
   173
   174    602.1 MiB      0.0 MiB           ages_readv = ages_lgm[(ages_lgm < start) & (ages_lgm >= end)]
   175    602.1 MiB      0.0 MiB           scale = palindrome(maxscale, ages_readv)
   176
   177                                     #scale factor same size as ice model ages
   178    602.1 MiB      0.0 MiB           pre = np.zeros(np.where(ages_lgm == start)[0])
   179    602.1 MiB      0.0 MiB           post = np.zeros(len(ages_lgm) - len(pre) - len(scale))
   180
   181    602.1 MiB      0.0 MiB           readv_scale = np.concatenate([pre, scale, post])
   182
   183                                     #scale factor into dataarray
   184    602.1 MiB      0.0 MiB           da_scale = xr.DataArray(readv_scale, coords=[('age', ages_lgm)])
   185
   186                                     # broadcast fingerprint & scale to same dimensions;
   187    602.1 MiB      0.0 MiB           fingerprint_out, fing_scaled = xr.broadcast(fingerprint.rsl, da_scale)
   188
   189                                     # mask fingerprint with scale to get LGM-pres timeseries
   190    602.1 MiB      0.0 MiB           ds_fingerprint = (fingerprint_out *
   191    861.0 MiB    259.0 MiB                             fing_scaled).transpose().to_dataset(name='rsl')
   192
   193                                     # scale dataset with fingerprint to LGM-present length & 0-max-0 over x years
   194    861.0 MiB      0.0 MiB           xrlist = []
   195   1104.0 MiB      0.0 MiB           for i, key in enumerate(da_scale):
   196   1104.0 MiB      1.1 MiB               mask = ds_fingerprint.sel(age=ds_fingerprint.age[i].values) * key
   197   1104.0 MiB      0.0 MiB               mask = mask.assign_coords(scale=key,
   198   1104.0 MiB      0.0 MiB                                         age=ages_lgm[i]).expand_dims(dim=['age'])
   199   1104.0 MiB      0.0 MiB               xrlist.append(mask)
   200   1330.6 MiB    259.2 MiB           ds_readv = xr.concat(xrlist, dim='age')
   201
   202   1330.6 MiB      0.0 MiB           ds_readv.coords['lon'] = pd.DataFrame((ds_readv.lon[ds_readv.lon >= 180] - 360)- 0.12) \
   203   1330.6 MiB      0.0 MiB                                   .append(pd.DataFrame(ds_readv.lon[ds_readv.lon < 180]) + 0.58) \
   204   1330.6 MiB      0.0 MiB                                   .reset_index(drop=True).squeeze()
   205   1330.6 MiB      0.0 MiB           ds_readv = ds_readv.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   206
   207                                     # Add readv to modeled RSL at locations with data
   208                                     ##### Need to fix this, as currently slice does not acknowledge new coords #########
   209   1330.6 MiB      0.0 MiB           ds_readv = ds_readv.sel(age=slice(tmax, tmin),
   210   1330.6 MiB      0.0 MiB                                   lon=slice(df_place.lon.min() + 180 - 2,
   211   1330.6 MiB      0.0 MiB                                             df_place.lon.max() + 180 + 2),
   212   1330.6 MiB      0.0 MiB                                   lat=slice(df_place.lat.max() + 2,
   213   1330.6 MiB      0.1 MiB                                             df_place.lat.min() - 2))
   214   1330.6 MiB      0.0 MiB           return ds_readv
   215
   216
   217                                 #Make deterministic readvance fingerprint
   218    328.7 MiB      0.0 MiB       start, end = 6100, 3000
   219    328.7 MiB      0.0 MiB       maxscale = 2.25
   220    602.1 MiB      0.0 MiB       ds_readv = make_fingerprint(start, end, maxscale)
   221
   222                                 #Make readvance prior
   223    602.1 MiB      0.0 MiB       start, end = 8000, 2000
   224    602.1 MiB      0.0 MiB       maxscale = 2.25
   225    828.6 MiB      0.0 MiB       ds_readvprior = make_fingerprint(start, end, maxscale)
   226    829.0 MiB      0.4 MiB       ds_readvprior_std = ds_readvprior * 0.3
   227
   228                                 ####################  Build  GIA models 	#######################
   229                                 #################### ---------------------- #######################
   230
   231                                 #Use either glac1d or ICE6G
   232    829.0 MiB      0.0 MiB       if ice_model == 'glac1d_':
   233
   234                                     def build_dataset(model):
   235                                         """download model runs from local directory."""
   236
   237                                         path = f'data/glac1d_/output_{model}'
   238                                         files = f'{path}*.nc'
   239                                         basefiles = glob.glob(files)
   240                                         modelrun = [
   241                                             key.split('glac1d/output_', 1)[1][:-3].replace('.', '_')
   242                                             for key in basefiles
   243                                         ]
   244                                         dss = xr.open_mfdataset(files,
   245                                                                 chunks=None,
   246                                                                 concat_dim='modelrun',
   247                                                                 combine='nested')
   248                                         lats, lons, times = dss.LAT.values[0], dss.LON.values[
   249                                             0], dss.TIME.values[0]
   250                                         ds = dss.drop(['LAT', 'LON', 'TIME'])
   251                                         ds = ds.assign_coords(lat=lats,
   252                                                               lon=lons,
   253                                                               time=times,
   254                                                               modelrun=modelrun).rename({
   255                                                                   'time': 'age',
   256                                                                   'RSL': 'rsl'
   257                                                               })
   258                                         return ds
   259
   260                                     def one_mod(names):
   261                                         """Organize model runs into xarray dataset."""
   262                                         ds1 = build_dataset(names[0])
   263                                         names = names[1:]
   264                                         ds = ds1.chunk({'lat': 10, 'lon': 10})
   265                                         for i in range(len(names)):
   266                                             temp = build_dataset(names[i])
   267                                             temp1 = temp.interp_like(ds1)
   268                                             temp1['modelrun'] = temp['modelrun']
   269                                             ds = xr.concat([ds, temp1], dim='modelrun')
   270                                         ds['age'] = ds['age'] * 1000
   271                                         ds = ds.roll(lon=256, roll_coords=True)
   272                                         ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \
   273                                                                 .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \
   274                                                                 .reset_index(drop=True).squeeze()
   275                                         ds.coords['lat'] = ds.lat[::-1]
   276                                         ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   277                                         return ds
   278
   279                                     #make composite of a bunch of GIA runs, i.e. GIA prior
   280                                     ds = one_mod([ice_model + lith_thickness])
   281
   282                                     ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   283                                                            lon=slice(df_place.lon.min() - 2,
   284                                                                      df_place.lon.max() + 2),
   285                                                            lat=slice(df_place.lat.min() - 2,
   286                                                                      df_place.lat.max() + 2))
   287                                     ds_area = ds_sliced.mean(dim='modelrun').load().to_dataset().interp(
   288                                         age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   289                                     ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(
   290                                         age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   291
   292                                     # make "true" RSL by adding single GIA run and fingerprint
   293                                     lithmantle = 'l71C_ump2_lm50'
   294                                     ds_diff = one_mod(
   295                                         [ice_model + 'l71C']).sel(modelrun=ice_model + lithmantle).rsl.sel(
   296                                             age=slice(tmax, tmin),
   297                                             lon=slice(df_place.lon.min() - 2,
   298                                                       df_place.lon.max() + 2),
   299                                             lat=slice(df_place.lat.min() - 2,
   300                                                       df_place.lat.max() + 2)).load().to_dataset().interp(
   301                                                           age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   302
   303                                 else:
   304
   305    909.5 MiB      0.0 MiB           def build_dataset(model):
   306                                         """download model runs from local directory."""
   307
   308    909.5 MiB      0.0 MiB               path = f'data/d6g_h6g_/output_{model}'
   309    909.5 MiB      0.0 MiB               files = f'{path}*.nc'
   310    909.5 MiB      0.0 MiB               basefiles = glob.glob(files)
   311                                         modelrun = [
   312    909.5 MiB      0.0 MiB                   key.split('d6g_h6g_/output_', 1)[1][:-3].replace('.', '_')
   313    909.5 MiB      0.0 MiB                   for key in basefiles
   314                                         ]
   315    909.5 MiB      0.0 MiB               dss = xr.open_mfdataset(files,
   316    909.5 MiB      0.0 MiB                                       chunks=None,
   317    909.5 MiB      0.0 MiB                                       concat_dim='modelrun',
   318    909.5 MiB     29.6 MiB                                       combine='nested')
   319    909.5 MiB      2.6 MiB               lats, lons, times = dss.LAT.values[0], dss.LON.values[
   320    909.5 MiB      0.6 MiB                   0], dss.TIME.values[0]
   321    909.5 MiB      0.0 MiB               ds = dss.drop(['LAT', 'LON', 'TIME'])
   322    909.5 MiB      0.0 MiB               ds = ds.assign_coords(lat=lats,
   323    909.5 MiB      0.0 MiB                                     lon=lons,
   324    909.5 MiB      0.0 MiB                                     time=times,
   325    909.5 MiB      0.0 MiB                                     modelrun=modelrun).rename({
   326    909.5 MiB      0.0 MiB                                         'time': 'age',
   327    909.5 MiB      0.0 MiB                                         'RSL': 'rsl'
   328                                                               })
   329    909.5 MiB      0.0 MiB               return ds
   330
   331    909.5 MiB      0.0 MiB           def one_mod(names):
   332                                         """Organize model runs into xarray dataset."""
   333    909.5 MiB      0.0 MiB               ds1 = build_dataset(names[0])
   334    909.5 MiB      0.0 MiB               names = names[1:]
   335    936.0 MiB     29.1 MiB               ds = ds1.chunk({'lat': 10, 'lon': 10})
   336    936.0 MiB      0.0 MiB               for i in range(len(names)):
   337                                             temp = build_dataset(names[i])
   338                                             temp1 = temp.interp_like(ds1)
   339                                             temp1['modelrun'] = temp['modelrun']
   340                                             ds = xr.concat([ds, temp1], dim='modelrun')
   341    936.0 MiB      0.0 MiB               ds['age'] = ds['age'] * 1000
   342   1044.8 MiB    111.4 MiB               ds = ds.roll(lon=256, roll_coords=True)
   343   1044.8 MiB      0.1 MiB               ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \
   344   1044.8 MiB      0.0 MiB                                       .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \
   345   1044.8 MiB      0.0 MiB                                       .reset_index(drop=True).squeeze()
   346   1044.8 MiB      0.0 MiB               ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   347   1044.8 MiB      0.0 MiB               return ds
   348
   349                                     #make composite of a bunch of GIA runs, i.e. GIA prior
   350   1002.4 MiB      0.0 MiB           ds = one_mod([ice_model + lith_thickness])
   351
   352   1002.4 MiB      0.0 MiB           ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   353   1002.4 MiB      0.0 MiB                                  lon=slice(df_place.lon.min() - 2,
   354   1002.4 MiB      0.0 MiB                                            df_place.lon.max() + 2),
   355   1002.4 MiB      0.0 MiB                                  lat=slice(df_place.lat.max() + 2,
   356   1002.6 MiB      0.1 MiB                                            df_place.lat.min() - 2))
   357    957.0 MiB      0.0 MiB           ds_area = ds_sliced.mean(dim='modelrun').load().to_dataset().interp(
   358    959.5 MiB      2.5 MiB               age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   359    955.8 MiB      0.0 MiB           ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(
   360    909.5 MiB      0.0 MiB               age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   361
   362                                     # make "true" RSL by adding single GIA run and fingerprint
   363    909.5 MiB      0.0 MiB           lithmantle = 'l71C_ump2_lm50'
   364    909.5 MiB      0.0 MiB           ds_diff = one_mod(
   365   1044.8 MiB      0.0 MiB               [ice_model + 'l71C']).sel(modelrun=ice_model + lithmantle).rsl.sel(
   366   1044.8 MiB      0.0 MiB                   age=slice(tmax, tmin),
   367   1044.8 MiB      0.0 MiB                   lon=slice(df_place.lon.min() - 2,
   368   1044.8 MiB      0.0 MiB                             df_place.lon.max() + 2),
   369   1044.8 MiB      0.0 MiB                   lat=slice(df_place.lat.max() + 2,
   370    912.4 MiB      0.0 MiB                             df_place.lat.min() - 2)).load().to_dataset().interp(
   371    913.6 MiB      1.2 MiB                                 age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   372
   373                                 #make residual by subtracting GIA prior and fingerprint prior from "true" GIA
   374    916.0 MiB      2.4 MiB       ds_true = ds_diff + ds_readv
   375    916.0 MiB      0.0 MiB       ds_prior = ds_area + ds_readvprior
   376    916.0 MiB      0.0 MiB       ds_priorstd = ds_areastd + ds_readvprior_std
   377    916.0 MiB      0.0 MiB       ds_truelessprior = ds_true - ds_prior
   378
   379
   380                                 #sample each model at points where we have RSL data
   381    916.0 MiB      0.0 MiB       def ds_select(ds):
   382    916.0 MiB      0.0 MiB           return ds.rsl.sel(age=[row.age],
   383    916.0 MiB      0.0 MiB                             lon=[row.lon],
   384    916.0 MiB      0.0 MiB                             lat=[row.lat],
   385    916.0 MiB      0.0 MiB                             method='nearest').squeeze().values
   386
   387
   388                                 #select points at which RSL data exists
   389    916.0 MiB      0.0 MiB       for i, row in df_place.iterrows():
   390    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_true'] = ds_select(ds_true)
   391    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_resid'] = ds_select(ds_truelessprior)
   392    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_realresid'] = df_place.rsl[i] - ds_select(ds_area)
   393
   394    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_totalprior'] = ds_select(ds_prior)
   395    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_totalprior_std'] = ds_select(ds_priorstd)
   396    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_giaprior'] = ds_select(ds_area)
   397    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_giaprior_std'] = ds_select(ds_areastd)
   398    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_readvprior'] = ds_select(ds_readvprior)
   399    916.0 MiB      0.0 MiB           df_place.loc[i, 'rsl_readvprior_std'] = ds_select(ds_readvprior_std)
   400    586.2 MiB      0.0 MiB       print('number of datapoints = ', df_place.shape)
   401
   402                                 ##################	  RUN GP REGRESSION 	#######################
   403                                 ##################  --------------------	 ######################
   404
   405    586.2 MiB      0.0 MiB       start = time.time()
   406
   407    586.2 MiB      0.0 MiB       Data = Tuple[tf.Tensor, tf.Tensor]
   408    586.2 MiB      0.0 MiB       likelihood = df_place.rsl_er_max.ravel()**2 + df_place.rsl_giaprior_std.ravel(
   409    586.2 MiB      0.0 MiB       )**2  # here we define likelihood
   410
   411
   412    586.2 MiB      0.0 MiB       class GPR_diag(gpf.models.GPModel):
   413                                     r"""
   414                                     Gaussian Process Regression.
   415                                     This is a vanilla implementation of GP regression with a pointwise Gaussian
   416                                     likelihood.  Multiple columns of Y are treated independently.
   417                                     The log likelihood of this models is sometimes referred to as the 'marginal log likelihood',
   418                                     and is given by
   419                                     .. math::
   420                                        \log p(\mathbf y \,|\, \mathbf f) =
   421                                             \mathcal N\left(\mathbf y\,|\, 0, \mathbf K + \sigma_n \mathbf I\right)
   422    586.2 MiB      0.0 MiB           """
   423    592.3 MiB      0.0 MiB           def __init__(self,
   424                                                  data: Data,
   425                                                  kernel: Kernel,
   426    586.2 MiB      0.0 MiB                        mean_function: Optional[MeanFunction] = None,
   427    586.2 MiB      0.0 MiB                        likelihood=likelihood):
   428    592.3 MiB      0.0 MiB               likelihood = gpf.likelihoods.Gaussian(variance=likelihood)
   429    592.3 MiB      0.0 MiB               _, y_data = data
   430    592.3 MiB      0.0 MiB               super().__init__(kernel,
   431    592.3 MiB      0.0 MiB                                likelihood,
   432    592.3 MiB      0.0 MiB                                mean_function,
   433    592.3 MiB      0.0 MiB                                num_latent=y_data.shape[-1])
   434    592.3 MiB      0.0 MiB               self.data = data
   435
   436    714.3 MiB      0.0 MiB           def log_likelihood(self):
   437                                         """
   438                                         Computes the log likelihood.
   439                                         """
   440    714.3 MiB      0.0 MiB               x, y = self.data
   441    738.2 MiB      0.5 MiB               K = self.kernel(x)
   442    738.2 MiB      0.0 MiB               num_data = x.shape[0]
   443    738.2 MiB      0.0 MiB               k_diag = tf.linalg.diag_part(K)
   444    738.2 MiB      0.0 MiB               s_diag = tf.convert_to_tensor(self.likelihood.variance)
   445    738.2 MiB      0.0 MiB               jitter = tf.cast(tf.fill([num_data], default_jitter()),
   446    738.2 MiB      0.0 MiB                                'float64')  # stabilize K matrix w/jitter
   447    738.2 MiB      0.0 MiB               ks = tf.linalg.set_diag(K, k_diag + s_diag + jitter)
   448    739.2 MiB      1.0 MiB               L = tf.linalg.cholesky(ks)
   449    739.2 MiB      0.2 MiB               m = self.mean_function(x)
   450
   451                                         # [R,] log-likelihoods for each independent dimension of Y
   452    739.2 MiB      0.0 MiB               log_prob = multivariate_normal(y, m, L)
   453    739.2 MiB      0.0 MiB               return tf.reduce_sum(log_prob)
   454
   455    722.2 MiB      0.0 MiB           def predict_f(self,
   456                                                   predict_at: tf.Tensor,
   457                                                   full_cov: bool = False,
   458    586.2 MiB      0.0 MiB                         full_output_cov: bool = False):
   459                                         r"""
   460                                         This method computes predictions at X \in R^{N \x D} input points
   461                                         .. math::
   462                                             p(F* | Y)
   463                                         where F* are points on the GP at new data points, Y are noisy observations at training data points.
   464                                         """
   465    722.2 MiB      0.0 MiB               x_data, y_data = self.data
   466    722.2 MiB      0.0 MiB               err = y_data - self.mean_function(x_data)
   467
   468    722.2 MiB      0.0 MiB               kmm = self.kernel(x_data)
   469    707.2 MiB      0.0 MiB               knn = self.kernel(predict_at, full=full_cov)
   470   1180.1 MiB      0.0 MiB               kmn = self.kernel(x_data, predict_at)
   471
   472   1180.1 MiB      0.0 MiB               num_data = x_data.shape[0]
   473   1180.1 MiB      0.0 MiB               s = tf.linalg.diag(tf.convert_to_tensor(
   474   1184.9 MiB      4.8 MiB                   self.likelihood.variance))  #changed from normal GPR
   475
   476   1184.9 MiB      0.0 MiB               conditional = gpf.conditionals.base_conditional
   477   1184.9 MiB      0.0 MiB               f_mean_zero, f_var = conditional(
   478   1189.7 MiB      4.8 MiB                   kmn, kmm + s, knn, err, full_cov=full_cov,
   479   1255.3 MiB     65.6 MiB                   white=False)  # [N, P], [N, P] or [P, N, N]
   480   1256.0 MiB      0.7 MiB               f_mean = f_mean_zero + self.mean_function(predict_at)
   481   1256.0 MiB      0.0 MiB               return f_mean, f_var
   482
   483
   484    586.2 MiB      0.0 MiB       def normalize(df):
   485    586.2 MiB      0.0 MiB           return np.array((df - df.mean()) / df.std()).reshape(len(df), 1)
   486
   487
   488    712.9 MiB      0.0 MiB       def denormalize(y_pred, df):
   489    712.9 MiB      0.0 MiB           return np.array((y_pred * df.std()) + df.mean())
   490
   491
   492    592.3 MiB      0.0 MiB       def bounded_parameter(low, high, param):
   493                                     """Make parameter tfp Parameter with optimization bounds."""
   494    592.3 MiB      0.1 MiB           affine = tfb.AffineScalar(shift=tf.cast(low, tf.float64),
   495    592.3 MiB      0.1 MiB                                     scale=tf.cast(high - low, tf.float64))
   496    592.3 MiB      0.0 MiB           sigmoid = tfb.Sigmoid()
   497    592.3 MiB      0.0 MiB           logistic = tfb.Chain([affine, sigmoid])
   498    592.3 MiB      0.1 MiB           parameter = gpf.Parameter(param, transform=logistic, dtype=tf.float64)
   499    592.3 MiB      0.0 MiB           return parameter
   500
   501
   502    586.2 MiB      0.0 MiB       class HaversineKernel_Matern52(gpf.kernels.Matern52):
   503                                     """
   504                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   505                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   506    586.2 MiB      0.0 MiB           """
   507                                     def __init__(
   508                                         self,
   509                                         lengthscale=1.0,
   510                                         variance=1.0,
   511    586.2 MiB      0.0 MiB               active_dims=None,
   512                                     ):
   513                                         super().__init__(
   514                                             active_dims=active_dims,
   515                                             variance=variance,
   516                                             lengthscale=lengthscale,
   517                                         )
   518
   519    586.2 MiB      0.0 MiB           def haversine_dist(self, X, X2):
   520                                         pi = np.pi / 180
   521                                         f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   522                                         f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   523                                         d = tf.sin((f - f2) / 2)**2
   524                                         lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   525                                                     tf.expand_dims(X2[:, 0] * pi, -2)
   526                                         cos_prod = tf.cos(lat2) * tf.cos(lat1)
   527                                         a = d[:, :, 0] + cos_prod * d[:, :, 1]
   528                                         c = tf.asin(tf.sqrt(a)) * 6371 * 2
   529                                         return c
   530
   531    586.2 MiB      0.0 MiB           def scaled_squared_euclid_dist(self, X, X2):
   532                                         """
   533                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   534                                         """
   535                                         if X2 is None:
   536                                             X2 = X
   537                                         dist = tf.square(self.haversine_dist(X, X2) / self.lengthscale)
   538                                         return dist
   539
   540
   541    586.2 MiB      0.0 MiB       class HaversineKernel_Matern32(gpf.kernels.Matern32):
   542                                     """
   543                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   544                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   545    586.2 MiB      0.0 MiB           """
   546    592.2 MiB      0.0 MiB           def __init__(
   547                                         self,
   548                                         lengthscale=1.0,
   549                                         variance=1.0,
   550    586.2 MiB      0.0 MiB               active_dims=None,
   551                                     ):
   552    592.2 MiB      0.0 MiB               super().__init__(
   553    592.2 MiB      0.0 MiB                   active_dims=active_dims,
   554    592.2 MiB      0.0 MiB                   variance=variance,
   555    592.2 MiB      5.7 MiB                   lengthscale=lengthscale,
   556                                         )
   557
   558   1181.0 MiB      0.0 MiB           def haversine_dist(self, X, X2):
   559   1181.0 MiB      0.0 MiB               pi = np.pi / 180
   560   1181.0 MiB      0.0 MiB               f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   561   1181.0 MiB      0.0 MiB               f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   562   2267.3 MiB   1086.3 MiB               d = tf.sin((f - f2) / 2)**2
   563   2267.3 MiB      0.0 MiB               lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   564   2267.3 MiB      0.0 MiB                           tf.expand_dims(X2[:, 0] * pi, -2)
   565   2810.4 MiB    543.1 MiB               cos_prod = tf.cos(lat2) * tf.cos(lat1)
   566   3352.7 MiB    542.3 MiB               a = d[:, :, 0] + cos_prod * d[:, :, 1]
   567   3895.8 MiB    543.1 MiB               c = tf.asin(tf.sqrt(a)) * 6371 * 2
   568   3895.8 MiB      0.0 MiB               return c
   569
   570   1181.0 MiB      0.2 MiB           def scaled_squared_euclid_dist(self, X, X2):
   571                                         """
   572                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   573                                         """
   574   1181.0 MiB      0.0 MiB               if X2 is None:
   575    796.1 MiB      0.0 MiB                   X2 = X
   576   1723.3 MiB      0.1 MiB               dist = tf.square(self.haversine_dist(X, X2) / self.lengthscale)
   577   1723.3 MiB      0.0 MiB               return dist
   578
   579
   580                                 ########### Section to Run GPR######################
   581                                 ##################################3#################
   582
   583                                 # Input space, rsl normalized to zero mean, unit variance
   584    586.2 MiB      0.0 MiB       X = np.stack((df_place['lon'], df_place['lat'], df_place['age']), 1)
   585    586.2 MiB      0.0 MiB       RSL = normalize(df_place.rsl_realresid)
   586
   587                                 #define kernels  with bounds
   588
   589    592.0 MiB      0.0 MiB       k1 = HaversineKernel_Matern32(active_dims=[0, 1])
   590    592.2 MiB      0.0 MiB       k1.lengthscale = bounded_parameter(5000, 30000, 10000)  #hemispheric space
   591    592.2 MiB      0.0 MiB       k1.variance = bounded_parameter(0.1, 100, 2)
   592
   593    592.2 MiB      0.0 MiB       k2 = HaversineKernel_Matern32(active_dims=[0, 1])
   594    592.2 MiB      0.0 MiB       k2.lengthscale = bounded_parameter(10, 5000, 100)  #GIA space
   595    592.2 MiB      0.0 MiB       k2.variance = bounded_parameter(0.1, 100, 2)
   596
   597    592.2 MiB      0.0 MiB       k3 = gpf.kernels.Matern32(active_dims=[2])  #GIA time
   598    592.2 MiB      0.0 MiB       k3.lengthscale = bounded_parameter(8000, 20000, 10000)
   599    592.2 MiB      0.0 MiB       k3.variance = bounded_parameter(0.1, 100, 1)
   600
   601    592.2 MiB      0.0 MiB       k4 = gpf.kernels.Matern32(active_dims=[2])  #shorter time
   602    592.3 MiB      0.0 MiB       k4.lengthscale = bounded_parameter(1, 8000, 1000)
   603    592.3 MiB      0.0 MiB       k4.variance = bounded_parameter(0.1, 100, 1)
   604
   605    592.3 MiB      0.0 MiB       k5 = gpf.kernels.White(active_dims=[2])
   606    592.3 MiB      0.0 MiB       k5.variance = bounded_parameter(0.1, 100, 1)
   607
   608    592.3 MiB      0.0 MiB       kernel = (k1 * k3) + (k2 * k4) + k5
   609
   610                                 #build & train model
   611    592.3 MiB      0.0 MiB       m = GPR_diag((X, RSL), kernel=kernel, likelihood=likelihood)
   612    592.3 MiB      0.0 MiB       print('model built, time=', time.time() - start)
   613
   614
   615    595.5 MiB      3.2 MiB       @tf.function(autograph=False)
   616                                 def objective():
   617    596.9 MiB      0.0 MiB           return -m.log_marginal_likelihood()
   618
   619
   620    592.3 MiB      0.0 MiB       o = gpf.optimizers.Scipy()
   621    718.0 MiB    121.0 MiB       o.minimize(objective, variables=m.trainable_variables)
   622    718.0 MiB      0.0 MiB       print('model minimized, time=', time.time() - start)
   623
   624                                 # output space
   625    718.0 MiB      0.0 MiB       nout = 50
   626    718.0 MiB      0.0 MiB       lat = np.linspace(min(ds_area.lat), max(ds_area.lat), nout)
   627    718.0 MiB      0.0 MiB       lon = np.linspace(min(ds_area.lon), max(ds_area.lon), nout)
   628    718.0 MiB      0.0 MiB       ages = ages_lgm[(ages_lgm < tmax) & (ages_lgm > tmin)]
   629    722.2 MiB      4.2 MiB       xyt = np.array(list(product(lon, lat, ages)))
   630
   631                                 #query model & renormalize data
   632    712.9 MiB      0.0 MiB       y_pred, var = m.predict_f(xyt)
   633    712.9 MiB      0.0 MiB       y_pred_out = denormalize(y_pred, df_place.rsl_realresid)
   634
   635                                 #reshape output vectors
   636    712.9 MiB      0.0 MiB       Xlon = np.array(xyt[:, 0]).reshape((nout, nout, len(ages)))
   637    712.9 MiB      0.0 MiB       Xlat = np.array(xyt[:, 1]).reshape((nout, nout, len(ages)))
   638    713.6 MiB      0.7 MiB       Zp = np.array(y_pred_out).reshape(nout, nout, len(ages))
   639    714.2 MiB      0.7 MiB       varp = np.array(var).reshape(nout, nout, len(ages))
   640
   641                                 #print kernel details
   642    714.3 MiB      0.1 MiB       print_summary(m, fmt='notebook')
   643    714.3 MiB      0.0 MiB       print('time elapsed = ', time.time() - start)
   644
   645    714.3 MiB      0.0 MiB       print('negative log marginal likelihood =',
   646    739.2 MiB      0.0 MiB             m.neg_log_marginal_likelihood().numpy())
   647
   648                                 ##################	  INTERPOLATE MODELS 	#######################
   649                                 ##################  --------------------	 ######################
   650
   651                                 # turn GPR output into xarray dataarray
   652    739.2 MiB      0.0 MiB       da_zp = xr.DataArray(Zp, coords=[lon, lat, ages],
   653    739.2 MiB      0.0 MiB                            dims=['lon', 'lat',
   654    739.2 MiB      0.0 MiB                                  'age']).transpose('age', 'lat', 'lon')
   655    739.2 MiB      0.0 MiB       da_varp = xr.DataArray(varp,
   656    739.2 MiB      0.0 MiB                              coords=[lon, lat, ages],
   657    739.2 MiB      0.0 MiB                              dims=['lon', 'lat',
   658    739.2 MiB      0.0 MiB                                    'age']).transpose('age', 'lat', 'lon')
   659
   660
   661    739.2 MiB      0.0 MiB       def interp_likegpr(ds):
   662    645.0 MiB      0.0 MiB           return ds.rsl.load().transpose().interp_like(da_zp)
   663
   664
   665                                 #interpolate all models onto GPR grid
   666    640.1 MiB      0.0 MiB       da_trueinterp = interp_likegpr(ds_true)
   667    640.1 MiB      0.0 MiB       ds_trueinterp = ds_true.interp(age=ages)
   668    640.9 MiB      0.0 MiB       da_priorinterp = interp_likegpr(ds_prior)
   669    640.9 MiB      0.0 MiB       ds_priorinterp = ds_prior.interp(age=ages)
   670    641.8 MiB      0.0 MiB       da_priorinterpstd = interp_likegpr(ds_priorstd)
   671    642.5 MiB      0.0 MiB       da_giapriorinterp = interp_likegpr(ds_area)
   672    642.5 MiB      0.0 MiB       ds_giapriorinterp = ds_area.interp(age=ages)
   673    643.6 MiB      0.0 MiB       da_giapriorinterpstd = interp_likegpr(ds_areastd)
   674    644.3 MiB      0.0 MiB       da_readvpriorinterp = interp_likegpr(ds_readvprior)
   675    645.0 MiB      0.0 MiB       da_readvpriorinterpstd = interp_likegpr(ds_readvprior_std)
   676
   677                                 # add total prior RSL back into GPR
   678    645.0 MiB      0.0 MiB       da_priorplusgpr = da_zp + da_giapriorinterp
   679
   680                                 ##################	  	 SAVE NETCDFS 	 	#######################
   681                                 ##################  --------------------	 ######################
   682
   683    645.0 MiB      0.0 MiB       path = 'output/'
   684    645.5 MiB      0.6 MiB       da_zp.to_netcdf(path + ice_model + lith_thickness + '_' + place + '_da_zp')
   685    645.5 MiB      0.0 MiB       da_giapriorinterp.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   686    645.6 MiB      0.0 MiB                                   '_giaprior')
   687    645.6 MiB      0.0 MiB       da_priorplusgpr.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   688    645.6 MiB      0.0 MiB                                 '_posterior')
   689    645.6 MiB      0.0 MiB       da_varp.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   690    645.6 MiB      0.0 MiB                         '_gp_variance')
   691
   692                                 ##################		  PLOT  MODELS 		#######################
   693                                 ##################  --------------------	 ######################
   694
   695    645.6 MiB      0.0 MiB       dirName = f'figs/{place}/'
   696    645.6 MiB      0.0 MiB       if not os.path.exists(dirName):
   697                                     os.mkdir(dirName)
   698                                     print("Directory ", dirName, " Created ")
   699                                 else:
   700    645.6 MiB      0.0 MiB           print("Directory ", dirName, " already exists")
   701
   702    658.5 MiB      0.0 MiB       for i, age in enumerate(ages):
   703    655.7 MiB      0.0 MiB           if (age / 500).is_integer():
   704    655.7 MiB      0.0 MiB               step = (ages[0] - ages[1])
   705    655.7 MiB      0.0 MiB               df_it = df_place[(df_place.age < age) & (df_place.age > age - step)]
   706    655.7 MiB      0.0 MiB               resid_it = da_zp.sel(age=slice(age, age - step))
   707    655.7 MiB      0.0 MiB               rsl, var = df_it.rsl, df_it.rsl_er_max.values**2
   708    655.7 MiB      0.0 MiB               lat_it, lon_it = df_it.lat, df_it.lon
   709    655.7 MiB      0.0 MiB               vmin = ds_giapriorinterp.rsl.min().values  # + 10
   710    655.7 MiB      0.0 MiB               vmax = ds_giapriorinterp.rsl.max().values  # - 40
   711    655.7 MiB      0.0 MiB               vmin_std = 0
   712    655.7 MiB      0.0 MiB               vmax_std = 1
   713    655.7 MiB      0.0 MiB               tmin_it = np.round(age - step, 2)
   714    655.7 MiB      0.0 MiB               tmax_it = np.round(age, 2)
   715    655.7 MiB      0.0 MiB               cbarscale = 0.3
   716    655.7 MiB      0.0 MiB               fontsize = 20
   717    655.7 MiB      0.0 MiB               cmap = 'coolwarm'
   718    655.7 MiB      0.0 MiB               cbar_kwargs = {'shrink': cbarscale, 'label': 'RSL (m)'}
   719
   720    655.7 MiB      0.0 MiB               proj = ccrs.PlateCarree()
   721    655.7 MiB      0.0 MiB               projection = ccrs.PlateCarree()
   722                                         fig, (ax1, ax2, ax3,
   723    655.7 MiB      0.0 MiB                     ax4) = plt.subplots(1,
   724    655.7 MiB      0.0 MiB                                         4,
   725    655.7 MiB      0.0 MiB                                         figsize=(24, 16),
   726    656.8 MiB      1.1 MiB                                         subplot_kw=dict(projection=projection))
   727
   728                                         # total prior mean + "true" data
   729    656.8 MiB      0.0 MiB               ax1.coastlines(color='k')
   730    656.8 MiB      0.0 MiB               pc1 = ds_giapriorinterp.rsl[i].transpose().plot(ax=ax1,
   731    656.8 MiB      0.0 MiB                                                               transform=proj,
   732    656.8 MiB      0.0 MiB                                                               cmap=cmap,
   733    656.8 MiB      0.0 MiB                                                               norm=MidpointNormalize(
   734    656.8 MiB      0.0 MiB                                                                   vmin, vmax, 0),
   735    656.8 MiB      0.0 MiB                                                               add_colorbar=False,
   736    656.9 MiB      0.3 MiB                                                               extend='both')
   737    656.9 MiB      0.0 MiB               cbar = fig.colorbar(pc1,
   738    656.9 MiB      0.0 MiB                                   ax=ax1,
   739    656.9 MiB      0.0 MiB                                   shrink=.3,
   740    656.9 MiB      0.0 MiB                                   label='RSL (m)',
   741    657.3 MiB      0.1 MiB                                   extend='both')
   742    657.3 MiB      0.0 MiB               scat = ax1.scatter(lon_it,
   743    657.3 MiB      0.0 MiB                                  lat_it,
   744    657.3 MiB      0.0 MiB                                  s=80,
   745    657.3 MiB      0.0 MiB                                  c=rsl,
   746    657.3 MiB      0.0 MiB                                  edgecolor='k',
   747    657.3 MiB      0.0 MiB                                  vmin=vmin,
   748    657.3 MiB      0.0 MiB                                  vmax=vmax,
   749    657.3 MiB      0.0 MiB                                  norm=MidpointNormalize(vmin, vmax, 0),
   750    657.3 MiB      0.1 MiB                                  cmap=cmap)
   751    657.3 MiB      0.0 MiB               ax1.set_title(f'{np.round(ds_trueinterp.rsl[i].age.values, -1)} yrs',
   752    657.3 MiB      0.0 MiB                             fontsize=fontsize)
   753                                         #         ax1.set_extent(extent_)
   754
   755                                         # Learned difference between prior and "true" data
   756    657.3 MiB      0.0 MiB               ax2.coastlines(color='k')
   757    657.3 MiB      0.0 MiB               pc = da_zp[i, :, :].plot(ax=ax2,
   758    657.3 MiB      0.0 MiB                                        transform=proj,
   759    657.3 MiB      0.0 MiB                                        cmap=cmap,
   760    657.3 MiB      0.0 MiB                                        extend='both',
   761    657.3 MiB      0.0 MiB                                        norm=MidpointNormalize(
   762    657.3 MiB      0.0 MiB                                            resid_it.min(), resid_it.max(), 0),
   763    657.3 MiB      0.2 MiB                                        add_colorbar=False)
   764    657.3 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   765    657.3 MiB      0.0 MiB                                   ax=ax2,
   766    657.3 MiB      0.0 MiB                                   shrink=.3,
   767    657.3 MiB      0.0 MiB                                   label='RSL (m)',
   768    657.6 MiB      0.1 MiB                                   extend='both')
   769    657.6 MiB      0.0 MiB               scat = ax2.scatter(lon_it,
   770    657.6 MiB      0.0 MiB                                  lat_it,
   771    657.6 MiB      0.0 MiB                                  s=80,
   772    657.6 MiB      0.0 MiB                                  facecolors='k',
   773    657.6 MiB      0.0 MiB                                  cmap=cmap,
   774    657.6 MiB      0.0 MiB                                  edgecolor='k',
   775    657.6 MiB      0.0 MiB                                  transform=proj,
   776    657.6 MiB      0.0 MiB                                  norm=MidpointNormalize(resid_it.min(),
   777    657.6 MiB      0.0 MiB                                                         resid_it.max(), 0))
   778    657.6 MiB      0.0 MiB               ax2.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   779                                         #         ax2.set_extent(extent_)
   780
   781                                         # GP regression
   782    657.6 MiB      0.0 MiB               ax3.coastlines(color='k')
   783    657.6 MiB      0.0 MiB               pc = da_priorplusgpr[i].plot(ax=ax3,
   784    657.6 MiB      0.0 MiB                                            transform=proj,
   785    657.6 MiB      0.0 MiB                                            norm=MidpointNormalize(vmin, vmax, 0),
   786    657.6 MiB      0.0 MiB                                            cmap=cmap,
   787    657.6 MiB      0.0 MiB                                            extend='both',
   788    657.6 MiB      0.1 MiB                                            add_colorbar=False)
   789    657.6 MiB      0.0 MiB               scat = ax3.scatter(lon_it,
   790    657.6 MiB      0.0 MiB                                  lat_it,
   791    657.6 MiB      0.0 MiB                                  s=80,
   792    657.6 MiB      0.0 MiB                                  c=rsl,
   793    657.6 MiB      0.0 MiB                                  edgecolor='k',
   794    657.6 MiB      0.0 MiB                                  cmap=cmap,
   795    657.7 MiB      0.0 MiB                                  norm=MidpointNormalize(vmin, vmax, 0))
   796    657.7 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   797    657.7 MiB      0.0 MiB                                   ax=ax3,
   798    657.7 MiB      0.0 MiB                                   shrink=.3,
   799    657.7 MiB      0.0 MiB                                   label='RSL (m)',
   800    657.9 MiB      0.1 MiB                                   extend='both')
   801    657.9 MiB      0.0 MiB               ax3.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   802                                         #         ax3.set_extent(extent_)
   803
   804                                         #GP regression standard deviation
   805    658.0 MiB      0.0 MiB               ax4.coastlines(color='k')
   806    658.0 MiB      0.0 MiB               pc = (2 * np.sqrt(da_varp[i])).plot(
   807    658.0 MiB      0.0 MiB                   ax=ax4,
   808    658.0 MiB      0.0 MiB                   transform=proj,
   809    658.0 MiB      0.0 MiB                   vmin=vmin_std,
   810    658.0 MiB      0.0 MiB                   vmax=vmax_std * 2,
   811    658.0 MiB      0.0 MiB                   cmap='Reds',
   812    658.0 MiB      0.0 MiB                   extend='both',
   813    658.2 MiB      0.3 MiB                   add_colorbar=False,
   814                                         )
   815    658.2 MiB      0.0 MiB               scat = ax4.scatter(lon_it,
   816    658.2 MiB      0.0 MiB                                  lat_it,
   817    658.2 MiB      0.0 MiB                                  s=80,
   818    658.2 MiB      0.0 MiB                                  c=2 * np.sqrt(var),
   819    658.2 MiB      0.0 MiB                                  vmin=vmin_std,
   820    658.2 MiB      0.0 MiB                                  vmax=vmax_std * 2,
   821    658.2 MiB      0.0 MiB                                  cmap='Reds',
   822    658.2 MiB      0.0 MiB                                  edgecolor='k',
   823    658.3 MiB      0.0 MiB                                  transform=proj)
   824    658.3 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   825    658.3 MiB      0.0 MiB                                   ax=ax4,
   826    658.3 MiB      0.0 MiB                                   shrink=.3,
   827    658.3 MiB      0.0 MiB                                   extend='both',
   828    658.5 MiB      0.3 MiB                                   label='RSL (m) (2 $\sigma$)')
   829    658.5 MiB      0.0 MiB               ax4.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   830                                 #         ax4.set_extent(extent_)
   831
   832                                 ########## ----- Save figures -------- #######################
   833    680.6 MiB      1.6 MiB       fig.savefig(dirName + f'{ages[i]}_{place}_realdata_fig_3D', transparent=True)
   834
   835                                 ##################	CHOOSE LOCS W/NUF SAMPS #######################
   836                                 ##################  --------------------	 ######################
   837
   838
   839    680.6 MiB      0.0 MiB       def locs_with_enoughsamples(df_place, place, number):
   840                                     """make new dataframe, labeled, of sites with [> number] measurements"""
   841    680.6 MiB      0.0 MiB           df_lots = df_place.groupby(['lat',
   842    680.6 MiB      0.0 MiB                                       'lon']).filter(lambda x: len(x) > number)
   843
   844    680.6 MiB      0.0 MiB           df_locs = []
   845    680.6 MiB      0.0 MiB           for i, group in enumerate(df_lots.groupby(['lat', 'lon'])):
   846    680.6 MiB      0.0 MiB               singleloc = group[1].copy()
   847    680.6 MiB      0.0 MiB               singleloc['location'] = place
   848    680.6 MiB      0.0 MiB               singleloc['locnum'] = place + '_site' + str(
   849    680.6 MiB      0.0 MiB                   i)  # + singleloc.reset_index().index.astype('str')
   850    680.6 MiB      0.0 MiB               df_locs.append(singleloc)
   851    680.6 MiB      0.0 MiB           df_locs = pd.concat(df_locs)
   852
   853    680.6 MiB      0.0 MiB           return df_locs
   854
   855
   856    680.6 MiB      0.0 MiB       number = 6
   857    680.6 MiB      0.0 MiB       df_nufsamps = locs_with_enoughsamples(df_place, place, number)
   858    680.6 MiB      0.0 MiB       len(df_nufsamps.locnum.unique())
   859
   860                                 ##################	PLOT LOCS W/NUF SAMPS   #######################
   861                                 ##################  --------------------	 ######################
   862
   863
   864    684.2 MiB      0.0 MiB       def slice_dataarray(da):
   865    684.2 MiB      0.0 MiB           return da.sel(lat=site[1].lat.unique(),
   866    684.2 MiB      0.0 MiB                         lon=site[1].lon.unique(),
   867    684.2 MiB      0.0 MiB                         method='nearest')
   868
   869
   870    682.6 MiB      2.0 MiB       fig, ax = plt.subplots(1, len(df_nufsamps.locnum.unique()), figsize=(18, 4))
   871    682.6 MiB      0.0 MiB       ax = ax.ravel()
   872    682.6 MiB      0.0 MiB       colors = ['darkgreen', 'darkblue', 'darkred']
   873    682.6 MiB      0.0 MiB       fontsize = 18
   874
   875    684.4 MiB      0.0 MiB       for i, site in enumerate(df_nufsamps.groupby('locnum')):
   876
   877                                     #slice data for each site
   878    684.2 MiB      0.0 MiB           prior_it = slice_dataarray(da_priorinterp)
   879    684.2 MiB      0.0 MiB           priorvar_it = slice_dataarray(da_priorinterpstd)
   880    684.2 MiB      0.0 MiB           top_prior = prior_it + priorvar_it * 2
   881    684.2 MiB      0.0 MiB           bottom_prior = prior_it - priorvar_it * 2
   882
   883    684.2 MiB      0.0 MiB           var_it = slice_dataarray(np.sqrt(da_varp))
   884    684.2 MiB      0.0 MiB           post_it = slice_dataarray(da_priorplusgpr)
   885    684.2 MiB      0.0 MiB           top = post_it + var_it * 2
   886    684.2 MiB      0.0 MiB           bottom = post_it - var_it * 2
   887
   888    684.2 MiB      0.0 MiB           site_err = 2 * (site[1].rsl_er_max + site[1].rsl_giaprior_std)
   889
   890    684.2 MiB      0.0 MiB           ax[i].scatter(site[1].age, site[1].rsl, c=colors[0], label='"true" RSL')
   891    684.2 MiB      0.0 MiB           ax[i].errorbar(
   892    684.2 MiB      0.0 MiB               site[1].age,
   893    684.2 MiB      0.0 MiB               site[1].rsl,
   894    684.2 MiB      0.0 MiB               site_err,
   895    684.2 MiB      0.0 MiB               c=colors[0],
   896    684.2 MiB      0.0 MiB               fmt='none',
   897    684.2 MiB      0.0 MiB               capsize=1,
   898    684.3 MiB      0.0 MiB               lw=1,
   899                                     )
   900
   901    684.3 MiB      0.0 MiB           prior_it.plot(ax=ax[i], c=colors[2], label='Prior $\pm 2 \sigma$')
   902    684.3 MiB      0.0 MiB           ax[i].fill_between(prior_it.age,
   903    684.3 MiB      0.0 MiB                              bottom_prior.squeeze(),
   904    684.3 MiB      0.0 MiB                              top_prior.squeeze(),
   905    684.3 MiB      0.0 MiB                              color=colors[2],
   906    684.3 MiB      0.0 MiB                              alpha=0.3)
   907
   908    684.3 MiB      0.0 MiB           post_it.plot(ax=ax[i], c=colors[1], label='Posterior $\pm 2 \sigma$')
   909    684.3 MiB      0.0 MiB           ax[i].fill_between(post_it.age,
   910    684.3 MiB      0.0 MiB                              bottom.squeeze(),
   911    684.3 MiB      0.0 MiB                              top.squeeze(),
   912    684.3 MiB      0.0 MiB                              color=colors[1],
   913    684.3 MiB      0.0 MiB                              alpha=0.3)
   914                                     #     ax[i].set_title(f'{site[0]} RSL', fontsize=fontsize)
   915    684.3 MiB      0.0 MiB           ax[i].set_title('')
   916
   917    684.4 MiB      0.0 MiB           ax[i].legend(loc='lower left')
   918
   919    684.4 MiB      0.0 MiB       path = 'figs/{place}'
   920    684.4 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_realdata_fig_1D',
   921    689.7 MiB      5.3 MiB                   transparent=True)
   922
   923                                 #plot locations of data
   924    689.7 MiB      0.0 MiB       fig, ax = plt.subplots(1,
   925    689.7 MiB      0.0 MiB                              len(df_nufsamps.locnum.unique()),
   926    689.7 MiB      0.0 MiB                              figsize=(18, 4),
   927    692.4 MiB      2.7 MiB                              subplot_kw=dict(projection=projection))
   928    692.4 MiB      0.0 MiB       ax = ax.ravel()
   929
   930    692.4 MiB      0.0 MiB       da_zeros = xr.zeros_like(da_zp)
   931
   932    693.8 MiB      0.0 MiB       for i, site in enumerate(df_nufsamps.groupby('locnum')):
   933    693.7 MiB      0.0 MiB           ax[i].coastlines(color='k')
   934    693.7 MiB      0.0 MiB           ax[i].plot(site[1].lon.unique(),
   935    693.7 MiB      0.0 MiB                      site[1].lat.unique(),
   936    693.7 MiB      0.0 MiB                      c=colors[0],
   937    693.7 MiB      0.0 MiB                      ms=7,
   938    693.7 MiB      0.0 MiB                      marker='o',
   939    693.7 MiB      0.0 MiB                      transform=proj)
   940    693.7 MiB      0.0 MiB           ax[i].plot(site[1].lon.unique(),
   941    693.7 MiB      0.0 MiB                      site[1].lat.unique(),
   942    693.7 MiB      0.0 MiB                      c=colors[0],
   943    693.7 MiB      0.0 MiB                      ms=25,
   944    693.7 MiB      0.0 MiB                      marker='o',
   945    693.7 MiB      0.0 MiB                      transform=proj,
   946    693.7 MiB      0.0 MiB                      mfc="None",
   947    693.7 MiB      0.0 MiB                      mec='red',
   948    693.8 MiB      0.0 MiB                      mew=4)
   949    693.8 MiB      0.2 MiB           da_zeros[0].plot(ax=ax[i], cmap='Greys', add_colorbar=False)
   950    693.8 MiB      0.0 MiB           ax[i].set_title(site[0], fontsize=fontsize)
   951                                 # plt.tight_layout()
   952    693.8 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_realdata_fig_1Dlocs',
   953    698.1 MiB      4.2 MiB                   transparent=True)
   954
   955                                 #################   DECOMPOSE GPR INTO KERNELS ####################
   956                                 ##################  --------------------	 ######################
   957
   958
   959    796.1 MiB      0.0 MiB       def predict_decomp_f(m,
   960                                                      custom_kernel,
   961                                                      predict_at: tf.Tensor,
   962                                                      full_cov: bool = False,
   963                                                      full_output_cov: bool = False,
   964    698.1 MiB      0.0 MiB                            var=None):
   965                                     """Decompose GP into individual kernels."""
   966
   967    796.1 MiB      0.0 MiB           x_data, y_data = m.data
   968    796.1 MiB      0.0 MiB           err = y_data - m.mean_function(x_data)
   969    796.1 MiB      0.0 MiB           kmm = m.kernel(x_data)
   970    796.1 MiB      0.0 MiB           knn = custom_kernel(predict_at, full=full_cov)
   971   1337.8 MiB    530.4 MiB           kmn = custom_kernel(x_data, predict_at)
   972   1337.8 MiB      0.0 MiB           num_data = x_data.shape[0]
   973   1337.8 MiB      0.0 MiB           s = tf.linalg.diag(tf.convert_to_tensor(var))  # added diagonal variance
   974   1337.8 MiB      0.0 MiB           conditional = gpf.conditionals.base_conditional
   975   1337.8 MiB      0.0 MiB           f_mean_zero, f_var = conditional(
   976   1337.8 MiB      0.0 MiB               kmn, kmm + s, knn, err, full_cov=full_cov,
   977   1339.9 MiB     55.0 MiB               white=False)  # [N, P], [N, P] or [P, N, N]
   978   1339.9 MiB      0.0 MiB           f_mean = np.array(f_mean_zero + m.mean_function(predict_at))
   979   1339.9 MiB      0.0 MiB           f_var = np.array(f_var)
   980   1339.9 MiB      0.0 MiB           return f_mean, f_var
   981
   982
   983    796.1 MiB      0.0 MiB       def reshape_decomp(k, var=None):
   984    796.1 MiB      0.0 MiB           A, var = predict_decomp_f(m, k, xyt, var=var)
   985    796.1 MiB      0.0 MiB           A = A.reshape(nout, nout, len(ages))
   986    796.1 MiB      0.0 MiB           var = var.reshape(nout, nout, len(ages))
   987    796.1 MiB      0.0 MiB           return A, var
   988
   989
   990    757.8 MiB      0.0 MiB       def make_dataarray(da):
   991    757.8 MiB      0.0 MiB           coords = [lon, lat, ages]
   992    757.8 MiB      0.0 MiB           dims = ['lon', 'lat', 'age']
   993    757.8 MiB      0.0 MiB           return xr.DataArray(da, coords=coords,
   994    757.8 MiB      0.5 MiB                               dims=dims).transpose('age', 'lat', 'lon')
   995
   996
   997    698.1 MiB      0.0 MiB       A1, var1 = reshape_decomp(k1,
   998    698.1 MiB      0.0 MiB                                 var=df_place.rsl_er_max.ravel()**2 +
   999    794.7 MiB      0.0 MiB                                 df_place.rsl_giaprior_std.ravel()**2)  #gia spatial
  1000    794.7 MiB      0.0 MiB       A2, var2 = reshape_decomp(k2,
  1001    794.7 MiB      0.0 MiB                                 var=df_place.rsl_er_max.ravel()**2 +
  1002    796.1 MiB      0.0 MiB                                 df_place.rsl_giaprior_std.ravel()**2)  #gia temporal
  1003    796.1 MiB      0.0 MiB       A3, var3 = reshape_decomp(
  1004    796.1 MiB      0.0 MiB           k3,
  1005    796.1 MiB      0.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1006    784.7 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance spatial
  1007    784.7 MiB      0.0 MiB       A4, var4 = reshape_decomp(
  1008    784.7 MiB      0.0 MiB           k4,
  1009    784.7 MiB      0.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1010    736.7 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance temporal
  1011    736.7 MiB      0.0 MiB       A5, var5 = reshape_decomp(
  1012    736.7 MiB      0.0 MiB           k5,
  1013    736.7 MiB      0.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1014    757.3 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance spatial
  1015
  1016    757.3 MiB      0.0 MiB       da_A1 = make_dataarray(A1)
  1017    757.3 MiB      0.0 MiB       da_var1 = make_dataarray(var1)
  1018
  1019    757.3 MiB      0.0 MiB       da_A2 = make_dataarray(A2)
  1020    757.3 MiB      0.0 MiB       da_var2 = make_dataarray(var2)
  1021
  1022    757.3 MiB      0.0 MiB       da_A3 = make_dataarray(A3)
  1023    757.3 MiB      0.0 MiB       da_var3 = make_dataarray(var3)
  1024
  1025    757.8 MiB      0.0 MiB       da_A4 = make_dataarray(A4)
  1026    757.8 MiB      0.0 MiB       da_var4 = make_dataarray(var4)
  1027
  1028    757.8 MiB      0.0 MiB       da_A5 = make_dataarray(A5)
  1029    757.8 MiB      0.0 MiB       da_var5 = make_dataarray(var5)
  1030
  1031                                 #################   PLOT DECOMPOSED KERNELS    ####################
  1032                                 ##################  --------------------	   ####################
  1033
  1034    759.3 MiB      1.4 MiB       fig, ax = plt.subplots(1, 6, figsize=(24, 4))
  1035    759.3 MiB      0.0 MiB       ax = ax.ravel()
  1036    759.7 MiB      0.5 MiB       da_A1[0, :, :].plot(ax=ax[0], cmap='RdBu_r')
  1037
  1038    760.8 MiB      1.1 MiB       da_A2[0, :, :].plot(ax=ax[1], cmap='RdBu_r')
  1039
  1040    761.2 MiB      0.4 MiB       da_A3[0, :, :].plot(ax=ax[2], cmap='RdBu_r')
  1041
  1042    761.2 MiB      0.0 MiB       da_A4[:, 0, 0].plot(ax=ax[3])
  1043
  1044    761.2 MiB      0.0 MiB       da_A5[:, 0, 0].plot(ax=ax[4])
  1045
  1046                                 # da_A6[:,0,0].plot(ax=ax[5])
  1047
  1048                                 # plt.tight_layout()
  1049
  1050    761.2 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_decompkernels',
  1051    764.3 MiB      3.0 MiB                   transparent=True)


