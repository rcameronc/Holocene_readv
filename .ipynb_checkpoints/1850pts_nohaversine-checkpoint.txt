number of datapoints =  (1868, 14)
model built, time= 0.1566600799560547
model minimized, time= 6856.46938085556
<IPython.core.display.HTML object>
time elapsed =  7261.729846954346
negative log marginal likelihood = 821.6376173750732
Directory  figs/europe/  already exists
Filename: readv_021720_realdata_europe_glac1d.py

Line #    Mem usage    Increment   Line Contents
================================================
    40    294.5 MiB    294.5 MiB   @profile
    41                             def readv():
    42
    43                                 # set the colormap and centre the colorbar
    44    294.5 MiB      0.0 MiB       class MidpointNormalize(Normalize):
    45    294.5 MiB      0.0 MiB           """Normalise the colorbar.  e.g. norm=MidpointNormalize(mymin, mymax, 0.)"""
    46    294.5 MiB      0.0 MiB           def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
    47    221.5 MiB      0.0 MiB               self.midpoint = midpoint
    48    221.5 MiB      0.0 MiB               Normalize.__init__(self, vmin, vmax, clip)
    49
    50    294.5 MiB     18.5 MiB           def __call__(self, value, clip=None):
    51    243.6 MiB      0.0 MiB               x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
    52    243.6 MiB      0.1 MiB               return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))
    53
    54
    55                                 ####################  Initialize parameters #######################
    56                                 #################### ---------------------- #######################
    57
    58    294.5 MiB      0.0 MiB       ice_model = 'd6g_h6g_'  #'glac1d_'
    59    294.5 MiB      0.0 MiB       lith_thickness = 'l7'  # 'l90C'
    60    294.5 MiB      0.0 MiB       place = 'europe'
    61
    62                                 locs = {
    63    294.5 MiB      0.0 MiB           'england': [-12, 2, 50, 60],
    64    294.5 MiB      0.0 MiB           'southchina': [110, 117, 19, 23],
    65    294.5 MiB      0.0 MiB           'easternhem': [50, 178, -45, 80],
    66    294.5 MiB      0.0 MiB           'westernhem': [-175, 30, -80, 75],
    67    294.5 MiB      0.0 MiB           'world': [-179.8, 179.8, -89.8, 89.8],
    68    294.5 MiB      0.0 MiB           'namerica': [-150, -20, 10, 75],
    69    294.5 MiB      0.0 MiB           'eastcoast': [-88, -65, 15, 40],
    70    294.5 MiB      0.0 MiB           'europe': [-20, 15, 35, 70]
    71                                 }
    72    294.5 MiB      0.0 MiB       extent = locs[place]
    73    294.5 MiB      0.0 MiB       tmax, tmin, tstep = 10050, 450, 100
    74
    75    294.5 MiB      0.0 MiB       ages_lgm = np.arange(100, 26000, tstep)[::-1]
    76
    77                                 #import khan dataset
    78    294.5 MiB      0.0 MiB       path = 'data/GSL_LGM_120519_.csv'
    79
    80    312.9 MiB     18.5 MiB       df = pd.read_csv(path, encoding="ISO-8859-15", engine='python')
    81    318.1 MiB      5.1 MiB       df = df.replace('\s+', '_', regex=True).replace('-', '_', regex=True).\
    82    321.0 MiB      0.4 MiB               applymap(lambda s:s.lower() if type(s) == str else s)
    83    321.1 MiB      0.0 MiB       df.columns = df.columns.str.lower()
    84    321.1 MiB      0.0 MiB       df.rename_axis('index', inplace=True)
    85    321.1 MiB      0.1 MiB       df = df.rename({'latitude': 'lat', 'longitude': 'lon'}, axis='columns')
    86    321.2 MiB      0.0 MiB       dfind, dfterr, dfmar = df[(df.type == 0)
    87    321.3 MiB      0.2 MiB                                 & (df.age > 0)], df[df.type == 1], df[df.type == -1]
    88    321.3 MiB      0.0 MiB       np.sort(list(set(dfind.regionname1)))
    89
    90                                 #select location
    91    321.3 MiB      0.0 MiB       df_place = dfind[(dfind.age > tmin) & (dfind.age < tmax) &
    92                                                  (dfind.lon > extent[0])
    93                                                  & (dfind.lon < extent[1])
    94                                                  & (dfind.lat > extent[2])
    95                                                  & (dfind.lat < extent[3])
    96    321.5 MiB      0.2 MiB                        & (dfind.rsl_er_max < 1)][[
    97    321.5 MiB      0.0 MiB                            'lat', 'lon', 'rsl', 'rsl_er_max', 'age'
    98                                                  ]]
    99                                 # & (df_place.rsl_er_max < 1)
   100    321.5 MiB      0.0 MiB       df_place.shape
   101
   102                                 ####################  	Plot locations  	#######################
   103                                 #################### ---------------------- #######################
   104
   105                                 #get counts by location rounded to nearest 0.1 degree
   106    321.5 MiB      0.0 MiB       df_rnd = df_place.copy()
   107    321.5 MiB      0.0 MiB       df_rnd.lat = np.round(df_rnd.lat, 1)
   108    321.5 MiB      0.0 MiB       df_rnd.lon = np.round(df_rnd.lon, 1)
   109    321.5 MiB      0.0 MiB       dfcounts_place = df_rnd.groupby(
   110    321.7 MiB      0.2 MiB           ['lat', 'lon']).count().reset_index()[['lat', 'lon', 'rsl', 'age']]
   111
   112                                 #plot
   113    327.5 MiB      5.8 MiB       fig = plt.figure(figsize=(10, 7))
   114    328.2 MiB      0.7 MiB       ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())
   115
   116    328.4 MiB      0.2 MiB       ax.set_extent(extent)
   117    328.4 MiB      0.0 MiB       ax.coastlines(resolution='110m', linewidth=1, zorder=2)
   118    328.4 MiB      0.0 MiB       ax.add_feature(cfeature.OCEAN, zorder=0)
   119    328.4 MiB      0.0 MiB       ax.add_feature(cfeature.LAND, color='palegreen', zorder=1)
   120    328.4 MiB      0.0 MiB       ax.add_feature(cfeature.BORDERS, linewidth=0.5, zorder=3)
   121    328.4 MiB      0.0 MiB       ax.gridlines(linewidth=1, color='white', alpha=0.5, zorder=4)
   122    328.4 MiB      0.0 MiB       scat = ax.scatter(dfcounts_place.lon,
   123    328.4 MiB      0.0 MiB                         dfcounts_place.lat,
   124    328.4 MiB      0.0 MiB                         s=dfcounts_place.rsl * 70,
   125    328.4 MiB      0.0 MiB                         c='lightsalmon',
   126    328.4 MiB      0.0 MiB                         vmin=-20,
   127    328.4 MiB      0.0 MiB                         vmax=20,
   128    328.4 MiB      0.0 MiB                         cmap='coolwarm',
   129    328.4 MiB      0.0 MiB                         edgecolor='k',
   130    328.4 MiB      0.0 MiB                         linewidths=1,
   131    328.4 MiB      0.0 MiB                         transform=ccrs.PlateCarree(),
   132    328.9 MiB      0.5 MiB                         zorder=5)
   133    328.9 MiB      0.0 MiB       size = Line2D(range(4),
   134    328.9 MiB      0.0 MiB                     range(4),
   135    328.9 MiB      0.0 MiB                     color="black",
   136    328.9 MiB      0.0 MiB                     marker='o',
   137    328.9 MiB      0.0 MiB                     linewidth=0,
   138    328.9 MiB      0.0 MiB                     linestyle='none',
   139    328.9 MiB      0.0 MiB                     markersize=16,
   140    328.9 MiB      0.0 MiB                     markerfacecolor="lightsalmon")
   141    328.9 MiB      0.0 MiB       labels = ['RSL datapoint location']
   142    328.9 MiB      0.0 MiB       leg = plt.legend([size],
   143    328.9 MiB      0.0 MiB                        labels,
   144    328.9 MiB      0.0 MiB                        loc='lower left',
   145    328.9 MiB      0.0 MiB                        bbox_to_anchor=(0.00, 0.00),
   146    328.9 MiB      0.0 MiB                        prop={'size': 20},
   147    328.9 MiB      0.0 MiB                        fancybox=True)
   148    328.9 MiB      0.0 MiB       leg.get_frame().set_edgecolor('k')
   149    328.9 MiB      0.0 MiB       ax.set_title('')
   150
   151                                 ####################  Make 3D fingerprint  #######################
   152                                 #################### ---------------------- #######################
   153
   154    328.9 MiB      0.0 MiB       filename = 'data/WAISreadvance_VM5_6ka_1step.mat'
   155
   156    327.9 MiB      0.0 MiB       waismask = io.loadmat(filename, squeeze_me=True)
   157    327.9 MiB      0.0 MiB       ds_mask = xr.Dataset({'rsl': (['lat', 'lon', 'age'], waismask['RSL'])},
   158                                                      coords={
   159    327.9 MiB      0.0 MiB                                'lon': waismask['lon_out'],
   160    327.9 MiB      0.0 MiB                                'lat': waismask['lat_out'],
   161    327.9 MiB      0.0 MiB                                'age': np.round(waismask['ice_time_new'])
   162                                                      })
   163    328.0 MiB      0.0 MiB       fingerprint = ds_mask.sel(age=ds_mask.age[0])
   164
   165
   166    601.3 MiB      0.0 MiB       def make_fingerprint(start, end, maxscale):
   167
   168                                     #palindromic scaling vector
   169    601.3 MiB      0.0 MiB           def palindrome(maxscale, ages):
   170                                         """ Make palindrome scale 0-maxval with number of steps. """
   171    601.3 MiB      0.0 MiB               half = np.linspace(0, maxscale, 1 + (len(ages) - 1) // 2)
   172    601.3 MiB      0.0 MiB               scalefactor = np.concatenate([half, half[::-1]])
   173    601.3 MiB      0.0 MiB               return scalefactor
   174
   175    601.3 MiB      0.0 MiB           ages_readv = ages_lgm[(ages_lgm < start) & (ages_lgm >= end)]
   176    601.3 MiB      0.0 MiB           scale = palindrome(maxscale, ages_readv)
   177
   178                                     #scale factor same size as ice model ages
   179    601.3 MiB      0.0 MiB           pre = np.zeros(np.where(ages_lgm == start)[0])
   180    601.3 MiB      0.0 MiB           post = np.zeros(len(ages_lgm) - len(pre) - len(scale))
   181
   182    601.3 MiB      0.0 MiB           readv_scale = np.concatenate([pre, scale, post])
   183
   184                                     #scale factor into dataarray
   185    601.3 MiB      0.0 MiB           da_scale = xr.DataArray(readv_scale, coords=[('age', ages_lgm)])
   186
   187                                     # broadcast fingerprint & scale to same dimensions;
   188    601.3 MiB      0.0 MiB           fingerprint_out, fing_scaled = xr.broadcast(fingerprint.rsl, da_scale)
   189
   190                                     # mask fingerprint with scale to get LGM-pres timeseries
   191    601.3 MiB      0.0 MiB           ds_fingerprint = (fingerprint_out *
   192    860.3 MiB    259.0 MiB                             fing_scaled).transpose().to_dataset(name='rsl')
   193
   194                                     # scale dataset with fingerprint to LGM-present length & 0-max-0 over x years
   195    860.3 MiB      0.0 MiB           xrlist = []
   196   1103.3 MiB      0.0 MiB           for i, key in enumerate(da_scale):
   197   1103.3 MiB      1.1 MiB               mask = ds_fingerprint.sel(age=ds_fingerprint.age[i].values) * key
   198   1103.3 MiB      0.0 MiB               mask = mask.assign_coords(scale=key,
   199   1103.3 MiB      0.0 MiB                                         age=ages_lgm[i]).expand_dims(dim=['age'])
   200   1103.3 MiB      0.0 MiB               xrlist.append(mask)
   201   1362.4 MiB    259.2 MiB           ds_readv = xr.concat(xrlist, dim='age')
   202
   203   1362.4 MiB      0.0 MiB           ds_readv.coords['lon'] = pd.DataFrame((ds_readv.lon[ds_readv.lon >= 180] - 360)- 0.12) \
   204   1362.4 MiB      0.0 MiB                                   .append(pd.DataFrame(ds_readv.lon[ds_readv.lon < 180]) + 0.58) \
   205   1362.4 MiB      0.0 MiB                                   .reset_index(drop=True).squeeze()
   206   1362.4 MiB      0.0 MiB           ds_readv = ds_readv.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   207
   208                                     # Add readv to modeled RSL at locations with data
   209                                     ##### Need to fix this, as currently slice does not acknowledge new coords #########
   210   1362.4 MiB      0.0 MiB           ds_readv = ds_readv.sel(age=slice(tmax, tmin),
   211   1362.4 MiB      0.0 MiB                                   lon=slice(df_place.lon.min() + 180 - 2,
   212   1362.4 MiB      0.0 MiB                                             df_place.lon.max() + 180 + 2),
   213   1362.4 MiB      0.0 MiB                                   lat=slice(df_place.lat.max() + 2,
   214   1362.4 MiB      0.1 MiB                                             df_place.lat.min() - 2))
   215   1362.4 MiB      0.0 MiB           return ds_readv
   216
   217
   218                                 #Make deterministic readvance fingerprint
   219    328.0 MiB      0.0 MiB       start, end = 6100, 3000
   220    328.0 MiB      0.0 MiB       maxscale = 2.25
   221    601.3 MiB      0.0 MiB       ds_readv = make_fingerprint(start, end, maxscale)
   222
   223                                 #Make readvance prior
   224    601.3 MiB      0.0 MiB       start, end = 8000, 2000
   225    601.3 MiB      0.0 MiB       maxscale = 2.25
   226    860.4 MiB      0.0 MiB       ds_readvprior = make_fingerprint(start, end, maxscale)
   227    860.4 MiB      0.0 MiB       ds_readvprior_std = ds_readvprior * 0.3
   228
   229                                 ####################  Build  GIA models 	#######################
   230                                 #################### ---------------------- #######################
   231
   232                                 #Use either glac1d or ICE6G
   233    860.4 MiB      0.0 MiB       if ice_model == 'glac1d_':
   234
   235                                     def build_dataset(model):
   236                                         """download model runs from local directory."""
   237
   238                                         path = f'data/glac1d_/output_{model}'
   239                                         files = f'{path}*.nc'
   240                                         basefiles = glob.glob(files)
   241                                         modelrun = [
   242                                             key.split('glac1d/output_', 1)[1][:-3].replace('.', '_')
   243                                             for key in basefiles
   244                                         ]
   245                                         dss = xr.open_mfdataset(files,
   246                                                                 chunks=None,
   247                                                                 concat_dim='modelrun',
   248                                                                 combine='nested')
   249                                         lats, lons, times = dss.LAT.values[0], dss.LON.values[
   250                                             0], dss.TIME.values[0]
   251                                         ds = dss.drop(['LAT', 'LON', 'TIME'])
   252                                         ds = ds.assign_coords(lat=lats,
   253                                                               lon=lons,
   254                                                               time=times,
   255                                                               modelrun=modelrun).rename({
   256                                                                   'time': 'age',
   257                                                                   'RSL': 'rsl'
   258                                                               })
   259                                         return ds
   260
   261                                     def one_mod(names):
   262                                         """Organize model runs into xarray dataset."""
   263                                         ds1 = build_dataset(names[0])
   264                                         names = names[1:]
   265                                         ds = ds1.chunk({'lat': 10, 'lon': 10})
   266                                         for i in range(len(names)):
   267                                             temp = build_dataset(names[i])
   268                                             temp1 = temp.interp_like(ds1)
   269                                             temp1['modelrun'] = temp['modelrun']
   270                                             ds = xr.concat([ds, temp1], dim='modelrun')
   271                                         ds['age'] = ds['age'] * 1000
   272                                         ds = ds.roll(lon=256, roll_coords=True)
   273                                         ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \
   274                                                                 .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \
   275                                                                 .reset_index(drop=True).squeeze()
   276                                         ds.coords['lat'] = ds.lat[::-1]
   277                                         ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   278                                         return ds
   279
   280                                     #make composite of a bunch of GIA runs, i.e. GIA prior
   281                                     ds = one_mod([ice_model + lith_thickness])
   282
   283                                     ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   284                                                            lon=slice(df_place.lon.min() - 2,
   285                                                                      df_place.lon.max() + 2),
   286                                                            lat=slice(df_place.lat.min() - 2,
   287                                                                      df_place.lat.max() + 2))
   288                                     ds_area = ds_sliced.mean(dim='modelrun').load().to_dataset().interp(
   289                                         age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   290                                     ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(
   291                                         age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   292
   293                                     # make "true" RSL by adding single GIA run and fingerprint
   294                                     lithmantle = 'l71C_ump2_lm50'
   295                                     ds_diff = one_mod(
   296                                         [ice_model + 'l71C']).sel(modelrun=ice_model + lithmantle).rsl.sel(
   297                                             age=slice(tmax, tmin),
   298                                             lon=slice(df_place.lon.min() - 2,
   299                                                       df_place.lon.max() + 2),
   300                                             lat=slice(df_place.lat.min() - 2,
   301                                                       df_place.lat.max() + 2)).load().to_dataset().interp(
   302                                                           age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   303
   304                                 else:
   305
   306   1210.7 MiB      0.0 MiB           def build_dataset(model):
   307                                         """download model runs from local directory."""
   308
   309   1210.7 MiB      0.0 MiB               path = f'data/d6g_h6g_/output_{model}'
   310   1210.7 MiB      0.0 MiB               files = f'{path}*.nc'
   311   1210.7 MiB      0.0 MiB               basefiles = glob.glob(files)
   312                                         modelrun = [
   313   1210.7 MiB      0.0 MiB                   key.split('d6g_h6g_/output_', 1)[1][:-3].replace('.', '_')
   314   1210.7 MiB      0.0 MiB                   for key in basefiles
   315                                         ]
   316   1210.7 MiB      0.0 MiB               dss = xr.open_mfdataset(files,
   317   1210.7 MiB      0.0 MiB                                       chunks=None,
   318   1210.7 MiB      0.0 MiB                                       concat_dim='modelrun',
   319   1210.7 MiB     30.3 MiB                                       combine='nested')
   320   1210.7 MiB      2.8 MiB               lats, lons, times = dss.LAT.values[0], dss.LON.values[
   321   1210.7 MiB      0.5 MiB                   0], dss.TIME.values[0]
   322   1210.7 MiB      0.0 MiB               ds = dss.drop(['LAT', 'LON', 'TIME'])
   323   1210.7 MiB      0.0 MiB               ds = ds.assign_coords(lat=lats,
   324   1210.7 MiB      0.0 MiB                                     lon=lons,
   325   1210.7 MiB      0.0 MiB                                     time=times,
   326   1210.7 MiB      0.0 MiB                                     modelrun=modelrun).rename({
   327   1210.7 MiB      0.0 MiB                                         'time': 'age',
   328   1210.7 MiB      0.0 MiB                                         'RSL': 'rsl'
   329                                                               })
   330   1210.7 MiB      0.0 MiB               return ds
   331
   332   1210.7 MiB      0.0 MiB           def one_mod(names):
   333                                         """Organize model runs into xarray dataset."""
   334   1210.7 MiB      0.0 MiB               ds1 = build_dataset(names[0])
   335   1210.7 MiB      0.0 MiB               names = names[1:]
   336   1234.9 MiB     29.1 MiB               ds = ds1.chunk({'lat': 10, 'lon': 10})
   337   1234.9 MiB      0.0 MiB               for i in range(len(names)):
   338                                             temp = build_dataset(names[i])
   339                                             temp1 = temp.interp_like(ds1)
   340                                             temp1['modelrun'] = temp['modelrun']
   341                                             ds = xr.concat([ds, temp1], dim='modelrun')
   342   1234.9 MiB      0.0 MiB               ds['age'] = ds['age'] * 1000
   343   1343.6 MiB    110.9 MiB               ds = ds.roll(lon=256, roll_coords=True)
   344   1343.6 MiB      0.1 MiB               ds.coords['lon'] = pd.DataFrame((ds.lon[ds.lon >= 180] - 360)- 0.12 ) \
   345   1343.6 MiB      0.0 MiB                                       .append(pd.DataFrame(ds.lon[ds.lon < 180]) + 0.58) \
   346   1343.6 MiB      0.0 MiB                                       .reset_index(drop=True).squeeze()
   347   1343.6 MiB      0.0 MiB               ds = ds.swap_dims({'dim_0': 'lon'}).drop('dim_0')
   348   1343.6 MiB      0.0 MiB               return ds
   349
   350                                     #make composite of a bunch of GIA runs, i.e. GIA prior
   351   1034.3 MiB      0.0 MiB           ds = one_mod([ice_model + lith_thickness])
   352
   353   1034.3 MiB      0.0 MiB           ds_sliced = ds.rsl.sel(age=slice(tmax, tmin),
   354   1034.3 MiB      0.0 MiB                                  lon=slice(df_place.lon.min() - 2,
   355   1034.3 MiB      0.0 MiB                                            df_place.lon.max() + 2),
   356   1034.3 MiB      0.0 MiB                                  lat=slice(df_place.lat.max() + 2,
   357   1034.4 MiB      0.1 MiB                                            df_place.lat.min() - 2))
   358   1193.6 MiB    159.2 MiB           ds_area = ds_sliced.mean(dim='modelrun').load().to_dataset().interp(
   359   1204.8 MiB     11.1 MiB               age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   360   1247.9 MiB     43.1 MiB           ds_areastd = ds_sliced.std(dim='modelrun').load().to_dataset().interp(
   361   1210.7 MiB      0.0 MiB               age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   362
   363                                     # make "true" RSL by adding single GIA run and fingerprint
   364   1210.7 MiB      0.0 MiB           lithmantle = 'l71C_ump2_lm50'
   365   1210.7 MiB      0.0 MiB           ds_diff = one_mod(
   366   1343.6 MiB      0.0 MiB               [ice_model + 'l71C']).sel(modelrun=ice_model + lithmantle).rsl.sel(
   367   1343.6 MiB      0.0 MiB                   age=slice(tmax, tmin),
   368   1343.6 MiB      0.0 MiB                   lon=slice(df_place.lon.min() - 2,
   369   1343.6 MiB      0.0 MiB                             df_place.lon.max() + 2),
   370   1343.6 MiB      0.0 MiB                   lat=slice(df_place.lat.max() + 2,
   371   1205.3 MiB      0.0 MiB                             df_place.lat.min() - 2)).load().to_dataset().interp(
   372   1216.9 MiB     11.6 MiB                                 age=ds_readv.age, lon=ds_readv.lon, lat=ds_readv.lat)
   373
   374                                 #make residual by subtracting GIA prior and fingerprint prior from "true" GIA
   375   1216.9 MiB      0.0 MiB       ds_true = ds_diff + ds_readv
   376   1216.9 MiB      0.0 MiB       ds_prior = ds_area + ds_readvprior
   377   1216.9 MiB      0.0 MiB       ds_priorstd = ds_areastd + ds_readvprior_std
   378   1216.9 MiB      0.0 MiB       ds_truelessprior = ds_true - ds_prior
   379
   380
   381                                 #sample each model at points where we have RSL data
   382   1217.3 MiB      0.0 MiB       def ds_select(ds):
   383   1217.3 MiB      0.0 MiB           return ds.rsl.sel(age=[row.age],
   384   1217.3 MiB      0.0 MiB                             lon=[row.lon],
   385   1217.3 MiB      0.0 MiB                             lat=[row.lat],
   386   1217.3 MiB      0.0 MiB                             method='nearest').squeeze().values
   387
   388
   389                                 #select points at which RSL data exists
   390   1217.3 MiB      0.0 MiB       for i, row in df_place.iterrows():
   391   1217.3 MiB      0.0 MiB           df_place.loc[i, 'rsl_true'] = ds_select(ds_true)
   392   1217.3 MiB      0.0 MiB           df_place.loc[i, 'rsl_resid'] = ds_select(ds_truelessprior)
   393   1217.3 MiB      0.0 MiB           df_place.loc[i, 'rsl_realresid'] = df_place.rsl[i] - ds_select(ds_area)
   394
   395   1217.3 MiB      0.0 MiB           df_place.loc[i, 'rsl_totalprior'] = ds_select(ds_prior)
   396   1217.3 MiB      0.0 MiB           df_place.loc[i, 'rsl_totalprior_std'] = ds_select(ds_priorstd)
   397   1217.3 MiB      0.1 MiB           df_place.loc[i, 'rsl_giaprior'] = ds_select(ds_area)
   398   1217.3 MiB      0.0 MiB           df_place.loc[i, 'rsl_giaprior_std'] = ds_select(ds_areastd)
   399   1217.3 MiB      0.2 MiB           df_place.loc[i, 'rsl_readvprior'] = ds_select(ds_readvprior)
   400   1217.3 MiB      0.0 MiB           df_place.loc[i, 'rsl_readvprior_std'] = ds_select(ds_readvprior_std)
   401   1217.3 MiB      0.0 MiB       print('number of datapoints = ', df_place.shape)
   402
   403                                 ##################	  RUN GP REGRESSION 	#######################
   404                                 ##################  --------------------	 ######################
   405
   406   1217.3 MiB      0.0 MiB       start = time.time()
   407
   408   1217.3 MiB      0.0 MiB       Data = Tuple[tf.Tensor, tf.Tensor]
   409   1217.3 MiB      0.0 MiB       likelihood = df_place.rsl_er_max.ravel()**2 + df_place.rsl_giaprior_std.ravel(
   410   1217.3 MiB      0.0 MiB       )**2  # here we define likelihood
   411
   412
   413   1217.3 MiB      0.0 MiB       class GPR_diag(gpf.models.GPModel):
   414                                     r"""
   415                                     Gaussian Process Regression.
   416                                     This is a vanilla implementation of GP regression with a pointwise Gaussian
   417                                     likelihood.  Multiple columns of Y are treated independently.
   418                                     The log likelihood of this models is sometimes referred to as the 'marginal log likelihood',
   419                                     and is given by
   420                                     .. math::
   421                                        \log p(\mathbf y \,|\, \mathbf f) =
   422                                             \mathcal N\left(\mathbf y\,|\, 0, \mathbf K + \sigma_n \mathbf I\right)
   423   1217.3 MiB      0.0 MiB           """
   424   1220.2 MiB      0.0 MiB           def __init__(self,
   425                                                  data: Data,
   426                                                  kernel: Kernel,
   427   1217.3 MiB      0.0 MiB                        mean_function: Optional[MeanFunction] = None,
   428   1217.3 MiB      0.0 MiB                        likelihood=likelihood):
   429   1220.2 MiB      0.0 MiB               likelihood = gpf.likelihoods.Gaussian(variance=likelihood)
   430   1220.2 MiB      0.0 MiB               _, y_data = data
   431   1220.2 MiB      0.0 MiB               super().__init__(kernel,
   432   1220.2 MiB      0.0 MiB                                likelihood,
   433   1220.2 MiB      0.0 MiB                                mean_function,
   434   1220.2 MiB      0.0 MiB                                num_latent=y_data.shape[-1])
   435   1220.2 MiB      0.0 MiB               self.data = data
   436
   437   1221.7 MiB      0.1 MiB           def log_likelihood(self):
   438                                         """
   439                                         Computes the log likelihood.
   440                                         """
   441   1221.7 MiB      0.0 MiB               x, y = self.data
   442   1222.4 MiB      0.7 MiB               K = self.kernel(x)
   443   1222.4 MiB      0.0 MiB               num_data = x.shape[0]
   444   1222.4 MiB      0.0 MiB               k_diag = tf.linalg.diag_part(K)
   445   1222.4 MiB      0.0 MiB               s_diag = tf.convert_to_tensor(self.likelihood.variance)
   446   1222.4 MiB      0.0 MiB               jitter = tf.cast(tf.fill([num_data], default_jitter()),
   447   1222.4 MiB      0.0 MiB                                'float64')  # stabilize K matrix w/jitter
   448   1222.4 MiB      0.0 MiB               ks = tf.linalg.set_diag(K, k_diag + s_diag + jitter)
   449   1222.4 MiB      0.0 MiB               L = tf.linalg.cholesky(ks)
   450   1222.5 MiB      0.1 MiB               m = self.mean_function(x)
   451
   452                                         # [R,] log-likelihoods for each independent dimension of Y
   453   1222.5 MiB      0.0 MiB               log_prob = multivariate_normal(y, m, L)
   454   1222.5 MiB      0.0 MiB               return tf.reduce_sum(log_prob)
   455
   456    105.3 MiB      0.0 MiB           def predict_f(self,
   457                                                   predict_at: tf.Tensor,
   458                                                   full_cov: bool = False,
   459   1217.3 MiB      0.0 MiB                         full_output_cov: bool = False):
   460                                         r"""
   461                                         This method computes predictions at X \in R^{N \x D} input points
   462                                         .. math::
   463                                             p(F* | Y)
   464                                         where F* are points on the GP at new data points, Y are noisy observations at training data points.
   465                                         """
   466    105.4 MiB      0.1 MiB               x_data, y_data = self.data
   467    106.9 MiB      1.5 MiB               err = y_data - self.mean_function(x_data)
   468
   469    295.3 MiB    188.4 MiB               kmm = self.kernel(x_data)
   470    305.4 MiB     10.0 MiB               knn = self.kernel(predict_at, full=full_cov)
   471    134.3 MiB      0.0 MiB               kmn = self.kernel(x_data, predict_at)
   472
   473    134.4 MiB      0.1 MiB               num_data = x_data.shape[0]
   474    134.5 MiB      0.0 MiB               s = tf.linalg.diag(tf.convert_to_tensor(
   475    165.0 MiB     30.6 MiB                   self.likelihood.variance))  #changed from normal GPR
   476
   477    165.1 MiB      0.0 MiB               conditional = gpf.conditionals.base_conditional
   478    165.1 MiB      0.0 MiB               f_mean_zero, f_var = conditional(
   479    218.4 MiB     53.3 MiB                   kmn, kmm + s, knn, err, full_cov=full_cov,
   480     86.0 MiB      0.0 MiB                   white=False)  # [N, P], [N, P] or [P, N, N]
   481     90.9 MiB      4.9 MiB               f_mean = f_mean_zero + self.mean_function(predict_at)
   482     90.9 MiB      0.0 MiB               return f_mean, f_var
   483
   484
   485   1217.3 MiB      0.0 MiB       def normalize(df):
   486   1217.3 MiB      0.0 MiB           return np.array((df - df.mean()) / df.std()).reshape(len(df), 1)
   487
   488
   489   1217.3 MiB      0.8 MiB       def denormalize(y_pred, df):
   490     92.3 MiB      0.0 MiB           return np.array((y_pred * df.std()) + df.mean())
   491
   492
   493   1220.2 MiB      0.0 MiB       def bounded_parameter(low, high, param):
   494                                     """Make parameter tfp Parameter with optimization bounds."""
   495   1220.2 MiB      0.0 MiB           affine = tfb.AffineScalar(shift=tf.cast(low, tf.float64),
   496   1220.2 MiB      0.0 MiB                                     scale=tf.cast(high - low, tf.float64))
   497   1220.2 MiB      0.0 MiB           sigmoid = tfb.Sigmoid()
   498   1220.2 MiB      0.0 MiB           logistic = tfb.Chain([affine, sigmoid])
   499   1220.2 MiB      0.0 MiB           parameter = gpf.Parameter(param, transform=logistic, dtype=tf.float64)
   500   1220.2 MiB      0.0 MiB           return parameter
   501
   502
   503   1217.3 MiB      0.0 MiB       class HaversineKernel_Matern52(gpf.kernels.Matern52):
   504                                     """
   505                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   506                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   507   1217.3 MiB      0.0 MiB           """
   508                                     def __init__(
   509                                         self,
   510                                         lengthscale=1.0,
   511                                         variance=1.0,
   512   1217.3 MiB      0.0 MiB               active_dims=None,
   513                                     ):
   514                                         super().__init__(
   515                                             active_dims=active_dims,
   516                                             variance=variance,
   517                                             lengthscale=lengthscale,
   518                                         )
   519
   520   1217.3 MiB      0.0 MiB           def haversine_dist(self, X, X2):
   521                                         pi = np.pi / 180
   522                                         f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   523                                         f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   524                                         d = tf.sin((f - f2) / 2)**2
   525                                         lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   526                                                     tf.expand_dims(X2[:, 0] * pi, -2)
   527                                         cos_prod = tf.cos(lat2) * tf.cos(lat1)
   528                                         a = d[:, :, 0] + cos_prod * d[:, :, 1]
   529                                         c = tf.asin(tf.sqrt(a)) * 6371 * 2
   530                                         return c
   531
   532   1217.3 MiB      0.0 MiB           def scaled_squared_euclid_dist(self, X, X2):
   533                                         """
   534                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   535                                         """
   536                                         if X2 is None:
   537                                             X2 = X
   538                                         dist = da.square(self.haversine_dist(X, X2) / self.lengthscale)
   539                             #             dist = tf.convert_to_tensor(dist)
   540                                         return dist
   541
   542
   543   1217.3 MiB      0.0 MiB       class HaversineKernel_Matern32(gpf.kernels.Matern32):
   544                                     """
   545                                     Isotropic Matern52 Kernel with Haversine distance instead of euclidean distance.
   546                                     Assumes n dimensional data, with columns [latitude, longitude] in degrees.
   547   1217.3 MiB      0.0 MiB           """
   548                                     def __init__(
   549                                         self,
   550                                         lengthscale=1.0,
   551                                         variance=1.0,
   552   1217.3 MiB      0.0 MiB               active_dims=None,
   553                                     ):
   554                                         super().__init__(
   555                                             active_dims=active_dims,
   556                                             variance=variance,
   557                                             lengthscale=lengthscale,
   558                                         )
   559
   560   1217.3 MiB      0.0 MiB           def haversine_dist(self, X, X2):
   561                                         pi = np.pi / 180
   562                                         f = tf.expand_dims(X * pi, -2)  # ... x N x 1 x D
   563                                         f2 = tf.expand_dims(X2 * pi, -3)  # ... x 1 x M x D
   564                                         d = tf.sin((f - f2) / 2)**2
   565                                         lat1, lat2 = tf.expand_dims(X[:, 0] * pi, -1), \
   566                                                     tf.expand_dims(X2[:, 0] * pi, -2)
   567                                         cos_prod = tf.cos(lat2) * tf.cos(lat1)
   568                                         a = d[:, :, 0] + cos_prod * d[:, :, 1]
   569                                         c = tf.asin(tf.sqrt(a)) * 6371 * 2
   570                                         return c
   571
   572   1217.3 MiB      0.0 MiB           def scaled_squared_euclid_dist(self, X, X2):
   573                                         """
   574                                         Returns (dist(X, X2ᵀ)/lengthscales)².
   575                                         """
   576                                         if X2 is None:
   577                                             X2 = X
   578                                         dist = tf.square(self.haversine_dist(X, X2) / self.lengthscale)
   579                             #             dist = tf.convert_to_tensor(dist) # return to tensorflow
   580                                         return dist
   581
   582
   583                                 ########### Section to Run GPR######################
   584                                 ##################################3#################
   585
   586                                 # Input space, rsl normalized to zero mean, unit variance
   587   1217.3 MiB      0.0 MiB       X = np.stack((df_place['lon'], df_place['lat'], df_place['age']), 1)
   588   1217.3 MiB      0.0 MiB       RSL = normalize(df_place.rsl_realresid)
   589
   590                                 #define kernels  with bounds
   591
   592                             #     k1 = HaversineKernel_Matern32(active_dims=[0, 1])
   593                             #     k1.lengthscale = bounded_parameter(5000, 30000, 10000)  #hemispheric space
   594                             #     k1.variance = bounded_parameter(0.1, 100, 2)
   595
   596   1220.1 MiB      2.8 MiB       k1 = gpf.kernels.Matern32(active_dims=[0, 1])
   597   1220.2 MiB      0.0 MiB       k1.lengthscale = bounded_parameter(10, 300, 50)  #hemispheric space
   598   1220.2 MiB      0.0 MiB       k1.variance = bounded_parameter(0.1, 100, 2)
   599
   600                             #     k2 = HaversineKernel_Matern32(active_dims=[0, 1])
   601                             #     k2.lengthscale = bounded_parameter(10, 5000, 100)  #GIA space
   602                             #     k2.variance = bounded_parameter(0.1, 100, 2)
   603
   604   1220.2 MiB      0.0 MiB       k2 = gpf.kernels.Matern32(active_dims=[0,1])
   605   1220.2 MiB      0.0 MiB       k2.lengthscale = bounded_parameter(1, 10, 4)  #GIA space
   606   1220.2 MiB      0.0 MiB       k2.variance = bounded_parameter(0.1, 100, 2)
   607
   608   1220.2 MiB      0.0 MiB       k3 = gpf.kernels.Matern32(active_dims=[2])  #GIA time
   609   1220.2 MiB      0.0 MiB       k3.lengthscale = bounded_parameter(8000, 20000, 10000)
   610   1220.2 MiB      0.0 MiB       k3.variance = bounded_parameter(0.1, 100, 1)
   611
   612   1220.2 MiB      0.0 MiB       k4 = gpf.kernels.Matern32(active_dims=[2])  #shorter time
   613   1220.2 MiB      0.0 MiB       k4.lengthscale = bounded_parameter(1, 8000, 1000)
   614   1220.2 MiB      0.0 MiB       k4.variance = bounded_parameter(0.1, 100, 1)
   615
   616   1220.2 MiB      0.0 MiB       k5 = gpf.kernels.White(active_dims=[2])
   617   1220.2 MiB      0.0 MiB       k5.variance = bounded_parameter(0.1, 100, 1)
   618
   619   1220.2 MiB      0.0 MiB       kernel = (k1 * k3) + (k2 * k4) + k5
   620
   621                                 #build & train model
   622   1220.2 MiB      0.0 MiB       m = GPR_diag((X, RSL), kernel=kernel, likelihood=likelihood)
   623   1220.2 MiB      0.0 MiB       print('model built, time=', time.time() - start)
   624
   625
   626   1221.7 MiB      1.5 MiB       @tf.function(autograph=False)
   627                                 def objective():
   628   1222.6 MiB      0.0 MiB           return -m.log_marginal_likelihood()
   629
   630
   631   1220.2 MiB      0.0 MiB       o = gpf.optimizers.Scipy()
   632    390.7 MiB      0.0 MiB       o.minimize(objective, variables=m.trainable_variables)
   633    390.8 MiB      0.1 MiB       print('model minimized, time=', time.time() - start)
   634
   635                                 # output space
   636    390.8 MiB      0.0 MiB       nout = 50
   637    395.0 MiB      4.2 MiB       lat = np.linspace(min(ds_area.lat), max(ds_area.lat), nout)
   638    395.0 MiB      0.0 MiB       lon = np.linspace(min(ds_area.lon), max(ds_area.lon), nout)
   639    395.0 MiB      0.0 MiB       ages = ages_lgm[(ages_lgm < tmax) & (ages_lgm > tmin)]
   640    105.3 MiB      0.0 MiB       xyt = np.array(list(product(lon, lat, ages)))
   641
   642                                 #query model & renormalize data
   643     89.6 MiB      0.0 MiB       y_pred, var = m.predict_f(xyt)
   644     92.3 MiB      0.0 MiB       y_pred_out = denormalize(y_pred, df_place.rsl_realresid)
   645
   646                                 #reshape output vectors
   647     97.8 MiB      5.5 MiB       Xlon = np.array(xyt[:, 0]).reshape((nout, nout, len(ages)))
   648     97.8 MiB      0.0 MiB       Xlat = np.array(xyt[:, 1]).reshape((nout, nout, len(ages)))
   649     99.7 MiB      1.8 MiB       Zp = np.array(y_pred_out).reshape(nout, nout, len(ages))
   650    101.5 MiB      1.8 MiB       varp = np.array(var).reshape(nout, nout, len(ages))
   651
   652                                 #print kernel details
   653    109.1 MiB      7.5 MiB       print_summary(m, fmt='notebook')
   654    109.1 MiB      0.0 MiB       print('time elapsed = ', time.time() - start)
   655
   656    109.1 MiB      0.0 MiB       print('negative log marginal likelihood =',
   657    252.8 MiB      0.0 MiB             m.neg_log_marginal_likelihood().numpy())
   658
   659                                 ##################	  INTERPOLATE MODELS 	#######################
   660                                 ##################  --------------------	 ######################
   661
   662                                 # turn GPR output into xarray dataarray
   663    252.8 MiB      0.0 MiB       da_zp = xr.DataArray(Zp, coords=[lon, lat, ages],
   664    252.8 MiB      0.0 MiB                            dims=['lon', 'lat',
   665    254.2 MiB      1.4 MiB                                  'age']).transpose('age', 'lat', 'lon')
   666    254.2 MiB      0.0 MiB       da_varp = xr.DataArray(varp,
   667    254.2 MiB      0.0 MiB                              coords=[lon, lat, ages],
   668    254.2 MiB      0.0 MiB                              dims=['lon', 'lat',
   669    254.2 MiB      0.0 MiB                                    'age']).transpose('age', 'lat', 'lon')
   670
   671
   672    254.2 MiB      0.0 MiB       def interp_likegpr(ds):
   673    127.9 MiB      0.0 MiB           return ds.rsl.load().transpose().interp_like(da_zp)
   674
   675
   676                                 #interpolate all models onto GPR grid
   677     98.5 MiB      0.0 MiB       da_trueinterp = interp_likegpr(ds_true)
   678     99.1 MiB      0.6 MiB       ds_trueinterp = ds_true.interp(age=ages)
   679    103.9 MiB      0.0 MiB       da_priorinterp = interp_likegpr(ds_prior)
   680    104.0 MiB      0.1 MiB       ds_priorinterp = ds_prior.interp(age=ages)
   681    108.8 MiB      0.0 MiB       da_priorinterpstd = interp_likegpr(ds_priorstd)
   682    111.7 MiB      0.0 MiB       da_giapriorinterp = interp_likegpr(ds_area)
   683    111.7 MiB      0.0 MiB       ds_giapriorinterp = ds_area.interp(age=ages)
   684    116.5 MiB      0.0 MiB       da_giapriorinterpstd = interp_likegpr(ds_areastd)
   685    125.1 MiB      0.0 MiB       da_readvpriorinterp = interp_likegpr(ds_readvprior)
   686    127.9 MiB      0.0 MiB       da_readvpriorinterpstd = interp_likegpr(ds_readvprior_std)
   687
   688                                 # add total prior RSL back into GPR
   689    128.0 MiB      0.1 MiB       da_priorplusgpr = da_zp + da_giapriorinterp
   690
   691                                 ##################	  	 SAVE NETCDFS 	 	#######################
   692                                 ##################  --------------------	 ######################
   693
   694    128.0 MiB      0.0 MiB       path = 'output/'
   695    135.3 MiB      7.3 MiB       da_zp.to_netcdf(path + ice_model + lith_thickness + '_' + place + '_da_zp')
   696    135.3 MiB      0.0 MiB       da_giapriorinterp.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   697    135.4 MiB      0.1 MiB                                   '_giaprior')
   698    135.4 MiB      0.0 MiB       da_priorplusgpr.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   699    135.5 MiB      0.1 MiB                                 '_posterior')
   700    135.5 MiB      0.0 MiB       da_varp.to_netcdf(path + ice_model + lith_thickness + '_' + place +
   701    135.6 MiB      0.0 MiB                         '_gp_variance')
   702
   703                                 ##################		  PLOT  MODELS 		#######################
   704                                 ##################  --------------------	 ######################
   705
   706    135.6 MiB      0.0 MiB       dirName = f'figs/{place}/'
   707    135.6 MiB      0.0 MiB       if not os.path.exists(dirName):
   708                                     os.mkdir(dirName)
   709                                     print("Directory ", dirName, " Created ")
   710                                 else:
   711    135.6 MiB      0.0 MiB           print("Directory ", dirName, " already exists")
   712
   713    222.4 MiB      0.0 MiB       for i, age in enumerate(ages):
   714    218.8 MiB      0.0 MiB           if (age / 500).is_integer():
   715    218.8 MiB      0.0 MiB               step = (ages[0] - ages[1])
   716    218.8 MiB      1.3 MiB               df_it = df_place[(df_place.age < age) & (df_place.age > age - step)]
   717    218.9 MiB      0.1 MiB               resid_it = da_zp.sel(age=slice(age, age - step))
   718    218.9 MiB      0.0 MiB               rsl, var = df_it.rsl, df_it.rsl_er_max.values**2
   719    218.9 MiB      0.0 MiB               lat_it, lon_it = df_it.lat, df_it.lon
   720    218.9 MiB      0.1 MiB               vmin = ds_giapriorinterp.rsl.min().values  # + 10
   721    218.9 MiB      0.0 MiB               vmax = ds_giapriorinterp.rsl.max().values  # - 40
   722    218.9 MiB      0.0 MiB               vmin_std = 0
   723    218.9 MiB      0.0 MiB               vmax_std = 1
   724    218.9 MiB      0.0 MiB               tmin_it = np.round(age - step, 2)
   725    218.9 MiB      0.0 MiB               tmax_it = np.round(age, 2)
   726    218.9 MiB      0.0 MiB               cbarscale = 0.3
   727    218.9 MiB      0.0 MiB               fontsize = 20
   728    218.9 MiB      0.0 MiB               cmap = 'coolwarm'
   729    218.9 MiB      0.0 MiB               cbar_kwargs = {'shrink': cbarscale, 'label': 'RSL (m)'}
   730
   731    218.9 MiB      0.3 MiB               proj = ccrs.PlateCarree()
   732    218.9 MiB      0.0 MiB               projection = ccrs.PlateCarree()
   733                                         fig, (ax1, ax2, ax3,
   734    218.9 MiB      0.0 MiB                     ax4) = plt.subplots(1,
   735    218.9 MiB      0.0 MiB                                         4,
   736    218.9 MiB      0.0 MiB                                         figsize=(24, 16),
   737    220.1 MiB     11.1 MiB                                         subplot_kw=dict(projection=projection))
   738
   739                                         # total prior mean + "true" data
   740    220.1 MiB      0.2 MiB               ax1.coastlines(color='k')
   741    220.1 MiB      0.0 MiB               pc1 = ds_giapriorinterp.rsl[i].transpose().plot(ax=ax1,
   742    220.1 MiB      0.0 MiB                                                               transform=proj,
   743    220.1 MiB      0.0 MiB                                                               cmap=cmap,
   744    220.1 MiB      0.0 MiB                                                               norm=MidpointNormalize(
   745    220.1 MiB      0.0 MiB                                                                   vmin, vmax, 0),
   746    220.1 MiB      0.0 MiB                                                               add_colorbar=False,
   747    220.3 MiB      5.2 MiB                                                               extend='both')
   748    220.3 MiB      0.0 MiB               cbar = fig.colorbar(pc1,
   749    220.3 MiB      0.0 MiB                                   ax=ax1,
   750    220.3 MiB      0.0 MiB                                   shrink=.3,
   751    220.3 MiB      0.0 MiB                                   label='RSL (m)',
   752    220.6 MiB      0.3 MiB                                   extend='both')
   753    220.6 MiB      0.0 MiB               scat = ax1.scatter(lon_it,
   754    220.6 MiB      0.0 MiB                                  lat_it,
   755    220.6 MiB      0.0 MiB                                  s=80,
   756    220.6 MiB      0.0 MiB                                  c=rsl,
   757    220.6 MiB      0.0 MiB                                  edgecolor='k',
   758    220.6 MiB      0.0 MiB                                  vmin=vmin,
   759    220.6 MiB      0.0 MiB                                  vmax=vmax,
   760    220.6 MiB      0.0 MiB                                  norm=MidpointNormalize(vmin, vmax, 0),
   761    220.7 MiB      0.7 MiB                                  cmap=cmap)
   762    220.7 MiB      0.0 MiB               ax1.set_title(f'{np.round(ds_trueinterp.rsl[i].age.values, -1)} yrs',
   763    220.7 MiB      0.0 MiB                             fontsize=fontsize)
   764                                         #         ax1.set_extent(extent_)
   765
   766                                         # Learned difference between prior and "true" data
   767    220.7 MiB      0.0 MiB               ax2.coastlines(color='k')
   768    220.7 MiB      0.0 MiB               pc = da_zp[i, :, :].plot(ax=ax2,
   769    220.7 MiB      0.0 MiB                                        transform=proj,
   770    220.7 MiB      0.0 MiB                                        cmap=cmap,
   771    220.7 MiB      0.0 MiB                                        extend='both',
   772    220.7 MiB      0.0 MiB                                        norm=MidpointNormalize(
   773    220.7 MiB      0.0 MiB                                            resid_it.min(), resid_it.max(), 0),
   774    220.9 MiB      0.3 MiB                                        add_colorbar=False)
   775    220.9 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   776    220.9 MiB      0.0 MiB                                   ax=ax2,
   777    220.9 MiB      0.0 MiB                                   shrink=.3,
   778    220.9 MiB      0.0 MiB                                   label='RSL (m)',
   779    221.3 MiB      0.1 MiB                                   extend='both')
   780    221.3 MiB      0.0 MiB               scat = ax2.scatter(lon_it,
   781    221.3 MiB      0.0 MiB                                  lat_it,
   782    221.3 MiB      0.0 MiB                                  s=80,
   783    221.3 MiB      0.0 MiB                                  facecolors='k',
   784    221.3 MiB      0.0 MiB                                  cmap=cmap,
   785    221.3 MiB      0.0 MiB                                  edgecolor='k',
   786    221.3 MiB      0.0 MiB                                  transform=proj,
   787    221.3 MiB      0.0 MiB                                  norm=MidpointNormalize(resid_it.min(),
   788    221.3 MiB      0.1 MiB                                                         resid_it.max(), 0))
   789    221.3 MiB      0.0 MiB               ax2.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   790                                         #         ax2.set_extent(extent_)
   791
   792                                         # GP regression
   793    221.3 MiB      0.0 MiB               ax3.coastlines(color='k')
   794    221.3 MiB      0.0 MiB               pc = da_priorplusgpr[i].plot(ax=ax3,
   795    221.3 MiB      0.0 MiB                                            transform=proj,
   796    221.3 MiB      0.0 MiB                                            norm=MidpointNormalize(vmin, vmax, 0),
   797    221.3 MiB      0.0 MiB                                            cmap=cmap,
   798    221.3 MiB      0.0 MiB                                            extend='both',
   799    221.5 MiB      0.3 MiB                                            add_colorbar=False)
   800    221.5 MiB      0.0 MiB               scat = ax3.scatter(lon_it,
   801    221.5 MiB      0.0 MiB                                  lat_it,
   802    221.5 MiB      0.0 MiB                                  s=80,
   803    221.5 MiB      0.0 MiB                                  c=rsl,
   804    221.5 MiB      0.0 MiB                                  edgecolor='k',
   805    221.5 MiB      0.0 MiB                                  cmap=cmap,
   806    221.5 MiB      0.0 MiB                                  norm=MidpointNormalize(vmin, vmax, 0))
   807    221.5 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   808    221.5 MiB      0.0 MiB                                   ax=ax3,
   809    221.5 MiB      0.0 MiB                                   shrink=.3,
   810    221.5 MiB      0.0 MiB                                   label='RSL (m)',
   811    221.9 MiB      0.1 MiB                                   extend='both')
   812    221.9 MiB      0.0 MiB               ax3.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   813                                         #         ax3.set_extent(extent_)
   814
   815                                         #GP regression standard deviation
   816    221.9 MiB      0.0 MiB               ax4.coastlines(color='k')
   817    221.9 MiB      0.2 MiB               pc = (2 * np.sqrt(da_varp[i])).plot(
   818    221.9 MiB      0.0 MiB                   ax=ax4,
   819    221.9 MiB      0.0 MiB                   transform=proj,
   820    221.9 MiB      0.0 MiB                   vmin=vmin_std,
   821    221.9 MiB      0.0 MiB                   vmax=vmax_std * 2,
   822    221.9 MiB      0.0 MiB                   cmap='Reds',
   823    221.9 MiB      0.0 MiB                   extend='both',
   824    222.1 MiB      0.3 MiB                   add_colorbar=False,
   825                                         )
   826    222.1 MiB      0.0 MiB               scat = ax4.scatter(lon_it,
   827    222.1 MiB      0.0 MiB                                  lat_it,
   828    222.1 MiB      0.0 MiB                                  s=80,
   829    222.1 MiB      0.0 MiB                                  c=2 * np.sqrt(var),
   830    222.1 MiB      0.0 MiB                                  vmin=vmin_std,
   831    222.1 MiB      0.0 MiB                                  vmax=vmax_std * 2,
   832    222.1 MiB      0.0 MiB                                  cmap='Reds',
   833    222.1 MiB      0.0 MiB                                  edgecolor='k',
   834    222.1 MiB      0.0 MiB                                  transform=proj)
   835    222.1 MiB      0.0 MiB               cbar = fig.colorbar(pc,
   836    222.1 MiB      0.0 MiB                                   ax=ax4,
   837    222.1 MiB      0.0 MiB                                   shrink=.3,
   838    222.1 MiB      0.0 MiB                                   extend='both',
   839    222.4 MiB      0.4 MiB                                   label='RSL (m) (2 $\sigma$)')
   840    222.4 MiB      0.0 MiB               ax4.set_title(f'{np.round(tmax_it,2)} yrs', fontsize=fontsize)
   841                                 #         ax4.set_extent(extent_)
   842
   843                                 ########## ----- Save figures -------- #######################
   844    246.9 MiB      3.3 MiB       fig.savefig(dirName + f'{ages[i]}_{place}_realdata_fig_3D', transparent=True)
   845
   846                                 ##################	CHOOSE LOCS W/NUF SAMPS #######################
   847                                 ##################  --------------------	 ######################
   848
   849
   850    246.9 MiB      0.0 MiB       def locs_with_enoughsamples(df_place, place, number):
   851                                     """make new dataframe, labeled, of sites with [> number] measurements"""
   852    246.9 MiB      0.0 MiB           df_lots = df_place.groupby(['lat',
   853    247.9 MiB      0.8 MiB                                       'lon']).filter(lambda x: len(x) > number)
   854
   855    247.9 MiB      0.0 MiB           df_locs = []
   856    248.2 MiB      0.0 MiB           for i, group in enumerate(df_lots.groupby(['lat', 'lon'])):
   857    248.2 MiB      0.0 MiB               singleloc = group[1].copy()
   858    248.2 MiB      0.2 MiB               singleloc['location'] = place
   859    248.2 MiB      0.0 MiB               singleloc['locnum'] = place + '_site' + str(
   860    248.2 MiB      0.0 MiB                   i)  # + singleloc.reset_index().index.astype('str')
   861    248.2 MiB      0.0 MiB               df_locs.append(singleloc)
   862    248.5 MiB      0.4 MiB           df_locs = pd.concat(df_locs)
   863
   864    248.5 MiB      0.0 MiB           return df_locs
   865
   866
   867    246.9 MiB      0.0 MiB       number = 8
   868    248.5 MiB      0.0 MiB       df_nufsamps = locs_with_enoughsamples(df_place, place, number)
   869    248.6 MiB      0.0 MiB       len(df_nufsamps.locnum.unique())
   870
   871                                 ##################	PLOT LOCS W/NUF SAMPS   #######################
   872                                 ##################  --------------------	 ######################
   873
   874
   875    490.3 MiB      0.0 MiB       def slice_dataarray(da):
   876    490.3 MiB      0.0 MiB           return da.sel(lat=site[1].lat.unique(),
   877    490.3 MiB      0.0 MiB                         lon=site[1].lon.unique(),
   878    490.3 MiB      0.0 MiB                         method='nearest')
   879
   880
   881    485.7 MiB    237.1 MiB       fig, ax = plt.subplots(1, len(df_nufsamps.locnum.unique()), figsize=(18, 4))
   882    485.7 MiB      0.0 MiB       ax = ax.ravel()
   883    485.7 MiB      0.0 MiB       colors = ['darkgreen', 'darkblue', 'darkred']
   884    485.7 MiB      0.0 MiB       fontsize = 18
   885
   886    490.5 MiB      0.1 MiB       for i, site in enumerate(df_nufsamps.groupby('locnum')):
   887
   888                                     #slice data for each site
   889    490.3 MiB      0.0 MiB           prior_it = slice_dataarray(da_priorinterp)
   890    490.3 MiB      0.0 MiB           priorvar_it = slice_dataarray(da_priorinterpstd)
   891    490.3 MiB      0.0 MiB           top_prior = prior_it + priorvar_it * 2
   892    490.3 MiB      0.0 MiB           bottom_prior = prior_it - priorvar_it * 2
   893
   894    490.3 MiB      0.0 MiB           var_it = slice_dataarray(np.sqrt(da_varp))
   895    490.3 MiB      0.0 MiB           post_it = slice_dataarray(da_priorplusgpr)
   896    490.3 MiB      0.0 MiB           top = post_it + var_it * 2
   897    490.3 MiB      0.0 MiB           bottom = post_it - var_it * 2
   898
   899    490.3 MiB      0.0 MiB           site_err = 2 * (site[1].rsl_er_max)
   900
   901    490.3 MiB      0.0 MiB           ax[i].scatter(site[1].age, site[1].rsl, c=colors[0], label='"true" RSL')
   902    490.3 MiB      0.0 MiB           ax[i].errorbar(
   903    490.3 MiB      0.0 MiB               site[1].age,
   904    490.3 MiB      0.0 MiB               site[1].rsl,
   905    490.3 MiB      0.0 MiB               site_err,
   906    490.3 MiB      0.0 MiB               c=colors[0],
   907    490.3 MiB      0.0 MiB               fmt='none',
   908    490.3 MiB      0.0 MiB               capsize=1,
   909    490.3 MiB      0.1 MiB               lw=1,
   910                                     )
   911
   912    490.4 MiB      0.0 MiB           prior_it.plot(ax=ax[i], c=colors[2], label='Prior $\pm 2 \sigma$')
   913    490.4 MiB      0.0 MiB           ax[i].fill_between(prior_it.age,
   914    490.4 MiB      0.0 MiB                              bottom_prior.squeeze(),
   915    490.4 MiB      0.0 MiB                              top_prior.squeeze(),
   916    490.4 MiB      0.0 MiB                              color=colors[2],
   917    490.4 MiB      0.0 MiB                              alpha=0.3)
   918
   919    490.4 MiB      0.0 MiB           post_it.plot(ax=ax[i], c=colors[1], label='Posterior $\pm 2 \sigma$')
   920    490.4 MiB      0.0 MiB           ax[i].fill_between(post_it.age,
   921    490.4 MiB      0.0 MiB                              bottom.squeeze(),
   922    490.4 MiB      0.0 MiB                              top.squeeze(),
   923    490.4 MiB      0.0 MiB                              color=colors[1],
   924    490.4 MiB      0.0 MiB                              alpha=0.3)
   925                                     #     ax[i].set_title(f'{site[0]} RSL', fontsize=fontsize)
   926    490.4 MiB      0.0 MiB           ax[i].set_title('')
   927
   928    490.5 MiB      0.1 MiB           ax[i].legend(loc='lower left')
   929
   930    490.5 MiB      0.0 MiB       path = 'figs/{place}'
   931    490.5 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_realdata_fig_1D',
   932    498.5 MiB      8.1 MiB                   transparent=True)
   933
   934                                 #plot locations of data
   935    498.5 MiB      0.0 MiB       fig, ax = plt.subplots(1,
   936    498.5 MiB      0.0 MiB                              len(df_nufsamps.locnum.unique()),
   937    498.5 MiB      0.0 MiB                              figsize=(18, 4),
   938    505.2 MiB      6.7 MiB                              subplot_kw=dict(projection=projection))
   939    505.2 MiB      0.0 MiB       ax = ax.ravel()
   940
   941    505.2 MiB      0.0 MiB       da_zeros = xr.zeros_like(da_zp)
   942
   943    508.3 MiB      0.1 MiB       for i, site in enumerate(df_nufsamps.groupby('locnum')):
   944    508.2 MiB      0.0 MiB           ax[i].coastlines(color='k')
   945    508.2 MiB      0.0 MiB           ax[i].plot(site[1].lon.unique(),
   946    508.2 MiB      0.0 MiB                      site[1].lat.unique(),
   947    508.2 MiB      0.0 MiB                      c=colors[0],
   948    508.2 MiB      0.0 MiB                      ms=7,
   949    508.2 MiB      0.0 MiB                      marker='o',
   950    508.2 MiB      0.0 MiB                      transform=proj)
   951    508.2 MiB      0.0 MiB           ax[i].plot(site[1].lon.unique(),
   952    508.2 MiB      0.0 MiB                      site[1].lat.unique(),
   953    508.2 MiB      0.0 MiB                      c=colors[0],
   954    508.2 MiB      0.0 MiB                      ms=25,
   955    508.2 MiB      0.0 MiB                      marker='o',
   956    508.2 MiB      0.0 MiB                      transform=proj,
   957    508.2 MiB      0.0 MiB                      mfc="None",
   958    508.2 MiB      0.0 MiB                      mec='red',
   959    508.2 MiB      0.0 MiB                      mew=4)
   960    508.3 MiB      0.3 MiB           da_zeros[0].plot(ax=ax[i], cmap='Greys', add_colorbar=False)
   961    508.3 MiB      0.0 MiB           ax[i].set_title(site[0], fontsize=fontsize)
   962                                 # plt.tight_layout()
   963    508.3 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_realdata_fig_1Dlocs',
   964    514.2 MiB      5.9 MiB                   transparent=True)
   965
   966                                 #################   DECOMPOSE GPR INTO KERNELS ####################
   967                                 ##################  --------------------	 ######################
   968
   969
   970    514.2 MiB      0.0 MiB       def predict_decomp_f(m,
   971                                                      custom_kernel,
   972                                                      predict_at: tf.Tensor,
   973                                                      full_cov: bool = False,
   974                                                      full_output_cov: bool = False,
   975    514.2 MiB      0.0 MiB                            var=None):
   976                                     """Decompose GP into individual kernels."""
   977
   978    514.2 MiB      0.0 MiB           x_data, y_data = m.data
   979    514.2 MiB      0.0 MiB           err = y_data - m.mean_function(x_data)
   980    695.4 MiB    181.2 MiB           kmm = m.kernel(x_data)
   981    699.1 MiB      3.7 MiB           knn = custom_kernel(predict_at, full=full_cov)
   982   3658.3 MiB   2959.2 MiB           kmn = custom_kernel(x_data, predict_at)
   983   3658.3 MiB      0.1 MiB           num_data = x_data.shape[0]
   984   3658.3 MiB     27.3 MiB           s = tf.linalg.diag(tf.convert_to_tensor(var))  # added diagonal variance
   985   3658.3 MiB      0.0 MiB           conditional = gpf.conditionals.base_conditional
   986   3658.3 MiB      0.0 MiB           f_mean_zero, f_var = conditional(
   987   3658.3 MiB     53.5 MiB               kmn, kmm + s, knn, err, full_cov=full_cov,
   988   2811.6 MiB   1408.4 MiB               white=False)  # [N, P], [N, P] or [P, N, N]
   989   2815.8 MiB      4.2 MiB           f_mean = np.array(f_mean_zero + m.mean_function(predict_at))
   990   2815.8 MiB      0.0 MiB           f_var = np.array(f_var)
   991   2815.8 MiB      0.0 MiB           return f_mean, f_var
   992
   993
   994    514.2 MiB      0.1 MiB       def reshape_decomp(k, var=None):
   995    175.3 MiB      0.0 MiB           A, var = predict_decomp_f(m, k, xyt, var=var)
   996    175.4 MiB      0.0 MiB           A = A.reshape(nout, nout, len(ages))
   997    175.4 MiB      0.0 MiB           var = var.reshape(nout, nout, len(ages))
   998    175.4 MiB      0.0 MiB           return A, var
   999
  1000
  1001    514.2 MiB      0.0 MiB       def make_dataarray(da):
  1002    130.9 MiB      0.0 MiB           coords = [lon, lat, ages]
  1003    130.9 MiB      0.0 MiB           dims = ['lon', 'lat', 'age']
  1004    130.9 MiB      0.0 MiB           return xr.DataArray(da, coords=coords,
  1005    130.9 MiB      2.6 MiB                               dims=dims).transpose('age', 'lat', 'lon')
  1006
  1007
  1008    514.2 MiB      0.0 MiB       A1, var1 = reshape_decomp(k1,
  1009    514.2 MiB      0.0 MiB                                 var=df_place.rsl_er_max.ravel()**2 +
  1010    176.0 MiB      0.7 MiB                                 df_place.rsl_giaprior_std.ravel()**2)  #gia spatial
  1011    176.0 MiB      0.0 MiB       A2, var2 = reshape_decomp(k2,
  1012    177.0 MiB      1.0 MiB                                 var=df_place.rsl_er_max.ravel()**2 +
  1013     70.0 MiB      0.0 MiB                                 df_place.rsl_giaprior_std.ravel()**2)  #gia temporal
  1014     70.0 MiB      0.0 MiB       A3, var3 = reshape_decomp(
  1015     70.0 MiB      0.0 MiB           k3,
  1016     71.0 MiB      1.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1017     58.1 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance spatial
  1018     58.1 MiB      0.0 MiB       A4, var4 = reshape_decomp(
  1019     58.1 MiB      0.0 MiB           k4,
  1020     59.1 MiB      1.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1021     57.3 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance temporal
  1022     57.3 MiB      0.0 MiB       A5, var5 = reshape_decomp(
  1023     57.3 MiB      0.0 MiB           k5,
  1024     58.3 MiB      1.0 MiB           var=df_place.rsl_er_max.ravel()**2 +
  1025    128.3 MiB      0.0 MiB           df_place.rsl_giaprior_std.ravel()**2)  #readvance spatial
  1026
  1027    130.9 MiB      0.0 MiB       da_A1 = make_dataarray(A1)
  1028    130.9 MiB      0.0 MiB       da_var1 = make_dataarray(var1)
  1029
  1030    130.9 MiB      0.0 MiB       da_A2 = make_dataarray(A2)
  1031    130.9 MiB      0.0 MiB       da_var2 = make_dataarray(var2)
  1032
  1033    130.9 MiB      0.0 MiB       da_A3 = make_dataarray(A3)
  1034    130.9 MiB      0.0 MiB       da_var3 = make_dataarray(var3)
  1035
  1036    130.9 MiB      0.0 MiB       da_A4 = make_dataarray(A4)
  1037    130.9 MiB      0.0 MiB       da_var4 = make_dataarray(var4)
  1038
  1039    130.9 MiB      0.0 MiB       da_A5 = make_dataarray(A5)
  1040    130.9 MiB      0.0 MiB       da_var5 = make_dataarray(var5)
  1041
  1042                                 #################   PLOT DECOMPOSED KERNELS    ####################
  1043                                 ##################  --------------------	   ####################
  1044
  1045    142.6 MiB     11.7 MiB       fig, ax = plt.subplots(1, 6, figsize=(24, 4))
  1046    142.6 MiB      0.0 MiB       ax = ax.ravel()
  1047    148.8 MiB      6.2 MiB       da_A1[0, :, :].plot(ax=ax[0], cmap='RdBu_r')
  1048
  1049    151.0 MiB      2.2 MiB       da_A2[0, :, :].plot(ax=ax[1], cmap='RdBu_r')
  1050
  1051    153.3 MiB      2.3 MiB       da_A3[0, :, :].plot(ax=ax[2], cmap='RdBu_r')
  1052
  1053    153.5 MiB      0.2 MiB       da_A4[:, 0, 0].plot(ax=ax[3])
  1054
  1055    153.5 MiB      0.0 MiB       da_A5[:, 0, 0].plot(ax=ax[4])
  1056
  1057                                 # da_A6[:,0,0].plot(ax=ax[5])
  1058
  1059                                 # plt.tight_layout()
  1060
  1061    153.5 MiB      0.0 MiB       fig.savefig(dirName + f'{ages[0]}to{ages[-1]}_{place}_decompkernels',
  1062    162.8 MiB      9.2 MiB                   transparent=True)


